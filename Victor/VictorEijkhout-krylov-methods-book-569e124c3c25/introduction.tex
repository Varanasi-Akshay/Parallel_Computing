Many articles have already been written about the conjugate gradient
method and the Lanczos algorithm.
Some of these focus on preconditioners, some treat different
variants of the basic method, and others derive error or
convergence speed properties of the methods
In this article  we will
give a unified presentation of the conjugate gradient method
and the Lanczos algorithm, and derive  properties regarding
orthogonality and equivalence of various formulations.
We will stress showing what the minimal assumptions are
for various properties. 

In general we will only be concerned with
qualitative properties of the methods, leaving all quantitative
results, such as convergence speed, aside.
The conjugate gradient and Lanczos methods
will here be derived as orthogonalization methods for Krylov
sequences, and minimization properties will be derived from this
orthogonality. Also, the fact that such methods can be used for
the iterative solution of linear systems will be presented as a
corollary rather than as a starting point of the discussion.

Additionally, the presentation here will differ from most others in
the literature (with the exception of~\cite{Householder:theory}) in that it is
given in terms of matrices instead of vector sequences. Although an
occasional part of the discussion may feel somewhat forced by this, in
general the presentation is very concise.

