\Level 0 {Polynomial squaring methods}
\label{sec:cgs}

The Lanczos method can be characterised in general
through right and left sequences $R$ and~$S$, generated by
\[ AM\inv R=RH,\qquad A^tM\invt S=SK \]
where $H$ and~$K$ are upper Hessenberg matrices.
(In the scalar case, $H$~and~$K$ are equal; in the block case
see equations (\ref{eq:u-right}) and~(\ref{eq:u-left}).)
For the vectors in the sequences we have
$r_n=\rho_n(AM\inv)r_1$ and $s_n=\tilde\rho_n(A^tM\invt)s_1$,
where $\rho_n$ and $\tilde\rho_n$ are $n-1$-st degree polynomials.
(Again, in the scalar case for BiCG, $H=K$ 
and $\rho_n\equiv\tilde\rho_n$; see section~\ref{sec:bi-ortho}
for the exact results that also hold in the block case.)
Since presumably both the left and right residuals tend to zero, and
\[ s_n^tM\inv r_n = (\rho_n(A^tM\invt)s_1)^tM\inv(\rho_n(A)r_1)
 = s_1^tM\inv(\rho^2_n(AM\inv)r_1) \]
it may seem like a good idea to compute the sequence
$\rho^2_n(AM\inv)r_1$ since, ideally, this sequence would have double the
convergence speed of the original sequence. Here is why computing this
sequence  is possible.

\Level 1 {Matrix derivation of polynomial squaring methods}

Let $X$ be a Krylov sequence of~$A$, that is, $AX=XJ$, and let
$R$~consist of linear combination of the $X$~sequence: $R=XU$. From
the foregoing discussion we know that we have $AR=RH$ with $H=U\inv JU$.
For the $n$-th residual we have $r_n=\rho_n(A)x_1$;
see lemma~\ref{lemma:polynomial:recurrence} for more details.

We now define in each $n$-th step a new Krylov vector sequence $Y^{(n)}$
by \[ Y^{(n)}=\rho_n(A)X \Rightarrow AY^{(n)}=Y^{(n)}J. \]
Using again the upper triangular matrix~$U$ to take linear
combinations, this time from~$Y^{(n)}$, we define~$S^{(n)}$ by
\begin{equation}
	S^{(n)}=Y^{(n)}U \Rightarrow AS^{(n)}=S^{(n)}H. 
	\label{cgs:SH:relation}\end{equation}
Note that the matrix~$H$ is the same as above, and in particular
independent of~$n$.

The justification for these new sequences is that
\[ y^{(n)}_1=r_n,\qquad s^{(n)}_n=\rho_n(A)y^{(n)}_1=\rho^2_n(A)r_1. \]
Sonneveld~\cite{Sonneveld1989:cgs}'s Conjugate Gradients Squared
was the first formulation of a computation of these squared sequences.

There is in fact an unused degree of freedom in the above method: we
can choose a different upper triangular matrix~$V$ to form the linear
combinations~$S^{(n)}$, so that
\begin{equation} S^{(n)}=Y^{(n)}V \Rightarrow AS^{(n)}=S^{(n)}K
    \label{eq:cgs-mat}\end{equation}
where $K=V\inv JV$ is another Hessenberg matrix.
One option would be to let the combinations~$V$
correspond to a steepest descent method;
the resulting method by van der Vorst~\cite{vdVorst1992:bicgstab}
is known as BiCG Stabilised.

In all polynomial squaring methods, $H$~is the Hessenberg matrix of
the Lanczos method, that is, its elements  can be computed from the inner
products $s_i^tAr_j$,~$s_i^tr_j$ directly or indirectly.
For the computation of these inner products
the sequence~$S$ is not explicitly needed except for its
first element. For instance,
$s_i^tr_j=s_1\rho_i(A)\rho_j(A)r_1=s_1^tr_i^{(j)}$.

\Level 1 {Coefficients in general Polynomial squaring methods}

Consider a generalized
Lanczos method with right and left residual sequences $R$ and~$S$ and upper
Hessenberg matrices $H$ and~$K$ such that
\[ AM\inv R=RH \qquad\hbox{and}\qquad A^tM\invt S=SK. \]
For these sequences there are corresponding
sequences of polynomials $\{\rho_i\}$ and~$\{\sigma_i\}$ so that
(see lemma~\ref{lemma:PR-polynomials})
$r_i=\rho_i(AM\inv)r_1$ and~$s_i=\sigma_i(A^tM\invt)s_1$,
where the $i$-th polynomials
have degree~$i-1$.
The Lanczos method uses the same polynomials for both sequences, so we
will introduce a reference sequence~$\tilde R$
satisfying~$A^tM\invt\tilde R=\tilde RH$,~$\tilde r_1=s_1$,
and when needed reference search directions~$\tilde P$
satisfying $\tilde PU=\nobreak M\invt\tilde R$.
Equivalently, the elements of these
sequences are generated as \[ \tilde r_i=\rho_i(A^tM\invt)\tilde r_1. \]

We now consider the matter of computing the elements of~$H$.
While we are interested in having 
for instance the Lanczos coefficient
\begin{equation}
  \tilde r_i^tM\inv r_i
  =\bigl(\rho_i(A^tM\invt)s_1\bigr)^tM\inv \bigl(\rho_i(AM\inv)r_1\bigr) =
  s_1^tM\inv\rho_i^2(AM\inv)r_1,
	\label{eq:rho-lanczos}\end{equation}
the only thing we can reasonably compute is
\begin{eqnarray*}
  s_1^tM\inv r_i^{(i)}&=&s_1^tM\inv\sigma_i(AM\inv)\rho_i(AM\inv)r_1\\
  &=&\bigl(\sigma_i(A^tM\invt)s_1\bigr)^tM\inv\bigl(\rho_i(AM\inv)r_1\bigr)\\
  &=&s_i^tM\inv r_i.
	\label{eq:rho-bicgs}\end{eqnarray*}
The key to retrieving the Lanczos coefficient is 
in comparing the left sequence $s_i=\sigma_i(A^tM\invt)s_1$ to the reference
Lanczos left sequence $\tilde r_i=\rho_i(A^tM\invt)s_1$,
where $\rho_i$~are the polynomials of the right sequence.

Since both are polynomial sequences,
with the degrees of
corresponding polynomials $\sigma_i$ and~$\rho_i$ the same,
there are upper triangular
matrices $U$~and~$V$ such that
\[ \tilde R=KU,\qquad S=KV\qquad \hbox{where $K=\KmethodAv{A^tM\invt}{s_1}$},
   \]
so the sequences are related by $S=\tilde RU^{-1}V$. This
relation can be carried over to the coefficients.
Below are two ways of using the computed
quantities to retrieve the original Lanczos coefficients.

Defining the upper triangular matrix $W=V^{-1}U$, we find that
	\[ \tilde R^tM\inv R=W^tS^tM\inv R\qquad\hbox{and}\qquad
		 \tilde R^tAR=W^tS^tAR, \]
in which $S^tR$ is lower triangular and $S^tAR$~is of lower Hessenberg
form. This implies that we can retrieve the inner products in equations
(\ref{eq:lanczos-rr}) and~(\ref{eq:lanczos-rar}) as
\[ \tilde r_i^tr_i = w_{ii}s_i^tr_i \qquad\hbox{and}\qquad
\tilde r_i^tAr_i = w_{ii}s_i^tAr_i+w_{i-1i}s_{i-1}^tAr_i. \]
The only remaining problem is computing the numbers $w_{ii}$
and~$w_{i-1i}$. Implementations of BiCGstab based on coupled two-term
recurrences needs only~$w_{ii}$.

The necessary elements of~$W$ follow from the equation~$VW=U$:
	\begin{equation}
	w_{ii}=u_{ii}/v_{ii} \qquad\hbox{and}\qquad
	w_{i-1i}=(u_{i-1i}-v_{i-1i}w_{ii})/v_{i-1i-1}
	\label{eq:w-from-uv}\end{equation}
where we compute the elements of~$U$
from~$H$ by the recurrences
	\begin{eqnarray*}
		u_{i+1i+1}h_{i+1i}&=&u_{ii}\\
		u_{ii+1}h_{i+1i}&=&u_{i-1i}-u_{ii}h_{ii}
	\end{eqnarray*}
and similar formulas for computing $V$ from~$K$.

Using these formulas directly gives to a method that has a danger of
leading quickly to floating point underflow. Therefore we note that we
only need the ratios 
\begin{equation}
	{\tilde r_{i+1}^tr_{i+1}\over \tilde r_i^tr_i}=
		{s_{i+1}^tr_{i+1}\over s_i^tr_i}\cdot
		{k_{i+1i}\over h_{i+1i}}
	\label{eq:rr-cor}\end{equation}
and
\begin{equation}
	{\tilde r_{i+1}^tAr_{i+1}\over \tilde r_{i+1}^tr_{i+1}}=
		{s_{i+1}^tAr_{i+1}\over s_{i+1}^tr_{i+1}}+
		{w_{ii+1}\over w_{i+1i+1}}\cdot
		{s_{i}^tAr_{i+1}\over s_{i+1}^tr_{i+1}}
	\label{eq:rar-cor-inprod-sr}\end{equation}
in order to compute the Hessenberg matrix of the bi-conjugate gradient
algorithm; see equation~(\ref{eq:H-from-rr}).

Similar formulas are arrived
at by taking for~$W$ the inverse of the above choice.

Define then the upper triangular matrix~$W=V^{-1}U$.
This leads to
	\[ S^tR=W^t\tilde R^tR\qquad\hbox{and}\qquad
		S^tAR=W^t\tilde R^tAR. \]
Now we find for the inner products in equations
(\ref{eq:lanczos-rr}) and~(\ref{eq:lanczos-rar}) that
	\[ w_{ii}\tilde r_i^tr_i = s_i^tr_i \]
which leads again to formula~(\ref{eq:rr-cor}), and
	\[ w_{ii}\tilde r_i^tAr_i =
		s_i^tAr_i-w_{i-1i}\tilde r_{i-1}^tAr_i. \]
which gives, analogous to by slightly differing
from~(\ref{eq:rar-cor-inprod-sr}) 
\begin{equation}
	{\tilde r_{i+1}^tAr_{i+1}\over \tilde r_{i+1}^tr_{i+1}}=
		{s_{i+1}^tAr_{i+1}\over s_{i+1}^tr_{i+1}}+
		{w_{ii+1}\over w_{i+1i+1}}\cdot
		{\tilde r_{i}^tAr_{i+1}\over \tilde r_{i+1}^tr_{i+1}}.
	\label{eq:rar-cor-inprod-tilrr}\end{equation}
However, computing according to equations
(\ref{eq:rar-cor-inprod-sr})~and~(\ref{eq:rar-cor-inprod-tilrr}) gives
methods that first of all take an extra inner product, and secondly,
turn out to be  extremely numerically unstable\footnote
{In fact, even on small, well-conditioned problems
they stall or diverge within a few iterations.}.
Therefore we
substitute for the first method
	\[	{w_{ii+1}\over w_{i+1i+1}}=
		{1\over k_{i+1i}}\left(
		{u_{ii+1}\over u_{i+1i+1}}-{v_{ii+1}\over v_{i+1i+1}}
		\right) \]
and interchanging the roles of $u$ and~$v$ for the second choice of~$w$
	\[ {w_{ii+1}\over w_{i+1i+1}}=
		{1\over h_{i+1i}}\left(
		{v_{ii+1}\over v_{i+1i+1}}-{u_{ii+1}\over u_{i+1i+1}}
		\right). \]
Using some elementary relation about the Hessenberg matrices
$H$~and~$K$, for instance
\[ \tilde r_i^tAr_{i+1}=\tilde r_{i+1}^tr_{i+1}h_{i+1i} \]
we then find for both choices of~$W$
\begin{equation}
	{\tilde r_{i+1}^tAr_{i+1}\over \tilde r_{i+1}^tr_{i+1}}=
		{s_{i+1}^tAr_{i+1}\over s_{i+1}^tr_{i+1}}+
		{u_{ii+1}\over u_{i+1i+1}}-{v_{ii+1}\over v_{i+1i+1}}.
	\label{eq:rar-cor-final}\end{equation}
The normalized polynomial coefficients $u_{ii+1}/u_{i+1i+1}$,
$v_{ii+1}/v_{i+1i+1}$~can be calculated recursively. For instance,
	\begin{equation}
		{u_{ii+1}\over u_{i+1i+1}}={u_{i-1i}\over u_{ii}}
		-h_{ii}.
	\label{eq:u-ratio}\end{equation}
but in practice we see no difference in numerical stability between
using equation~(\ref{eq:rar-cor-final}) directly, and modifying it
with~(\ref{eq:u-ratio}).

\Level 1 {Standard presentation of Polynomial Squaring methods}
\label{sec:cgs-2term}

\def\B#1#2{\fbox{$\,#1$}_{#2}}

We will now present Polynomial Squaring methods based on coupled
two-term recurrences.

Denote in preconditioned bi-conjugate gradient-type methods left and right
residuals by~$s_i$,~$r_i$ and left and right search directions
by~$q_i$,~$p_i$, and let
$M$~be the preconditioning matrix.

Assume that the left and right sequence are computed
from tridiagonal Hessenberg matrices.
The residuals and search directions are computed as
\[ r_{i+1}=r_i-Ap_i\ar_i,\qquad p_{i+1}=M\inv r_{i+1}+p_i\br_i, \]
and the left sequences of residuals and search directions
are computed as
\[ s_{i+1}=s_i-A^tq_i\al_i,\qquad
	q_{i+1}=M\invt s_{i+1}+q_i\bl_i, \]
where $\al_i$,~$\bl_i$ and $\ar_i$,~$\br_i$
may be different for the left and right sequences.
The coefficients for the right sequence are computed in the
traditional manner
\[ \ar_i=s_i^tM\inv r_i/q_i^tAp_i, \qquad
	\br_i=s_{i+1}^tM\inv r_{i+1}/s_i^tM\inv r_i. \]
For the Conjugate Gradient Squared 
method we choose
\[ \al_i=\ar_i\equiv\alpha_i,\qquad \bl_i=\br_i\equiv\beta_i. \]
For the BiConjugate Gradient Stabilized
we make a different choice, where the left sequence~$s_i$
satisfies a two-term recurrence,
namely determined by a steepest descent method, giving
\[ \al_i\not=\ar_i, \bl_i=0. \]

There exist
polynomials (see lemma~\ref{lemma:PR-polynomials})
$\rho_i$, $\pi_i$, $\sigma_i$, $\chi_i$
of degree~$i-1$ such that
\[ r_i=\rho_i(AM\inv)r_1,\quad p_i=M\inv\pi_i(AM\inv)r_1,\]
and
\[ s_i=\sigma_i(A^tM\invt)s_1,\quad	q_i=M\invt\chi_i(A^tM\invt)s_1. \]
We find relations for the polynomials 
(again, see lemma~\ref{lemma:PR-polynomials})
\[ \rho_{i+1}(x)=\rho_i(x)-x\pi_i(x)\alpha_i,\qquad
	\pi_{i+1}=\rho_{i+1}+\pi_i\beta_i. \]

\iffalse
In the following derivation of the cgs and bcgs methods, the
polynomials $\rho_i$ and~$\pi_i$ will be taken to refer to the left
sequences $s_i$ and~$q_i$, and we will not explicitly talk about the
polynomials of the right sequences.\fi

For cgs the polynomials for the left and right sequences are identical.
In bcgs we base the left sequence on the steepest descent method.
This implies that the search directions are identical to the
residuals, and for the left polynomials we find
\begin{equation}
    \sigma_{i+1}(x)=\sigma_i(x)-x\chi_i\al_i,
    \qquad \chi_{i+1}=\sigma_{i+1}.
    \label{eq:bcgs-left-poly}\end{equation}

\Level 2 {Vector recurrences}

The derivation of the squared residuals
$\sigma_i(\MA)M\inv r_i$, and various auxiliaries,
makes extensive use of the relations of \eqref{eq:rp-poly-recurrence}
between polynomials.
For compactness of the exposition we will mostly omit
the polynomial argument~$\AM$, that is, if we write~$\rho_i$ without
an argument, an argument of~$\AM$ is implicitly assumed.

First a strictly auxiliary quantity:
\begin{eqnarray*}
	\B1{i+1}&\buildrel D\over=&\chi_i(\MA)M\inv r_{i+1}\\
	&=&   \chi_i(\MA)M\inv r_i-\chi_i(\MA)\MA p_i\ar_i\\
	&=&	\chi_i(\MA)M\inv r_i-\MA\chi_i(\MA)p_i\ar_i \\
	&=&	\begin{cases} \B3i-\MA\B4i\ar_i&CGS\cr
			\B2i-\MA\B4i\ar_i&BiCGstab\cr\end{cases}
	\end{eqnarray*}
Next the squared residual:
\begin{eqnarray*}
\B2{i+1}&\buildrel D\over=&\sigma_{i+1}(\MA)M\inv r_{i+1}\\
	&=&	 (\sigma_i(\MA)-\MA\chi_i(\MA)\al_i)M\inv r_{i+1}\\
	&=&	\sigma_i(\MA)M\inv r_i-\sigma_i(\MA)\MA p_i\ar_i\\
	& &		\hphantom{\sigma_i(\MA)M\inv r_i}
		-\MA\chi_i(\MA)M\inv r_{i+1}\al_i\\
	&=&	\B2i-\MA\sigma_i(\MA)p_i\ar_i-\MA\B1{i+1}\al_i\\
	&=&\begin{cases}
		\B2i-\MA(\B3i\ar_i+\B1{i+1}\al_i)&CGS\cr
		\B2i-\MA(\B4i\ar_i+\B1{i+1}\al_i)&BiCGstab\cr\end{cases}
	\end{eqnarray*}
where we used that
\begin{eqnarray*}
    &\rlap{$\sigma_i(\MA)p_i=\sigma_i(\MA)M\inv\pi_ir_1$}\\
    &=&\begin{cases}M\inv\sigma_i\pi_ir_1=M\inv\pi_i\sigma_ir_1
                         =\pi_i(\MA)M\inv\sigma_ir_1\cr
              \qquad=\pi_i(\MA)M\inv r_i=\B3i    &CGS\cr
              \chi_i(\MA)p_i=\B4i	&BiCGstab\cr\end{cases}
\end{eqnarray*}
The next quantity is not needed for BiCGstab, since 
by $\chi_i\equiv\sigma_i$ it is equal to the previous:
\begin{eqnarray*}
\B3{i+1}&\buildrel D\over=&
		\chi_{i+1}(\MA)M\inv r_{i+1}\\
	&=&	\sigma_{i+1}(\MA)M\inv r_{i+1}
			+\chi_i(\MA)M\inv r_{i+1}\bl_i\\
	&=&	\B2{i+1}+\B1{i+1}\bl_i
			\qquad\hbox{CGS only}
	\end{eqnarray*}
For the squared search directions we find
\begin{eqnarray*}
\B4{i+1} &\buildrel D\over=& \chi_{i+1}(\MA)p_{i+1}\\
	&=&\begin{cases}
		\sigma_{i+1}(\MA)p_{i+1}+\chi_i(\MA)p_{i+1}\bl_i
			&CGS\cr
		\sigma_{i+1}(\MA)p_{i+1}
			&BiCGstab\cr\end{cases}\\
\noalign{(now, for CGS use
\begin{eqnarray*}
    \sigma_{i+1}(\MA)p_{i+1}&=&M\inv\sigma_{i+1}\pi_{i+1}r_1
    =M\inv\pi_{i+1}\sigma_{i+1}r_1\\
    &=&\pi_{i+1}(\MA)M\inv r_{i+1}=\B3{i+1};\end{eqnarray*}
for BiCGstab use
\begin{eqnarray*}
    \sigma_{i+1}(\MA)p_{i+1}
    &=&\sigma_{i+1}(\MA)(M\inv r_{i+1}+p_i\br_i)\\
    &=&\B1{i+1}+(\sigma_i(\MA)+M\inv A\chi_i(\MA)\al_i)p_i\br_i\\
    &=&\B1{i+1}+(\chi_i(\MA)+M\inv A\chi_i(\MA)\al_i)p_i\br_i
    \end{eqnarray*}
to find:)
}
	\B4{i+1}&=& \begin{cases}
		\B3{i+1}+\chi_i(\MA)
			(M\inv r_{i+1}+p_i\br_i)\bl_i\cr
		\qquad = \B3{i+1}+(\B1{i+1}+\B4i\br_i)\bl_i
			&CGS\cr
		\B1{i+1}+(\B4i+\MA\B4i\al_i)\br_i
			&BiCGstab\cr\end{cases}
	\end{eqnarray*}

In order to start the algorithm off, we note that from 
$\rho_1(M\inv A)r_1=\nobreak r_1$:
\[ \rho_1(x)=\sigma_1(x)=\pi_1(x)=\chi_1(x)=1, \]
giving
\[ \B21=\B31=\B41=M\inv r_1,  \]
and we enter the iteration loop by computing~$\B1{i+1}$ for~$i=\nobreak1$.

\begin{question}
Derive the solution update.
\end{question}
\begin{question}
Derive error estimate in each iteration.
\end{question}

\Level 2 {Work estimates}

The amount of work per iteration
for both methods consists of two matrix-vector
products and preconditioner solves, 
performed in the computation of $\B1{i+1}$ and~$\B2{i+1}$.
In BiCGstab,
the vector $\MA\B4i$, computed for the construction of~$\B1{i+1}$,
can be reused in the construction of~$\B4{i+1}$.
Additionally, for cgs there are 6~vector
additions or vector-plus-scalar-times-vector operations, for bcgs
there are 4~such operations. Updating the iterate takes one extra
vector operation (analogous to updating~ $\B2i$) for cgs and two
(corresponding to update $\B1i$~and~$\B2i$) for bcgs.

\Level 2 {Computation of coefficients in CGS}

In CGS, we take the same update for the left and right sequence,
so first of all $\al_i\equiv\ar_i$ and~$\bl_i\equiv\nobreak\br_i$.

For the computation of the $\alpha$ and~$\beta$ scalars
we need the $\tilde r_i^tM\inv r_i$ and $\tilde p_i^tAp_i$
inner products. Using the vectors derived above,
\[ \tilde r_i^tM\inv r_i=(\sigma_i(A^tM\invt)\tilde r_1)^tM\inv r_1 =
   \tilde r_1^t\sigma_i(\MA)M\inv r_i =\tilde r_1^t\B2i, \]
and
\[ \tilde p_i^tAp_i=\tilde r_1^t\chi_i(\MA)M\inv Ap_i
     =\tilde r_1^tM\inv A\tilde \pi_i(\MA)p_i=\tilde r_1\MA\B4i. \]
Note that $\MA\B4i$ is computed in the above algorithm.

\Level 2 {Full CGS algorithm}

We give the full CGS algorithm in algorithm~\ref{alg:cgs}.
\begin{algorithm}{Conjugate Gradients Squared}
\label{alg:cgs}
Let $A$, $M$, and $x_1$ be given. 
%r1 = A*x-b;
Compute $r_1=Ax_1-b$ 
% rr = M\inv r1;
and $p_1=M\inv r_1$.
Now iterate for $i=1,\ldots$:
\begin{itemize}
\item
%  rr_dot = r1'*rr; if it>1, beta = rr_dot/rr_dotp; end;
Compute one dot product $\rho_i = r_1^tM\inv r_i$,
and, if $i>1$, $\beta_i=\rho_i/\rho_{i-1}$.
\item If $i>1$, compute a temporary $\B3i$:
%  pr = rr + pr1*beta;
\[ s = r_i+ q_{i-1}\beta_i. \]
\item Update the squared search directions $\B4i$ for $i>1$:
%  pp = pr + (pr1+pp*beta)*beta;
\[ p_i = s+(q_i+p_i\beta_i)\beta_i. \]
\item Compute the product $M\inv Ap_i$ with the search directions,
%  mapp = M \ (A*pp);
and use it for the scalars
%  pap_dot = r1'*mapp;
\[ \pi_i = r_1^tM\inv Ap_i, \qquad
%  alpha = rr_dot/pap_dot;
\alpha_i=\rho_i/\pi_i. \]
\item Update the auxiliary quantity $\B1i$, using the
product $M\inv Ap_i$ already computed:
%  pr1 = pr - M\ (A*pp*alpha);
\[ q_i=p_i-M\inv Ap_i\alpha_i. \]
\item 
%  search = (pr+pr1)*alpha;
Compute the update vector 
\[ t=(p_i+q_i)\alpha_i, \]
%  x = x-search;
and use it to update the solution \[ x_{i+1}=x_i-t, \]
%  rr = rr - M\ (A*search);
and residual $\B2{i+1}$:
\[ r_{i+1}=r_i-M\inv A*t,\]
where for the residual update we perform the second multiplication
by~$M\inv A$.
\end{itemize}
\end{algorithm}

We give the algorithm as Matlab code in section~\ref{sec:matlab-cgs}.

\Level 2 {Computation of coefficients in BiCGstab}

\Level 1 {Three-term recurrence derivation Conjugate Gradient Squared}

For expository purposes it is most convenient to derive first
the version of Conjugate Gradients Squared based on three-term
recurrences, and without preconitioning.
We present the practically preferred versions
of CGS and BiCGstab,
based on coupled two-term recurrences, and with preconditioning,
in section~\ref{sec:cgs-2term}.

Naturally, we do not wish to build the whole sequence~$S^{(n)}$ 
(see~\eqref{eq:cgs-mat})
in the $n$-th step.
It turns out that of each $S^{(n)}$ we need only four
elements\footnote{This presentation is in the context of 
a tridiagonal matrix~$H$ generating the iterative method
to be squared, as is the case with BiCG.
More general methods may be possible.}.

The residual polynomials~$\pi_n$ are related by
\begin{equation}
	\pi_{n+1}(t)h_{n+1n} + \pi_n(t)h_{nn} + \pi_{n-1}(t)h_{n-1n}
	= t \pi_n(t); 
	\label{cgs:pi:relation}\end{equation}
(see lemma~\ref{lemma:polynomial:recurrence}).
From this we find that
\[ S^{(n+1)}h_{n+1n} + S^{(n)}h_{nn} + S^{(n-1)}h_{n-1n} = AS^{(n)} \]
and combining this with~(\ref{cgs:SH:relation}) we find the following
computation:
\begin{eqnarray*}
	s^{(n+1)}_{n-1} h_{n+1n} &=& As^{(n)}_{n-1} -
		( s^{(n)}_{n-1} h_{nn} + s^{(n-1)}_{n-1} h_{n-1n} ) \\
	s^{(n+1)}_{n} h_{n+1n} &=& As^{(n)}_{n} -
		( s^{(n)}_{n} h_{nn} + s^{(n-1)}_{n} h_{n-1n} ) \\
	s^{(n+1)}_{n+1} h_{n+1n} &=& A s^{(n+1)}_{n} -
		( s^{(n+1)}_{n} h_{nn} + s^{(n+1)}_{n-1} h_{n-1n} ) \\
	s^{(n+1)}_{n+2} h_{n+2n+1} &=& A s^{(n+1)}_{n+1} -
		( s^{(n+1)}_{n+1} h_{n+1n+1} + s^{(n+1)}_{n} h_{nn+1} )
\end{eqnarray*}
where the left hand sides are computed in the order stated.
In the last two steps we perform a matrix-vector product; the
results of these can be reused in the first two steps of the next iteration.
We see that this method requires two matrix-vector products, but
neither is with the transpose of the matrix.

For the computation of the coefficients of~$H$ we have to go back to
the Lanczos method. In section~\ref{sec:three-term} we saw that we need the
expressions (now denoting the left sequence of residuals by~$\tilde
r_n$)
\[ \tilde r_n^tAr_n\quad (\ref{eq:d-lanczos}),\qquad
   \tilde r_n^tr_n\quad \hbox{(\ref{eq:d-lanczos}) 
				and (\ref{eq:u-lanczos})}. \]
From the above it is easy to see that
\[ \tilde r_n^tr_n = \tilde r_1^t\pi_n^2(A)r_1 = \tilde r_1^ts^{(n)}_n,
	\qquad
   \tilde r_n^tAr_n = \tilde r_1^tA\pi_n^2(A)r_1 =
	\tilde r_1^tAs^{(n)}_n. \]
Thus the left sequence can be eliminated completely, and only its
first element is ever needed.
Using the above two expression, the elements of~$H$ can be computed,
for instance as in equation~(\ref{eq:three-term:H}).

\Level 2 {Computation of coefficients in CGS}

In the conjugate gradient squared method, the choices $\rho_i\equiv
\sigma_i$ and~$H=K$ are made, where the tridiagonal matrix~$H$ is
chosen as the one generated by the Lanczos method. Denoting the left
and right residuals of the
Lanczos method by~$s_i$,~$r_i$, we need  the values of
	\begin{equation} s_i^tr_i=
		\bigl(\rho_i(A^t)r_1\bigr)^t\bigl(\rho_i(A)r_1\bigr)=
		r_1^t\rho_i^2(A)r_1=r_1^tr_i^{(i)}, 
	\label{eq:lanczos-rr}\end{equation}
and	\begin{equation} s_i^tAr_i=
		\bigl(\rho_i(A^t)r_1\bigr)^tA\bigl(\rho_i(A)r_1\bigr)=
		r_1^tA\rho_i^2(A)r_1=r_1^tAr_i^{(i)}.
	\label{eq:lanczos-rar}\end{equation}
The coefficients of~$H$ then follow from
	\begin{equation}
	h_{ii}={s_iAr_i\over s_i^tr_i},\qquad
	h_{i-1i}={s_{i}r_{i}\over s_{i-1}^tr_{i-1}}
		h_{ii-1},\qquad
	h_{i+1i}=-h_{ii}-h_{i-1i}.
	\label{eq:H-from-rr}\end{equation}
(For a survey of such identities, see~\cite{Eij:lawn51}.)

\Level 2 {Computation of coefficients in BiCGstab}

The BiCGstab algorithm is based on taking a different recurrence in
$i$~direction from the one in $j$~direction. While the latter one is
still based on the matrix~$H$ from the Lanczos method, the former
recurrence takes steepest descent steps. Thus, we perform the
steepest descent update 
	\[ r_{i+1}^{(i+1)}=r_i^{(i+1)}+\omega_iAr_i^{(i+1)} \]
with 	\[ \omega_i=-{r_i^{(i+1)^t}Ar_i^{(i+1)} \over
		(Ar_i^{(i+1)})^t(Ar_i^{(i+1)}) }. \]
In order to write this as a three-term recurrence
	\[ r_{i+1}^{(i+1)}k_{i+1i}+r_{i}^{(i+1)}k_{ii}+r_{i-1}^{(i+1)}k_{i-1i}
		=Ar_{i}^{(i+1)} \]
we choose $k_{i-1i}=0$, and
	\[ k_{i+1i}=-k_{ii}=1/\omega_i. \]

\begin{comment}
\Level 1 {CGS with search directions; alternative algorithm}

In fact, this one may very well be wrong.

It is also possible to state CGS with search directions.
For this we consider the sequences $r_n$ and~$p_n$ generated by
BiConjugate Gradients, and the corresponding polynomials $\rho_n(\cdot)$
and~$\pi_n(\cdot)$ such that
\[ r_n = \rho_n(A)r_1,\qquad p_n=\pi_n(A)r_1, \]
and for the left sequence
\[ \tilde r_n = \rho_n(A^t)r_1,\qquad \tilde p_n=\pi_n(A^t)r_1. \]
With these we define the squared sequences
\[ r_{nm} = \rho_{n+m}r_1,\qquad p_{nm}= \pi_{n+m}r_1. \]

Now suppose, inductively, that $r_{nn}$, $p_{nn}$, $r_{nn-1}$, and~$p_{nn-1}$
are available.
We now compute
\begin{eqnarray*}
    r_{n+1n-1}&=&r_{nn-1}  -Ap_{nn-1}{\tilde r_n^tr_n\over \tilde p_n^tAp_n}\\
    p_{n+1n-1}&=&r_{n+1n-1}-p_{nn-1}{\tilde r_n^tr_n\over \tilde r_{n-1}^tr_{n-1}}\\
    r_{n+1n}  &=&r_{nn}    -Ap_{nn}{\tilde r_n^tr_n\over \tilde p_n^tAp_n}\\
    p_{n+1n}  &=&r_{n+1n}  -p_{n+1n-1}{\tilde r_n^tr_n\over \tilde r_{n-1}^tr_{n-1}}\\
    r_{n+1n+1}&=&r_{n+1n}  -Ap_{n+1n}{\tilde r_n^tr_n\over \tilde p_n^tAp_n}\\
    p_{n+1n+1}&=&r_{n+1n+1}-p_{n+1n}{\tilde r_{n+1}^tr_{n+1}\over \tilde r_n^tr_n}
\end{eqnarray*}
The inner products, as above, follow from such identities
as \[ \tilde r_n^tr_n = \tilde r_1^tr_{nn}. \]
If the product $Ap_{n+1n}$  is saved for the next iteration, only
two matrix-vector products are needed per iteration,
as for the previous implementation of the method.

\end{comment}

