\Level 0 {Iterative refinement}

Given a system $A\bar x=f$ and $M\approx A$,
set $x_1=M\inv f$, and define $r_1=Ax_1-f$ and $e_1=x_1-\bar x=A\inv r_1$,
since $\bar x=x_1-A\inv r_1$, we iterate $x_2=x_1-M\inv r_1$.

The terms iterative refinement is also used in the context of direct 
methods, where a system is solved a small number of times,
without further parameters in the correction since the full matrix
solve is close to the exact solve.

\begin{notes}
See section~\ref{app:iterative-refinement} for a matlab script that simulates 
an exact solve contaminated by roundoff.
\end{notes}

\Level 1 {Propagation of error and residual}

For the error $e_n=x_n-x$ (where $x$~is the exact
solution of $Ax=f$) and the residual $r_n=Ax_n-b$,
we have $Ae_n=r_n$, so
\begin{equation}
	e_{n+1}-e_n=-M\inv Ae_n \Rightarrow e_{n+1}=(I-M\inv A)e_n=Ge_n
	\label{error:propagation}\end{equation}
and
\begin{equation}
	r_{n+1}-r_n=AM\inv Ae_n=AM\inv r_n \Rightarrow r_{n+1}=(I-AM\inv)r_n
	\label{residual:propagation}\end{equation}
The operators are similar:
\[ I-AM\inv = M(I-M\inv A)M\inv = A(I-M\inv A)M\inv. \]

\Level 0 {Rewrites of Stationary Iteration}

The basic equation for polynomial iterative methods is
\[ x_{n+1}-x_1\in {\rm Span}\{M\inv r_1,\ldots,M\inv r_n\} \]
where $M$~is a preconditioner matrix, or equivalently,
\[ x_{n+1}-x_n\in {\rm Span}\{M\inv r_1,\ldots,M\inv r_n\}. \]
For stationary iteration this becomes
\[ x_{n+1}-x_n=-M\inv r_n. \]
Equivalently, with the splitting $A=M-N$ and
$G=I-M\inv A=M\inv N$ the error propagation operator 
(equation~\ref{error:propagation}),
\[ x_{n+1}=Gx_n+(I-G)A\inv b=Gx_n+M\inv b.\]

The update equation $x_{n+1}-x_n=-M\inv r_n$ is often written as
$M(x_{n+1}-x_n)=-rn$. Rearranging the terms in this gives
\begin{eqnarray*}
 Mx_{n+1}&=&Mx_n-r_n\\
         &=&Ax_n-Nx_n-(Ax_n-b)\\
         &=&Nx_n+b
\end{eqnarray*}
To turn this around: the scheme $Mx_{n+1}=Nx_n+b$ is an iterative
method for the system $(M-N)x=b$. As a corollary, for any~$B$
\[ (M+B)x_{n+1}=(N+B)x_n-b \] is also a method for $(M-N)x=b$.

If the splitting involves a scaling, for instance in SOR
\[ (\omega\inv D+L)x_{n+1}=(\omega\inv D+U)x_n+b \]
this is also written as
\[ ( D+\omega L)x_{n+1}=( D+\omega U)x_n+\omega b. \]

\Level 0 {Stationary Iteration with varying solver}

Let varying splittings of the coefficient matrix be given, say
\[ A= M_1-N_1=M_2-N_2=M_3-N_3=\ldots\]
and let iterates be generated by 
\begin{eqnarray*}
x_{i+1} &=& x_i- M_1\inv r_i\\
x_{i+2} &=& x_{i+1}- M_2\inv r_{i+1}\\
x_{i+3} &=& x_{i+2}- M_3\inv r_{i+2}\\
&\ldots&
\end{eqnarray*}

This process can be summarised as
\[ x_{i+k} = x_i-M_{\sigma,k}\inv r_i \]
where $M_{\sigma,k}\inv A$ is inductively determined by
\begin{eqnarray*}
x_{i+k}&=&x_{i+k-1}-M_k\inv r_{i+k-1}=x_{i+k-1}-M_k\inv(Ax_{i+k-1}-b)\\
    &=&(I-M_k\inv A)x_{i+k-1}+M_k\inv b\\
    &=&(I-M_k\inv A)(x_i-M_{\sigma,k-1}\inv r_i)+M_k\inv b\\
    &=&x_i-M_k\inv(I-AM_{\sigma,k-1}\inv)r_i +M_{\sigma,k-1}\inv r_i
\end{eqnarray*}
that is,
\begin{equation}
  M_{\sigma,k}\inv = M_{\sigma,k-1}\inv+M_{k}\inv(I-AM_{\sigma,k-1}\inv)
  \label{eq:compound-prec}\end{equation}
This also describes the compound application process
$x_{i+k}=x_i+t_{i,k}$ with
\[ t_{i,1}=M_1\inv r_i,\qquad t_{i,k} = t_{i,k-1}+M_k\inv(r_i-At_{i,k-1}). \]

Here is $M_{\sigma,3}\inv$ explicitly:
 \[ M_{\sigma,3}\inv=\sum_{i=1} M_i\inv
    -\sum_{i_1>i_2}M_{i_1}\inv AM_{i_2}\inv + M_3\inv AM_2\inv AM_1\inv. \]
The error propagator is
\[ I-M_{\sigma,3}\inv A = (I-M_3\inv A)(I-M_2\inv A)(I-M_1\inv A) \]
with obvious extensions to more terms. Proof using 
equation~(\ref{error:propagation}). 
Here is another equivalent formula:
\begin{equation}
   M_{\sigma,3}\inv = 
   M_3\inv\left[I+N_3M_2\inv[I+N_2M_1\inv]
           \right]
	\label{eq:multiplicative:stat:mult}\end{equation}
again with obvious extensions to more terms;
this is easily seen to be equivalent to~(\ref{eq:compound-prec}).

From \eqref{eq:multiplicative:stat:mult} we also get for the case $x_0=0$:
\begin{equation}
    x_k=M_{\sigma,k}\inv f\Rightarrow M_kx_k = f+N_kM_{k-1}\inv[\cdots]f.
    \label{eq:x-from-zero}\end{equation}

\begin{example}{Derive SSOR from forward/backward SOR}
Construct the forward and backward SOR splittings
\begin{eqnarray*} A&=&D+L+U \\
    &=& (D+L)-(-U) = M_1-N_1\\
    &=& (D+U)-(-L) = M_2-N_2
\end{eqnarray*}
and combine them to form
\begin{eqnarray*}
 M\inv_{\sigma,2}
        &=& M_1\inv+M_2\inv-M_2\inv AM_1\inv=
		M_2\inv N_2M_1\inv+M_2\inv=M_2\inv(N_2+M_1)M_1\inv \\
        &=& (D+U)\inv D(D+L)\inv
\end{eqnarray*}
\end{example}

\begin{remark}{Symmetry and definiteness of compound preconditioners}
Suppose $A$ and~$M_2$ are symmetric and $M_1=M_3^t$. Then
 \begin{equation}
 A\inv-M_{\sigma,3}\inv = (I-M_3\inv A)(A\inv -M_2\inv)(I-AM_1\inv)
                \label{eq:compound-3-prec}\end{equation}
so the resulting preconditioner is symmetric.
This holds for any number of factors. Note that $M_{\sigma,3}$ is symmetric
but not necessarily positive definite, even if $A$ and $M_2$ are.
A sufficient condition is (with inequalities in the pd-sense):
\[ M_2\geq A\Rightarrow A\inv-M_2\inv\geq 0\Rightarrow 
    A\inv-M_{\sigma,3}\inv\geq0 \Rightarrow M_{\sigma,3}\geq A\geq 0.\]
\end{remark}

