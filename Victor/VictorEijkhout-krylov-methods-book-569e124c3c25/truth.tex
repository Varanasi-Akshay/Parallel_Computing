\documentclass[11pt]{artikel3}

\input preamble

\begin{document}

\title{The Truth about Conjugate Gradients}
\author{Victor Eijkhout 
\\ Texas Advanced Computing Center \\ The University of Texas at Austin 
\\ Austin, TX 78758 \\ {\tt eijkhout@tacc.utexas.edu} }
\maketitle

\begin{abstract}
This paper brings together a bundle of facts about polynomial
iterative methods for solving linear systems of equations.
\end{abstract}

\tableofcontents
\pagebreak
\listofstatements
\pagebreak

\begin{Outline}
\Level 0 {Executive summary}

\input exec

\Level 0 {Introduction}
\input introduction

\Level 0 {Definition and elementary properties of polynomial
iterative methods}
\input polynomial

\Level 0 {Hessenberg matrices}
\label{sec:hessenberg}
\input hessenberg

\Level 0 {Characterisation of polynomial iterative methods}
\input characterisation

\Level 0 {Minimization}\label{sec:minimisation}
\input minimisation

\Level 0 {Orthogonalization}
\label{ch:ortho}

Section~\ref{sec:minimisation} discusses the relation between orthogonality
and minimization, thus providing the theoretical justification for 
letting iterative methods be based on orthogonalization. Here we
look more constructively at orthogonalization, investigating in particular
the conditions under which orthogonality can be satisfied.

Methods such as the Conjugate Gradients algorithm fall under the 
following broad description:
\begin{itemize}\item
Given a matrix $A$, a Hessenberg matrix~$H_{[n,n-1]}$,
and a sequence~$R_n$ that satisfies some orthogonality condition,
related by \[ AM\inv R_{n-1}=R_nH_{[n,n-1]}, \]
\item Find a way to extend
\[ R_n\rightarrow R_{n+1},\quad H_{[n,n-1]}\rightarrow H_{[n+1,n]} \]
while preserving the orthogonality condition.
\end{itemize}
The study of orthogonalization in iterative methods is then
the study of the conditions under which
such an extension is feasible.

\Level 1 {Types of orthogonality}
\label{sec:ortho-types}

The simplest choice of orthogonalisation is to let a residual
sequence~$R$, generated by $AM\inv R=RH$, be orthogonal under some
inner product. However, we will also consider the question of how to
let~$R$ be semi-orthogonal to another, arbitrary, sequence~$S$.

As indicated above, we extend the $R$ series by suitable construction
of the Hessenberg matrix~$H$, which is equivalent to constructing the
combinations~$U$ such that $R=KU$ where $K=\KmethodAv{AM\inv}{r_1}$.

\begin{ddefinition}{General semi-orthogonality condition}
The general semi-orthogonality condition for sequences $S,R$ is
\begin{equation} \hbox{$S^tN R$ is nonsingularly lower triangular,}
    \label{eq:cond-ortho}\end{equation}
where $N$~is a general inner product matrix.
\end{ddefinition}

In practice, $N$~will be a combination of $A$
and~$M\inv$, and in particular either $M\inv$ (corresponding to
most simple methods such as CG and BiCG) 
or~$AM\inv$ (corresponding to residual minimising methods such as GMRES).

For the relation of this condition to minimisation,
see section~\ref{sec:abstract-true-min}, and in particular 
remark~\ref{lemma:ramr-uptri}. From this condition will follow the
scalars in the implementation of the iterative methods; see
section~\ref{sec:scalars-nonsymm} for the basic methods.

In all of the following, we will consider three cases.
\begin{description}
\item[Lanczos orthogonalization] This is the case where $S\not\equiv R$
and $S$~itself is based on a Krylov sequence;
this leads to the BiConjugate Gradient and Quasi Minimum Residual methods.
\item[Arnoldi orthogonalization] This is the case where $S\equiv R$;
we distinguish two subcases:
\begin{description}
\item[Symmetric case] where $A$ and $M$ are symmetric, 
which leads to the Conjugate Gradient and Minimum Residual methods; and
\item[Nonsymmetric case] where at least one of $A$ and $M$  is not symmetric,
which leads to methods such as Orthodir and GMRES.
\end{description}
\end{description}

\begin{remark}
\Eqref{eq:cond-ortho} can also give rise to methods that we do not
consider further. For instance, choosing $S=N=I$ makes $R$ lower triangular,
and $AM\inv R=RH$ becomes a sort of Gaussian elimination method.
This method was proposed by Hessenberg~\cite{Hessenberg:thesis}
\end{remark}

\Level 1 {Theoretical conditions on orthogonality}
\label{sec:ortho}

We will inductively satisfy the orthogonality condition~(\ref{eq:cond-ortho})
by constructing~$r_{n+1}$ to satisfy
\begin{equation} S_n^tN r_{n+1}=0, \qquad s_{n+1}^tN r_{n+1}\not=0
    \label{eq:sMr-semi-ortho}\end{equation}
for all~$n$.
Writing the $n+\nobreak1$-st column of the equation $AM\inv R=RH$ as
    \begin{equation} r_{n+1}h_{n+1,n}=AM\inv r_n-R_nh_n,
    \label{eq:h-gen-ortho}\end{equation}
we find that the conditions in \eqref{eq:sMr-semi-ortho}
%a sufficient condition for $S_n^tM\inv r_{n+1}=\nobreak0$
are equivalent to 
    \begin{equation} S_n^tN AM\inv r_n=S_n^tN R_nh_n
        \qquad\hbox{and}\qquad
                AM\inv r_n\not=R_nh_n.
    \label{eq:h-solve-ortho}\end{equation}
We arrive at the following algorithm, which is presented more for
theoretical than for practical purposes.

\begin{algorithm}{Semi-orthogonalise Krylov sequence to arbitrary sequence}
\label{SR:algorithm}
Let $A$, $M$, $N$, $r_1$ and a sequence~$S$ be given.
We construct a Hessenberg matrix~$H$
and a sequence~$R$ such that $AM\inv R=RH$,
and $S^tN R$ is lower triangular.
\begin{itemize}
\item Solve the $n\times1$ vector~$h_n$ from
\[ S_n^tN AM\inv r_n=S_n^tN R_nh_n. \]
\item Then pick any nonzero value for~$h_{n+1,n}$, and define $r_{n+1}$ from
\[ r_{n+1}h_{n+1n}=AM\inv r_n-R_nh_n. \]
\end{itemize}
\end{algorithm}

We will now present the conditions for this process to be defined.
First we observe that we are only interested in a particular form
of the Hessenberg matrix~$H$:
\begin{itemize}
\item we need $h_{n+1n}\not=0$, since otherwise we are in
an invariant subspace $R_n$, which is typically associated with the
iteration
being finished or breaking down
(see section~\ref{sec:termination}); and
\item we need $h_{1:n,n}\not\equiv0$, since otherwise
the zero column sum condition on~$H$ can not be satisfied.
\end{itemize}

\begin{lemma}
\label{lemma:next-r}
Let sequences $R_n$ and $S_n$ be given, such that
\[ AM\inv R_{n-1}=R_nH_{[n,n-1]} \]
and $S_n^tN R_n$ is non-singular lower triangular.
Then there exist $r_{n+1}\not=\nobreak0$ and $h_{*,n}$ 
with $h_{n+1,n}\not=\nobreak0$ and $h_{1:n,n}\not\equiv0$
such that
\begin{equation}
    AM\inv R_n=R_{n+1}H_{[n+1,n]},\quad\hbox{and}\quad S_n^tN r_{n+1}=0
    \label{eq:gen-h-req}\end{equation}
if and only if $S_n^tN AM\inv r_n\not=0$ and
$R_n$ is not $AM\inv$-invariant.
\end{lemma}

\begin{proof} Suppose that $S_n^tN R_n$ is nonsingular.
From \eqref{eq:h-gen-ortho} we find that
there exist $r_{n+1}$ and~$h_{n+1n}$ satisfying the
requirements in \eqref{eq:gen-h-req} above iff
\[ \exists_{h_n\equiv h_{[1:n,n]}}\colon
    S_n^tN(AM\inv r_n-R_nh_n)=0 \quad\hbox{and}\quad
    AM\inv r_n\not=R_nh_n. \]
With $S_n^tN AM\inv r_n\not=0$ we then solve $h_n$ from
\begin{equation} h_n= (S_n^tN R_n)\inv S_n^tN AM\inv r_n
    \label{eq:solve-next-h}\end{equation}
(see \eqref{eq:h-solve-ortho}),
which is not zero because of the $S_n^tN AM\inv r_n\not=0$
condition of the statement.

We are left to satisfy the condition~$AM\inv r_n\not=\nobreak R_nh_n$.
This last condition is equivalent to $R_n$~not being $AM\inv$-invariant:
if $R_n$~is not $AM\inv$-invariant, we have that 
$AM\inv r_n\not=R_nh_n$ for all~$h_n$;
if $R_n$~is $AM\inv$-invariant, there is a $k_n$ 
such that $AM\inv r_n=R_nk_n$, so we have
\[ S_n^tN AM\inv r_n=S_n^tN R_nk_n \Rightarrow 
    h_n=k_n \Rightarrow AM\inv r_n=R_nh_n. \]
This concludes the `if' part of the proof.

On the other hand, let $r_{n+1}$ and $h_{*,n+1}$ exist satisfying
\eqref{eq:gen-h-req}. We get
\begin{eqnarray*}S_n^tN AM\inv r_n&=&
    S_n^tN r_{n+1}h_{n+1n}+S_n^tN R_nh_{*,n}\\
    &=&0+S_n^tN R_nh_{*,n}\end{eqnarray*}
so, from the nonsingularity of $S_nN R_n$,
we find that $S_n^tN AM\inv r_n\not=0$.
Since $h_{n+1n}\not=0$, we find from \eqref{eq:h-gen-ortho} that
$AM\inv r_n\not=\nobreak R_nh_n$. By the above argument,
this implies that $R_n$ is not $AM\inv$-invariant.
\end{proof}

Substituting \eqref{eq:solve-next-h} into \eqref{eq:h-gen-ortho},
we find
\begin{equation}
 r_{n+1}h_{n+1n}=AM\inv r_n-R_n(S_n^tN R_n)\inv S_n^tN AM\inv r_n.
    \label{eq:rn+1:by-projection}\end{equation}
This states that $r_{n+1}$ is obtained, up to scaling,
by projecting $AM\inv r_n$ onto $R_n$,
and the result is $N$-orthogonal to~$S_n$.

\begin{comment}
A sufficient condition for lemma~\ref{lemma:next-r} would be 
for $S_n^tM\inv R_n$ to be non-singular,
but it is the lower triangular case we are interested in.
\end{comment}
From the result $S_n^tN r_{n+1}=\nobreak0$ we see that
algorithm~\ref{SR:algorithm} is almost inductively defined.  For it to
be fully defined we need one more condition, namely that the $r_{n+1}$
vector generated not satisfy $s_{n+1}^tN r_{n+1}=\nobreak0$. This
is called a breakdown condition for the algorithm, and we will
investigate it further in section~\ref{sec:breakdown:theory}.

We tie all of the above up in a theorem:

\begin{theorem}\label{cor:next-r}
Let $S_n$ be any sequence, and $R_n$~be a residual sequence, such that 
\[ \begin{cases}AM\inv R_{n-1}=R_nH_{[n,n-1]},
                \quad\hbox{$H_{[n,n-1]}$ irreducible}\cr
    \hbox{and $S_n^tN R_n$ is non-singular lower triangular},\end{cases} \]
and~$S_n^tN AM\inv r_n\not=0$.
We can extend $R$ and~$H$ to 
\[ \begin{cases}AM\inv R_n=R_{n+1}H_{[n+1,n]},
                \quad\hbox{$H_{[n+1,n]}$ irreducible}\cr
 \hbox{$S_{n+1}^tN R_{n+1}$ is non-singular lower triangular}\end{cases} \]
with $R_{n+1}$~a residual sequence iff $R_n$~is not $AM\inv $-invariant,
and the vector $r_{n+1}$ defined by \eqref{eq:rn+1:by-projection}
does not make the condition $s_{n+1}^tN r_{n+1}=0$ occur.
\end{theorem}
\begin{proof} Let $S_n^tN R_n$ be non-singular lower triangular, then by
the above lemma, and the condition $S_n^tN AM\inv r_n\not=0$,
we can extend $R$~and~$H$ iff $R_n$ is not $AM\inv$-invariant.
From \eqref{eq:rn+1:by-projection}, $r_{n+1}$ is determined up to its
norm; from lemma~\ref{lemma:next-r} we know that $H_{[n+1,n]}$ is such
that it can have zero column sums by proper choice of~$h_{n+1n}$.
Then $R_{n+1}$~is a residual sequence.

We have $S_n^tN r_{n+1}=0$, so if $s_{n+1}^tN r_{n+1}\not=0$,
$S_{n+1}^tN R_{n+1}$ is again nonsingular lower triangular.
\end{proof}

The condition that $r_{i+1}$ is orthogonal to the $i$-dimensional space
spanned by~$S_i$ is often called a `Galerkin condition',
or a `Petrov-Galerkin condition'.
The connection between 
Petrov-Galerkin conditions and minimisation is discussed 
in section~\ref{sec:minimisation}.

\Level 1 {Breakdown conditions}
\label{sec:breakdown:theory}

From theorem~\ref{cor:next-r} we see that there are three ways in which 
the process of generating an orthogonal residual sequence can halt.
\begin{itemize}
\item $R_n$ is $AM\inv$-invariant.
\item $S_n^tN AM\inv r_n=0$.
\item $S_n^tN r_n=0$.
\end{itemize}
We know from corollary~\ref{cor:solution} that the first
condition coincides with finding the solution to the linear system.
It remains to consider the second and third condition in more detail.
As an aside we note that, traditionally, iterative methods were
derived in unpreconditioned form, that is, $N=M=I$. In that case, the
third condition ceases to be a problem, since $r_n=0$ implies that the
solution has been found.

\Level 2 {Arnoldi orthogonalization under $M\inv$ inner product;
symmetric case}
\label{sec:symmetric-breakdown}

The Conjugate Gradient method is a case of Arnoldi orthogonalization
(that is, $S\equiv R$) in the symmetric case, and 
using the $M\inv$~inner product. 
Assuming nonsingularity -- and in particular under the assumption of
positive definiteness -- we have that $x^tAx\not=0$ 
and $x^tM\inv x\not=0$ for all nonzero~$x$.

Clearly, from symmetry, both $S_n^tN r_n=R_n^tM\inv r_n$ and
$S_n^tN AM\inv r_n=R_n^tM\inv AM\inv r_n$ are nonzero,
so the algorithm will not break down.

\Level 2 {Arnoldi orthogonalization under $M\inv$ inner product;
nonsymmetric case}

Methods such as OrthoDir use Arnoldi orthogonalization in the 
nonsymmetric case, with the $M\inv$~inner product.
Both the expression $S_n^tN r_n=R_n^tM\inv r_n$ 
and $S_n^tN AM\inv r_n=R_n^tM\inv AM\inv r_n$ can be zero;
in the special case of $M$~pd
--~in particular in the unpreconditioned case of $M=I$~--
only the second expression can be zero.
Thus this method can break down.

\Level 2 {Arnoldi orthogonalization under $AM\inv$ inner product}
\label{sec:breakdown-aminv}

The MinRes method  for symmetric systems, and the
GCR, OrthoMin, and GMRES methods for nonsymmetric systems, use Arnoldi 
orthogonalization,
and are equivalent to using the $N^t=AM\inv$ inner product
(Combine \eqref{eq:cond-ortho} and remark~\ref{lemma:ramr-uptri}).
Therefore, under the assumption of nonsingular~$A$,
they do not suffer from the second breakdown condition,
and, if $M$~is pd --~in particular in the unpreconditioned case of $M=I$~--
neither from the third.

However, in practice, breakdown is also dependent on the implementation of
the algorithm. As we show in section~\ref{sec:gcr}, 
algorithms such as GCR,
while mathematically equivalent to GMRES, suffer from breakdown
by the second condition, while GMRES does not.

\Level 2 {Lanczos orthogonalization}

Think hard about this one.

\Level 1 {General Arnoldi orthogonalization}
\label{sec:arnoldi-gen}

The Arnoldi method corresponds to the case $S\equiv R$ and $N=M\inv$,
so that $R^tM\inv R$ is diagonal. We prove the existence of the sequence~$R$.

\begin{llemma}{Existence of general Arnoldi orthogonalization}
\label{lemma:next-arnoldi}
Let a sequence $R_n$ be given, such that
\[ AM\inv R_{n-1}=R_nH_{[n,n-1]} \]
and $R_n^tM\inv R_n$ is non-singular lower triangular.
Then there exist $r_{n+1}\not=\nobreak0$ and $h_{*,n}$ 
with $h_{n+1,n}\not=\nobreak0$ and $h_{1:n,n}\not\equiv0$
such that
\begin{equation}
    AM\inv R_n=R_{n+1}H_{[n+1,n]},\quad\hbox{and}\quad R_n^tM\inv r_{n+1}=0
    \label{eq:gen-h-req}\end{equation}
if and only if $R_n^tM\inv AM\inv r_n\not=0$ and
$R_n$ is not $AM\inv$-invariant.
\end{llemma}

\begin{proof}
This is a special case of lemma~\ref{lemma:next-r} for $S\equiv R$ and
$N=M\inv$.
\end{proof}

\Level 1 {Arnoldi orthogonalisation and the Conjugate Gradients method}
\label{sec:arnoldi-cg}

\begin{question}
This section should use $M$.
\end{question}

The classical conjugate gradient method stems from the choice $S=R$
in an Arnoldi method, applied to a symmetric matrix~$A$.

\begin{lemma}\label{lemma:cg-tri}
The Arnoldi method corresponds to a transformation of~$A$
to upper Hessenberg form; if $A$ is symmetric this is a
tridiagonalization; if $A$ is positive definite the breakdown
condition will not occur.
\end{lemma}
\begin{proof} 
If $S=R$, the lower triangular matrix $S^tR$ is diagonal, and
$S^tAR=R^tAR=R^tRH$ is a transformation of $A$ to Hessenberg form.  If
$A$ is symmetric, $R^tAR=R^tRH=H^tR^tR$, so $H$ is of both upper and lower
Hessenberg form, hence tridiagonal.  The absence of breakdown follows
from the discussion in section~\ref{sec:symmetric-breakdown}.
\end{proof}

\begin{lemma}\label{lemma:symmetric-H}
In the conjugate gradient method for a
symmetric matrix~$A$, the Hessenberg matrix~$H$ is symmetric iff
$R$~is equilibrated, that is, there is an~$\alpha$ such that
$\forall_i\colon\|r_i\|=\alpha$.
\end{lemma}

\begin{proof}
Let $R$ be equilibrated with $\|r_i\|\equiv\alpha$, then
$R^tAR=R^tRH=\alpha^2 H$, so $H$~is symmetric.  For the converse,
assume that $H$~is symmetric, and let $D=R^tR$. Since $R^tAR$ is
symmetric, we have $DH=H^tD$, so $DHD\inv=H^t=H$. From the symmetry
of~$H$, $h_{ii+1}=h_{i+1i}$, so $d_{i+1}/d_i=d_i/d_{i+1}$, that is,
$\|r_i\|=\|r_{i+1}\|$.
\end{proof}

We draw the trivial conclusion.

\begin{corollary}
If the sequence $R$ is normalized in CG for a symmetric matrix, $H$~is
symmetric.
\end{corollary}

The symmetry of $H$ is used for instance in SVD calculations; see
section~\ref{sec:svd}.

The question whether for unsymmetric matrices $A$ the upper Hessenberg
matrix can take on a banded form with a small bandwidth is of
practical importance, since such a form limits the length of the
recurrences such as~(\ref{res:direct-recurrence})
or~(\ref{res,p:recurrence}). This question was answered largely
negatively by~\cite{Vo:cg} and more generally
by~\cite{FaberManteuffel:conditions-for-existence}: the conjugate
gradient algorithm gives a tridiagonal matrix if the spectrum of $A$
lies on a line in the complex plane; for other matrices the bandwidth
is a large fraction of the matrix size.

\Level 1 {Lanczos orthogonalisation and the BiConjugate Gradients method}
\label{sec:bi-ortho}

So far we have let the sequence~$S$ to which $R$ was orthogonalised
be arbitrary, except when we took~$S=\nobreak R$. It is easy to show
that letting $S$~be based on a Krylov sequence leads to full
(bi-\nobreak)\discretionary{}{}{}orthogonalisation of $R$ and~$S$.
This is the basis of the Lanczos and BiConjugate Gradients methods.

We will limit the inner product to the choice~$N=M\inv$.

First we show that the semi-orthogonalisation condition that $S^tM\inv R$
be lower triangular becomes a bi-orthogonalisation condition
where $S^tM\inv R$ is diagonal if $S$ is itself based on a Krylov sequence.

\begin{lemma}\label{lemma:bi-orth}
Let the sequences $R,S$ satisfy
\[ AM\inv R=RH,\quad A^tM\invt S=S\tilde H\qquad \]
with $H$, $\tilde H$ upper Hessenberg and assume that $S^tM\inv R$
lower triangular. Then $S^tM\inv R$ is diagonal, and $H$,~$\tilde H$
are tridiagonal.
\end{lemma}
\begin{proof}
We have \[ \tilde H^tS^tM\inv R = S^tM\inv AM\inv R=S^tM\inv RH. \]
The first term is lower Hessenberg, so $S^tRH$, which is a priori 
upper Hessenberg or full,
is also lower Hessenberg. Therefore
both $H$ and~$\tilde H$ must be tridiagonal, and $S^tM\inv R$~diagonal.
Additionally, $S^tM\inv AM\inv R$~is tridiagonal.
\end{proof}

We note that, unlike in the CG case, these
conclusions do not depend on symmetry of~$A$ and~$M$,
or equality of the sequences $R$ and~$S$. 

We now have the ingredients for the Bi-conjugate Gradient method.
We first give the theoretical algorithm analogous to 
algorithm~\ref{SR:algorithm}, where we also solve the columns of~$K$.

\begin{algorithm}{Bi-orthogonalisation}\label{alg:RS-bicg}
Let $A$, $r_1$, $s_1$ be given.
We inductively construct Hessenberg matrices $H$ and~$K$
and sequences $R$ and~$S$ such that
$AM\inv R=\nobreak RH$, $A^tM\invt S=SK$,
and $S^tM\inv R$ is nonsingular diagonal.
\begin{itemize}
\item Solve the $n\times1$ vector~$h_n$ from
    \[ S_n^tM\inv AM\inv r_n=S_n^tM\inv R_nh_n.\]
\item Then pick any nonzero value for~$h_{n+1,n}$, and define $r_{n+1}$ from
    \[ r_{n+1}h_{n+1,n}=AM\inv r_n-R_nh_n.\]
\item Solve $k_n$ from
    \[s_n^tM\inv AM\inv R_n=k_n^tS_n^tM\inv R_n.\]
\item Pick any nonzero value for~$k_{n+1n}$, and define $s_{n+1}$ from
    \[k_{n+1,n}s_{n+1}=A^tM\invt s_n-S_nk_n.\]
\end{itemize}
\end{algorithm}

This algorithm is not intended as a practical way of computing the
bi-orthogonal sequences $S$ and~$R$. We merely using it to establish
the theoretical conditions for the existence of the sequences.

\begin{ttheorem}{BiCG orthogonalisation}
Let $S_n$ be a sequence satisfying $A^tM\invt S_{n-1}=S_n\tilde H_{[n,n-1]}$
with $\tilde H$~irreducible upper Hessenberg,
and let $R_n$ be a residual sequence satisfying
$AM\inv R_{n-1}=R_nH_{[n,n-1]}$.
We can extend~$R$ and~$H$ to
\[ \begin{cases}AM\inv R_n=R_{n+1}H_{[n+1,n]}\cr
      \hbox{$S_{n+1}^tM\inv R_{n+1}$ 
                is non-singular lower triangular}\end{cases} \]
with $R_{n+1}$~a residual sequence iff $R_n$~is not $AM\inv$-invariant,
and the condition $s_{n+1}^tM\inv r_{n+1}=0$ does not occur.
\end{ttheorem}
\begin{proof}
Use corollary~\ref{cor:next-r}; by lemma~\ref{lemma:bi-orth}
the condition $S_n^tM\inv AM\inv r_n=\nobreak0$ can not occur,
since that would imply that $(S_n^tM\inv R_n)h_{1:n,n}=\nobreak 0$,
which by the zero column  sum requirement of~$H$
(theorem~\ref{zero-col-residual})
would imply~$h_{n+1n}=\nobreak0$,
in other words that $H$~has a zero column.
\end{proof}

Obviously, if $r_1=s_1$ and $A$ and $M$ are symmetric,
then $S_n=R_n$ for all~$n$, and $S_{n+1}^tM\inv R_{n+1}$ is
singular iff $r_{n+1}=0$.
The case where $A$ is not symmetric, hence $S\not\equiv\nobreak R$,
corresponds to the Lanczos method.
We can choose $r_1$ and $s_1$ independently, but since there is
no practical algorithm for the choice of~$s_1$, usually they
are taken the same.

However, there are limitations on the choice of~$K$. In fact,
$K$~is completely determined by $H$ and~$\Omega=S^tM\inv R$,
according to the following lemma.

\begin{lemma}\label{lanczos:H}
The Lanczos method generates Hessenberg matrices $H$ and $K$ that are
of tridiagonal form; with $\Omega=S^tM\inv R$ they satisfy
\begin{equation}\Omega H=K^t\Omega.\label{eq:bicg-HK}\end{equation}
\end{lemma}
\begin{proof}
We have that $S^tM\inv R$ is a diagonal matrix by lemma~\ref{lemma:bi-orth},
so for $S^tM\inv AM\inv R$ we find
\[S^tM\inv AM\inv R =S^tM\inv RH =K^tS^tM\inv R. \]
This establishes \eqref{eq:bicg-HK}.
\end{proof}

\begin{comment}
Inspecting the elements of $S^tM\inv AM\inv R$, we find from the $(n,n)$ 
position that
\[s_n^tM\inv AM\inv r_n=s_n^tM\inv r_nh_{nn}=k_{nn}^ts_n^tM\inv r_n \]
\[ \Rightarrow \begin{cases}h_{nn}=(s_n^tM\inv r_n)\inv (s_n^tM\inv AM\inv r_n)\cr
                k_{nn}=(s_n^tM\inv r_n)\invt (s_n^tM\inv AM\inv r_n)^t\end{cases}. \]
Note the similarities between $h_{nn}$ and~$k_{nn}$; in the scalar
case they are obviously equal, but in the block case they are 
neither equal nor each other's transpose.

In a similar manner we derive
\[(n+1,n):\quad k_{nn+1}s_n^tr_n=s_{n+1}^tr_{n+1}h_{n+1n}\]
and
\[(n,n+1):\quad k_{n+1n}s_{n+1}^tr_{n+1}=s_n^tr_nh_{nn+1}.\]
The matrices $H$ and $K$ are related by
\[ H=\Omega^{-1}K^t\Omega \]
where $\Omega$ is the diagonal matrix~$S^tR$.
We see again that $h_{nn}=k_{nn}$ for any scaling of $R$~and~$S$;
if we choose $h_{n+1n}=k_{n+1n}$, then
\[h_{nn+1}=k_{nn+1}={s_{n+1}^tr_{n+1}\over s_n^tr_n}h_{n+1n} \]
so $H=K$. In general, any scaling of the $R$ or 
$S$~sequence that makes $\Omega=I$ causes $H$ and~$K$ to be equal.
\end{comment}

\Level 1 {Projection}
\label{sec:projection}\index{Projection|(}
\input projection
\index{Projection|)}

\Level 0 {The iterative formulation of the conjugate gradient and
Lanczos methods}
\label{ch:iterative}

In chapter~\ref{ch:ortho} we derived in the abstract conditions on the
existence of various iterative schemes. 
We will now derive in detail the scalar coefficients in
the actual recurrences, in particular we will derive the quantities
$d_{ii}$ and~$u_{ii+k}$ in \eqref{res,p:recurrence}.
The derivations will follow from the orthogonality conditions of the
methods, that is,
for Arnoldi orthogonalisation
\begin{equation}r_i^tM\inv r_j=0\qquad\hbox{for $i\not=j$},
    \label{eq:r-ortho}\end{equation}
and for Lanczos orthogonalisation
\begin{equation}s_i^tM\inv r_j=0\qquad\hbox{for $i\not=j$}.
    \label{eq:r-biortho}\end{equation}

The coefficients~$h_{ij}$ for the three-term recurrence formulation
in \eqref{res:direct-recurrence}
will be derived in section~\ref{sec:three-term}.

\Level 1 {Search directions and the solution of linear systems}
\label{coupled:recur}\label{update:relation}

In item~\ref{APD=R(I-J)} of theorem~\ref{main-theorem} we introduced
the idea of splitting the generating recurrence for the residuals~$R$
in coupled recurrences for $R$ and the search directions~$P$.
For the individual elements of the $R$ and~$P$ sequences we
find the recurrences:
\[ r_{i+1}=r_i-Ap_id_i,\qquad p_i=M\inv r_i+\sum_{j<i}p_ju_{ji},\]
where
the constants $d_i$ and~$u_{ji}$ follow from orthogonality relations
for the $p_i$ and~$r_i$ vectors, all derived from the orthogonality
conditions \eqref{eq:r-ortho} and \eqref{eq:r-biortho}.

In the coupled recurrrences for the $R$ and~$P$ sequences we recognize
the classical formulation of the conjugate gradient method in the case
that the generating equation for~$p_i$ is a two-term recurrence.

Since the residuals $r_i$ are
orthogonal, for some value~$n$ we will have $r_n=0$. For that~$n$,
$x_n$~is the solution of the linear system $Ax=f$. Note that this is
the theoretical exact convergence of the conjugate gradient method,
discovered by~\cite{HestenesStiefel1952:cg};
in practice the method usually converges
to a given tolerance in far fewer iterations. This `speed of
convergence' depends mostly on the eigenvalues of the matrix;
see for instance \cite{AxLi:convergencerate}
or~\cite{vdSlvdVo:rateofconvergence}.

\Level 1 {Derivation of scalars under Arnoldi and Lanczos orthogonalisation}
\label{sec:scalars-nonsymm}
\label{sec:pr-ortho}

We derive the scalars in the Conjugate and BiConjugate Gradients algorithms,
that is, in methods based on orthogonalising~$R$,
and bi-orthogonalising $R$ against another sequence~$S$.

In this section we consider the case of coupled residual~/ search direction
sequences, which reduces to coupled two-term recurrences
in the cases of symmetric CG and of BiCG.

With the usual factorisation $H=(I-J)D^{-1}(I-U)$ of the Hessenberg matrix,
we have the relations
for the left and right search directions and residuals:
\[ A PD_R=R(I-J),\quad M\inv R=P(I-U_R);\]
\[ A^t QD_R=S(I-J),\quad M\invt S=Q(I-U_S), \]
where the second pair is only needed for the BiConjugate Gradient method,
and in the scalar case $U_R=\nobreak U_S$ and~$D_R=\nobreak D_S$.

We distinguish between three cases: symmetric CG, nonsymmetric CG, and
BiCG. Here are the structural differences between the methods.
\begin{enumerate}
\item BiCG (Lanczos orthogonalization):
here we know from lemma~\ref{lanczos:H} that $H$ is tridiagonal, and
hence $I-U$~is upper bidiagonal.
\item CG (Arnoldi orthogonalization)
in the symmetric case of $A=A^t$ and $M=M^t$:
we know from lemma~\ref{lemma:cg-tri}
that $H$ is tridiagonal and hence $I-U$ upper bidiagonal.
Furthermore, we can immediately derive CG results from the corresponding
BiCG ones by substituting $Q\rightarrow P$ and~$S\rightarrow\nobreak R$.
\item Nonsymmetric CG (Arnoldi orthogonalization):
here $H$ is of general upper Hessenberg shape,
and $I-U$ is a general upper triangular matrix.
We can not, in this case, substitute $Q\equiv\nobreak P$:
in effect, we have the
relation \[ Q(I-U) = M\inv S \] instead of the above.
\end{enumerate}

All three methods derive their particular scalars from the
orthogonality condition that $S^tM\inv R$ is diagonal. This is the
practical form of the orthogonality \eqref{eq:cond-ortho} that we
studied in section~\ref{sec:ortho-types}.

In preparation for the block case (section~\ref{sec:block})
we will not assume commutativity or even transposability of scalars. 
Any slight increase in complexity of the exposition is
far outweighed by the generality of the results derived.

With the usual factorisation $H=(I-J)D^{-1}(I-U)$ of the Hessenberg matrix,
there are two sets of scalars that we need to know: 
the quantities $d_{ii}$,
and~$u_{i-k,i}$ where for 
symmetric CG (Arnoldi orthogonalization in the symmetric case)
and for BiCG (Lanczos orthogonalization) only $k=1$ is needed.

\Level 2 {Coefficients under Arnoldi orthogonalization}
\label{sec:arnoldi-minv}

We start with a general discussion of orthogonality properties of
Arnoldi methods, and the coefficients that follow from them. We will
then move on to the special case of a symmetric coefficient matrix.

The Arnoldi method follows from the basic Krylov equations
\[ APD=R(I-J),\qquad M\inv R=P(I-U) \]
by positing that $\Omega\equiv R^tM\inv R$ is diagonal%
\footnote{It is enough for the
following results to require $R^tM\inv R$ to be upper triangular.
I~have not found results that distinguish between the two.
The upper triangularity condition arises from minimisation
in section~\ref{sec:abstract-true-min}.}.

\begin{question}
Can we do a story like this for $N\inv=AM\inv$?
\end{question}

First we derive that
\begin{equation}
  \label{eq:arnoldi-ptr}
  \begin{array}{l}
    \hbox{$R^tP(I-U)=R^tM\inv R=\Omega$ is diagonal}\\
    \hbox{$\Rightarrow P^tR$ is lower triangular}\\
    \hbox{also: $\diag(P^tR)=\Omega$}
  \end{array}
\end{equation}

\begin{remark}
  The lower triangularity of $P^tR$ implies that $p_i$ is orthogonal
  to $r_j$ for $j<i$, or $A$-orthogonal to $e_j$. In other words, the
  $p_i$ component of the solution is exact.
\end{remark}

\begin{remark}
  The fact that $R^tP$ is upper triangular immediately leads to the
  computability of the columns of~$U$: they can be solved individually
  from the relation $R^tP(I-U)=\Omega$. However, in order to derive the
  classical formulas for~$U$, we have to continue our analysis a bit further.
\end{remark}

Next we find\footnote{We remark parenthetically that our block framework
yields the various orthogonality relation without the usual
`lengthy inductive argument[s]'~\cite{LewisRehm:nonseparable}.} that
\begin{equation}
  \label{eq:arnoldi-ptap}
  \begin{array}{l}
    P^tAP=P^tR(I-J)D\inv\\
    \hbox{$\Rightarrow P^tAP$ is lower triangular}\\
    \hbox{so $\diag(P^tAP)=\diag(P^tR)D\inv = \Omega D\inv$}
  \end{array}
\end{equation}
In the symmetric case this last equation becomes the defining equation
for the coefficients of~$D$:
\begin{equation}
  \label{eq:arnoldi-ptap-sym}
  \hbox{$P^tAP=\Omega D\inv$ or $d_{ii}=r_i^tM\inv r_i/p_i^tAp_i$}
\end{equation}
We now combine these results to find
\begin{eqnarray*}
  M\inv R&=&P(I-U)\\
  \Omega&=&R^tP(I-U)\\
  D\invt(I-J^t)\Omega&=&P^tA^tP(I-U)\\
  \hbox{symmetric case: }&=&D\invt \Omega (I-U)
\end{eqnarray*}
which in the symmetric case leads to a simple equation for the
coefficents of~$U$:
\begin{equation}
  \label{eq:cg-u}
  \hbox{$\Omega\inv (I-J^t)\Omega = (I-U)$ or 
    $u_{ii+1}=\omega_{i+1}/\omega_i$}.
\end{equation}

Summarising,
the Conjugate Gradient algorithm for symmetric matrices
is based around the following two recurrences:
\begin{equation}
    r_{i+1}=r_i-Ap_i\alpha_i,\quad \alpha_i=(p_i^tAp_i)\inv(r_i^tM\invt r_i),
    \label{eq:r-update}\end{equation}
which is the symmetric case of \eqref{d:symm}, and
\begin{equation}
    p_{i+1}=M\inv r_{i+1}+p_i(r_i^tM\inv r_i)\inv(r_{i+1}^tM\inv r_{i+1}).
    \label{eq:p-update}\end{equation}
which uses \eqref{eq:u-symm}.

\begin{algorithm}{Conjugate Gradients Method}
In order to solve $Ax=f$, choose $x_1$ arbitrarily.
Let $r_1=Ax_1-f$.
Then perform the following steps for $i=1,\ldots$:
\begin{itemize}
\item Perform a stopping test on~$\|r_i\|$.
\item Compute the preconditioned residual \[ M\inv r_i \]
and the inner product $r_i^tM\inv r_i$.
\item Compute the new search direction as $p_1=M\inv r_1$, 
and for~$i>\nobreak 1$
\[ p_i=M\inv r_i+p_{i-1}u_{i-1,i}, \]
where $u_{i-1i}=r_i^tM\inv r_i/r_{i-1}^tM\inv r_{i-1}$.
\item Compute the $A$ product with the search direction
\[ Ap_i \]
and the inner product~$p_i^tAp_i$.
\item Now update the iterate
\[ x_{i+1}=x_i-p_id_{ii} \]
and the residual
\[ r_{i+1}=r_i-Ap_id_{ii} \]
where $d_{ii}=r_i^tM\inv r_i/p_i^tAp_i$.
\end{itemize}
\end{algorithm}

\Level 2 {Arnoldi orthogonalisation in the nonsymmetric case}
\label{sec:arnoldi-coeff-gen}

Likewise, in the case of nonsymmetric CG, we find from
\begin{eqnarray*}
P^tA^tM\inv R&=&P^tA^tP(I-U)\\ &=&D\invt(I-J)^tR^tM\inv R
\end{eqnarray*}
that \begin{equation}
 D\invt(I-J^t)R^tM\inv R=P^tA^tP(I-U). \label{eq:cg-u-def}\end{equation}
In \eqref{eq:cg-u-def} we find for the $n+1$-st column of~$U$
\begin{equation}
        \begin{pmatrix}0\cr \vdots \cr 0 \cr -d_n\invt\cr d_{n+1}\invt\end{pmatrix}
                r_{n+1}^tM\inv r_{n+1}
        =\begin{pmatrix}p_1^tA^tp_1&\ldots&p_1^tA^tp_{n+1}\cr
                &\ddots&\vdots\cr
                &&p_{n+1}^tA^tp_{n+1}\end{pmatrix}
        \begin{pmatrix}-u_{1n+1}\cr \vdots \cr -u_{nn+1}\cr 1\end{pmatrix}
\end{equation}
from which first of all we find by considering the last row%
\footnote{The value for $d_{ii}$ can also be derived directly from
\begin{eqnarray*}
\diag(P^tAPD)&=&\diag(P^tR(I-J))=\diag(P^tR)=\diag((I-U)\invt R^tM\invt R)\\
&=&\diag(R^tM\invt R)\end{eqnarray*} hence
\begin{eqnarray*}
    p_i^tAp_i d_i&=&p_i^tr_i=r_i^tM\invt r_i\\
    \Rightarrow d_i&=&(p_i^tAp_i)\inv (r_i^tM\invt r_i)
    \end{eqnarray*}
}
\begin{eqnarray}
    d_{n+1}\invt r_{n+1}^tM\inv r_{n+1}&=&p_{n+1}^tA^tp_{n+1}
                \label{d:symm1}\\
    d_{n+1}(r_{n+1}^tM\invt r_{n+1})\inv&=&(p_{n+1}^tAp_{n+1})\inv
                \nonumber\\
    d_{n+1}&=&(p_{n+1}^tAp_{n+1})\inv (r_{n+1}^tM\invt r_{n+1})
                \label{d:symm}\end{eqnarray}
\begin{comment}
Observe the close resemblance of this formula to \eqref{eq:d-lanczos};
however, the occurrences of~$A$ are transposed between the one and
the other. This is due to the fact that BiCG has a generating
equation~$A^tQ=\nobreak\ldots$, while CG has~$AP=\nobreak\ldots$
in its place.
\end{comment}

Eliminating the last row and column in \eqref{eq:cg-u-def}
we find, using \eqref{d:symm1} above,
\[
        \begin{pmatrix}0\cr \vdots \cr 0 \cr
                (r_n^tM\inv r_n)\inv r_{n+1}^tM\inv r_{n+1}\end{pmatrix}
        +(p_n^tAp_n)\inv\begin{pmatrix}p_1^tA^tp_{n+1}\cr\vdots\cr p_n^tA^tp_{n+1}\end{pmatrix}
\]
\begin{equation}
        = (p_n^tA^tp_n)\inv
        \begin{pmatrix}p_1^tA^tp_1&\ldots&p_1^tA^tp_n\cr
                &\ddots&\vdots\cr
                &&p_n^tA^tp_n\end{pmatrix}
        \begin{pmatrix}u_{1n+1}\cr \vdots \cr u_{nn+1}\end{pmatrix}
\end{equation}
which reduces immediately to
\begin{equation}
    u_{nn+1}=(r_n^tM\inv r_n)\inv (r_{n+1}^tM\inv r_{n+1}),
    \label{eq:u-symm}\end{equation}
for the case of a symmetric system%
\footnote{In the symmetric case we can derive $u_{ii+1}$ from
investigating the relation $R^tM\invt AM\inv R=R^tM\inv RH$.
In the lower triangle we find
\begin{eqnarray*}
  (i+1,i):&&
    r_{i+1}^tM\invt AM\inv r_i=-r_{i+1}^tM\inv r_{i+1} d_i\inv \\
    &\Rightarrow&
    r_i^tM\invt A^tM\inv r_{i+1}=- d_i^{-t}r_{i+1}^tM\invt r_{i+1},
    \end{eqnarray*}
which result we use (with the symmetry of~$A$) for the following
relation in the upper triangle:
\begin{eqnarray*}
   (i,i+1):&&
    r_i^tM\invt AM\inv r_{i+1}=(r_i^tM\inv r_i) d_{i+1}\inv u_{ii+1}\\
    &\Rightarrow& u_{ii+1}=
      - d_i(r_i^tM\inv r_i)\inv d_i^{-t}(r_{i+1}^tM\inv r_{i+1})\\
    &&\hphantom{u_{ii+1}}=
      -(p_i^tAp_i)\inv d_i^{-t}(r_{i+1}^tM\inv r_{i+1})\\
    &&\hphantom{u_{ii+1}}=
      -(p_i^tAp_i)\inv (p_i^tAp_i)(r_i^tM\inv r_i)\inv (r_{i+1}^tM\inv r_{i+1})\\
    &&\hphantom{u_{ii+1}}=
      -(r_i^tM\inv r_i)\inv(r_{i+1}^tM\inv r_{i+1})
    \end{eqnarray*}
}%
.
We note that this equation would also be obtained
by making the identifications $Q\equiv P$, $S\equiv\nobreak R$
in \eqref{eq:u-lanczos}.

The method we have described in this section is sometimes called the
Full Orthogonalisation Method (FOM)\index{FOM}; it reduces to the ordinary
CG method in the case of a symmetric matrix. Various methods have been
proposed in the literature implementing Arnoldi orthogonalisation
under the $M\inv$ inner product --~in fact, most authors use
unpreconditioned methods in their expositions~-- in the nonsymmetric
case: OrthoRes\index{OrthoRes}~\cite{YoungJea1980:generalizedcg} and
GENCG~\cite{CoGo:cgnonsymmetric,Wi:nonsymm}.

\Level 2 {Lanczos case}

We start with the basic premisse:
\[ S^tM\inv R\qquad\hbox{is diagonal}. \]
Observing that $S^tM\inv AM\inv R=S^tM\inv RH=H^tS^tM\inv R$
is of both upper and lower Hessenberg form:
\[ S^tM\inv AM\inv R\qquad\hbox{is tridiagonal}. \]
From $S^tP(I-U)=S^tM\inv R$:
\[ S^tP\qquad\hbox{is upper triangular}, \]
and likewise from $(I-U)^tQ^tR=S^tM\inv R$
\[ Q^tR\qquad\hbox{lower triangular}, \]
and from the last two, noting that by
$Q^tAP=Q^tR(I-J)D\inv$ we find $Q^tAP$ to be lower triangular,
and by $Q^tAP=D\invt(I-J^t)S^tP$ upper triangular,
\[ Q^tAP\qquad\hbox{diagonal}. \]

\begin{comment}
Here are a few more.
\begin{table}[htb]
\begin{tabular}{rlll}
 Equation\hfill/\hfill Shape&Relation&Valid for\\
 \hline
 $S^tAP=S^tR(I-J)D^{-1}$\hfill\hbox{} \\
 lower bidiagonal& $s_i^tAp_j=0$& $j>i$, $i>j+1$\\
 $Q^tAR=D^{-1}(I-J)^tS^tR$\hfill\hbox{} \\
 upper bidiagonal& $q_i^tAr_j=0$& $i>j$, $j>i+1$\\
 $S^tAP=(I-U)^tQ^tAP$\hfill\hbox{}& $s_i^tAp_i=q_i^tAp_i$\\
 $Q^tAR=Q^tAP(I-U)$\hfill\hbox{}& $q_i^tAr_i=q_i^tAp_i$
\end{tabular}
\caption{Orthogonality properties of the Lanczos
method.}
\label{ortholan:table}
\end{table}
\end{comment}
 
\begin{comment}
For completeness,
table~\ref{ortho:table} gives orthogonality properties for the
gradients and search directions thus derived.

\begin{comment}
\begin{eqnarray*}
S^tP(I- U)=S^tM\inv R&\Rightarrow&
    \hbox{$S^tP$ is upper triangular}\\
    &\hbox{and}&s_i^tp_i=s_i^tM\inv r_i
    \end{eqnarray*}
Using the fact that $S^tP$ is upper triangular we find:
\begin{eqnarray*}
P^tA^tQ D=P^tS(I-J)&\Rightarrow&
    \hbox{$P^tA^tQ$ is lower triangular}\\
    &\Rightarrow&p_i^tA^tq_j=0\quad\hbox{for}\quad
                \begin{cases}i<j&in general\cr i\not=j&symmetric case.\end{cases}
\end{eqnarray*}
\end{comment}

\Level 2 {Lanczos bi-orthogonalisation}

In the case of BiCG, we need to find the coefficients
for the left and right sequences. 

In algorithm~\ref{alg:RS-bicg} the choice for $h_{n+1n}$ and~$k_{n+1n}$
was left unspecified.
Under the choice $-\sum_{i\leq n}h_{in}$ and~$-\sum_{i\leq n}h_{in}$
respectively,
both $R$ and~$S$ become residual sequences, and we can derive further
relations between $H$ and~$K$.
If $R$~is a residual sequence, $H$~allows a factorisation
(see lemma \ref{H=(I-J)-fac} and theorem~\ref{zero-col-residual})
\[ H=(I-J)D_R\inv(I-U_R). \]
Since $K^t=\Omega H\Omega\inv$, we find
\[ K=(I-\Omega\invt U_R^t\Omega^t)\Omega\invt D_R\invt \Omega^t
                (I-\Omega\invt J^t\Omega^t). \]
If $K$~is a residual sequence, it too must allow a factorisation
\[ K=(I-J)D_S\inv(I-U_S), \]
so
\[ \Omega\invt U_R^t\Omega^t=J, \]
from which we find
\[ (s_{n+1}^tM\inv r_{n+1})\invt u^t_{R;nn+1}(s_n^tM\inv r_n)^t=I, \]
that is,
\begin{equation}
 u_{R;nn+1} = (s_n^tM\inv r_n)\inv(s_{n+1}^tM\inv r_{n+1}).
    \label{eq:u-right}\end{equation}
Likewise,
\begin{equation}
 u_{S;nn+1} = (r_n^tM\invt s_n)\inv(r_{n+1}^tM\invt s_{n+1}).
    \label{eq:u-left}\end{equation}
\begin{block}
The above equations are already in a block form;
in the scalar case the coefficients are identical
for the left and right sequences.
\end{block}

We find from
\[ Q^tAM\inv R= \begin{cases}Q^tAP(I-U_R)\cr D_S\invt(I-J)^tS^tM\inv R\end{cases} \]
that \begin{equation}
 D_S\invt(I-J^t)S^tM\inv R=Q^tAP(I-U_R). \label{eq:bicg-u-def}\end{equation}
and from
\[ P^tA^tM\invt S= \begin{cases}P^tA^tQ(I-U_S)\cr D_R\invt(I-J)^tR^tM\invt S\end{cases} \]
that \begin{equation}
 D_R\invt(I-J^t)R^tM\invt S=P^tA^tQ(I-U_S).
    \label{eq:bicg-ul-def}\end{equation}
In the scalar case the two equations are the same; in the block case
we need both of them.

In \eqref{eq:bicg-u-def} we find for the $n+1$-st column of~$U$,
recalling from section~\ref{sec:pr-ortho} that $Q^tAP$ is diagonal,
\begin{equation}
        \begin{pmatrix}0\cr \vdots \cr 0 \cr -d_{S;n}\invt\cr d_{S;n+1}\invt\end{pmatrix}
                s_{n+1}^tM\inv r_{n+1}
        =\diag(q_i^tAp_i)_{i=1\ldots n+1}
        \begin{pmatrix}-u_{R;1n+1}\cr \vdots \cr -u_{R;nn+1}\cr 1\end{pmatrix}
        \label{eq:col-of-u}\end{equation}
and a similar result from~\eqref{eq:bicg-ul-def}.
This first of all gives from row~$n+\nobreak1$
\begin{eqnarray}
    d_{S;n+1}\invt s_{n+1}^tM\inv r_{n+1}&=&q_{n+1}^tAp_{n+1}
        \label{eq:d-lanczos1}\\
    d_{S;n+1}(r_{n+1}M\invt s_{n+1})\inv&=&(p_{n+1}^tA^tq_{n+1})\inv\nonumber\\
    d_{S;n+1}&=&(p_{n+1}^tA^tq_{n+1})\inv (r_{n+1}^tM\invt s_{n+1}).
        \label{eq:dl-lanczos}\end{eqnarray}
and likewise
\begin{equation} d_{R;n+1}=(q_{n+1}^tAp_{n+1})\inv (s_{n+1}^tM\inv r_{n+1}).
    \label{eq:d-lanczos}\end{equation}
Furthermore, from row~$n$ of \eqref{eq:col-of-u},
and using \eqref{eq:d-lanczos1} above,
\begin{eqnarray}
    q_n^tAp_nu_{R;nn+1}&=&d_{S;n}\invt s_{n+1}^tM\inv r_{n+1}\nonumber\\
    q_n^tAp_nu_{R;nn+1}&=&(q_n^tAp_n)(s_n^tM\inv r_n)\inv(s_{n+1}^tM\inv r_{n+1})
                \nonumber\\
    u_{R;nn+1}&=&(s_n^tM\inv r_n)\inv (s_{n+1}^tM\inv r_{n+1}).
    \label{eq:u-lanczos}\end{eqnarray}
and similarly
\begin{equation} u_{S;nn+1}=(r_nM\invt s_n)\inv(r_{n+1}M\invt s_{n+1}).
    \label{eq:ul-lanczos}\end{equation}

\Level 2 {Algorithm for the BiConjugate Gradients method}

The BiConjugate Gradients algorithm in the scalar case
is based on the recurrences
\begin{eqnarray*}
    r_{n+1}&=&r_n-Ap_i(p_n^tA^tq_n)\inv (r_n^tM\invt s_n),\\
    s_{n+1}&=&s_n-A^tq_i(p_n^tA^tq_n)\inv (r_n^tM\invt s_n),
\end{eqnarray*}
and
\begin{eqnarray*}
    p_{n+1}&=&M\inv r_{n+1}+p_i(s_n^tM\inv r_n)\inv (s_{n+1}^tM\inv r_{n+1}),\\
    q_{n+1}&=&M\invt q_{n+1}+q_i(s_n^tM\inv r_n)\inv (s_{n+1}^tM\inv r_{n+1}).
\end{eqnarray*}

\begin{algorithm}{BiConjugate Gradients Method}
In order to solve $Ax=f$, choose $x_1$ arbitrarily.
Let $r_1=Ax_1-f$ and choose $s_1$ arbitrarily, for instance $s_1=r_1$. 
Then perform the following steps for $i=1,\ldots$:
\begin{itemize}
\item Perform a stopping test on~$\|r_i\|$.
\item Compute the preconditioned residuals
\[ M\inv r_i,\qquad M\invt s_i \]
and the inner product $s_i^tM\inv r_i$.
\item Compute the new search directions as $p_1=M\inv r_1$, 
$q_1=M\inv s_1$, and for~$i>\nobreak 1$
\[ p_i=M\inv r_i+p_{i-1}u_{i-1,i},\qquad 
   q_i=M\invt s_i+q_{i-1}u_{i-1,i}. \]
where $u_{i-1i}=s_i^tM\inv r_i/s_{i-1}^tM\inv r_{i-1}$.
\item Compute the $A$ products with the search directions
\[ Ap_i,\qquad A^tq_i \]
and the inner product~$q_i^tAp_i$.
\item Now update the iterate
\[ x_{i+1}=x_i-p_id_{ii} \]
and the residuals
\[ r_{i+1}=r_i-Ap_id_{ii},\qquad s_{i+1}=s_i-A^tq_id_{ii} \]
where $d_{ii}=s_i^tM\inv r_i/q_i^tAp_i$.
\end{itemize}
\end{algorithm}

\Level 2 {Other definitions of search directions}
\label{sec:alternative-search}

We could have defined the search directions
slightly differently by letting $\tilde P=R(I-U)\inv$.
This gives the coupled equations
\[ AM\inv \tilde PD=R(I-J), \qquad \tilde P(I-U)=R. \]
Since the matrices $D$ and~$U$ are derived only from~$H$, hence independent
of the introduction of~$\tilde P$, they remain the same as in the ordinary
derivation of Conjugate Gradient methods.
Hence, we still have to compute the inner product
$p_i^tAp_i=(M\inv\tilde p_i)^tA(M\inv\tilde p_i)$.
Clearly, we need the vector $M\inv p_i$, in addition to the usual $M\inv r_i$.
We can avoid the cost of an extra preconditioner application by
recursively computer $M\inv \tilde p_i$ from the equation
$M\inv\tilde P(I-U)=\nobreak M\inv R$. We arrive at the following 
iterative formulation:
\begin{eqnarray*}
    z_i&=&M\inv r_i\\
    p_i&=&r_i-p_{i-1}{z_i^tr_i\over z_{i-1}^tr_{i-1}}
                \qquad\hbox{omit second term if $i=1$}\\
    q_i&=&z_i-q_{i-1}{z_i^tr_i\over z_{i-1}^tr_{i-1}}
                \qquad\hbox{omit second term if $i=1$}\\
    r_{i+1}&=&r_i-Aq_i{z_i^tr_i\over q_i^tAq_i}
\end{eqnarray*}

The update relation for the search direction can be written as $M\inv
R=P(I-\Omega\inv J\Omega)$, or
\begin{equation}
    M\inv R\Omega\inv=\tilde P(I-J),\qquad \tilde P=P\Omega\inv.
    \label{eq:axpy-p-update}\end{equation}
Writing the residual update as $R(I-J)=AP(P^tAP)\inv\Omega$, this
becomes
\[ R(I-J) = A\tilde P(\tilde P^tA\tilde P)\inv. \]
\index{BLAS!in search direction update}
%
Since the $P$ update in \eqref{eq:axpy-p-update} is now a BLAS Level~1
AXPY~operation, we can use standard libraries and perhaps gain some
performance efficiency.

\Level 2 {Arnoldi case; $AM\inv$ inner product}

As we observed in section~\ref{sec:abstract-true-min}, making
residuals orthogonal under the $AM\inv$ inner product leads to
minimisation of their lengths over the subspaces they are part of. We
will discuss specific implementations of this orthogonalisation scheme
in section~\ref{sec:resmin}.

\Level 2 {Arnoldi case; General orthogonalisation}
\label{sec:cg:inprod}

So far, we have considered for the orthogonality of~$R$ only 
a few special cases. 
We will now investigate the general case where
\[ \hbox{$R^tN\invt R$ is upper triangular / diagonal}, \]
with the diagonal case probably only corresponding to a symmetric~$N$.
In this orthogonality condition, $N$~can be expressable in terms
of $A$ and~$M$, but need not be.

Using again the coupled two-term recurrences 
\[ APD=R(I-J),\qquad P(I-U)=M\inv R,\]
with $D$ and~$U$ to be derived from the orthogonality condition,
we find that
\[ R^tN\invt R=R^tN\inv MP(I-U), \]
whence $R^tN\invt MP$ is upper triangular.
With
\[ P^tM^tN\inv R(I-J)=P^tM^tN\invt APD \]
we find that $P^tM^tN\inv APD$ is lower triangular,
and from considering the diagonal of this and the previous equation we find
\[ r_i^tN\inv r_i=p_i^tM^tN\inv r_i=p_i^tM^tN\inv p_id_{ii}, \]
so \[ d_{ii}^t=r_i^tN\invt r_i/p_i^tA^tN\invt Mp_i. \]

As in section~\ref{sec:scalars-nonsymm}, we derive
\begin{eqnarray*} P^tA^tN\invt MP(I-U)&=&
        R^tN\inv R(I-J)D\inv\\
        &=&P^tA^tN\invt R,\end{eqnarray*}
\[ \Rightarrow P^tA^tN\invt MP(I-U)=D\invt (I-J)^tR^tN\invt R \]
and we consider column~$n+\nobreak1$ of this equation, giving
which is an equality between upper triangular matrices.
We obtain a size~$n+\nobreak1$ system
\[ \begin{pmatrix}\begin{cases}p_i^tA^tN\invt Mp_j
                &\begin{matrix}j\geq i,\cr i,j\leq n+1\end{matrix}\cr
                   0&otherwise\end{cases}\end{pmatrix}
    \begin{pmatrix}-u_{1n+1}\cr \vdots\cr -u_{nn+1}\cr 1\end{pmatrix} = 
    D\invt \begin{pmatrix}(r_1-r_2)^tN\invt r_{n+1}\cr \vdots\cr
                (r_n-r_{n+1})^tN\invt r_{n+1}\cr r_{n+1}^tN\invt r_{n+1}\end{pmatrix}. \]
The last line of this confirms our derivation of~$d_{ii}$ above.
After deleting the last row and column, we are left with the size~$n$ system
\[ \begin{pmatrix}\begin{cases}p_i^tA^tN\invt Mp_j&j\geq i,\cr
                                     &i,j\leq n\cr 0&otherwise\end{cases}\end{pmatrix}
    \begin{pmatrix}u_{1n+1}\cr \vdots\cr u_{nn+1}\end{pmatrix} = 
    \begin{pmatrix}p_1^tA^tN\invt r_{n+1}\cr\vdots\cr p_n^tA^tN\invt r_{n+1}\end{pmatrix}, \]
which can be solved recursively.

\Level 2 {Iterating under general inner product}

There are several equivalent ways of computing the iteration
under general inner product.

Residuals:
\[ r_{i+1} = r_i+Ap_id_{ii}\]
\[ N\inv r_{i+1} = N\inv r_i+N\invt Ap_id_{ii} \]
\[ d_{ii} = r_i^tN\invt r_i/p_i^tA^tN\invt Mp_i \]
\[ N\invt r_{i+1} = N\invt r_i+N\invt Ap_id_{ii} \]
in which the quantity $N\invt Ap_i$ is not computed anywhere else.

Search directions:
\[ p_{i+1} = M\inv r_{i+1}+\sum_{j\leq i}p_ju_{ji} \]
\[ Mp_{i+1} = r_{i+1}+\sum_{j\leq i}M p_ju_{ji} \]
\[ N\invt Mp_{i+1} = N\invt r_{i+1}+\sum_{j\leq i}N\invt Mp_ju_{ji} \]
\[ A^tN\invt Mp_{i+1} = A^tN\invt r_{i+1}+\sum_{j\leq i}A^tN\invt Mp_ju_{ji} \]

\Level 1 {Three-term recurrences}
        \label{sec:three-term}

Some authors \cite{EnGiRuSt:refined,HaYo:applied}, have considered
a three-term recurrence for the iterands in the conjugate gradient
algorithm, corresponding to the three-term recurrence for the
residuals. Such a three-term recurrence is the usual mode of
presentation for the Lanczos method and the Chebyshev semi-iterative
method~\cite{HaYo:applied}. 

From the splitting $H=(I-J)D^{-1}(I-U)$ of the Hessenberg matrix 
we find
\[ H=\begin{pmatrix}&-d_{n-1}\inv u_{n-1n}\cr
    -d_{n-1}\inv&d_n\inv+d_{n-1}\inv u_{n-1n}&-d_n\inv u_{nn+1}\cr
    &-d_n\inv \end{pmatrix}. \]
This gives a residual recurrence from $AM\inv R=RH$:
\begin{equation}
        -r_{n+1}d_{nn}^{-1}+r_n(d_{nn}\inv+d_{n-1n-1}\inv u_{n-1n})
        -r_{n-1}d_{n-1n-1}\inv u_{n-1n}=AM\inv r_n.
        \label{eq:three-term:H}\end{equation}
with a corresponding update equation for the iterates
\[ x_{n+1}=(1+u_{n-1n}d_{nn}/d_{n-1n-1})x_n-M\inv r_n
        -u_{n-1n}(d_{nn}/d_{n-1n-1})x_{n-1}. \]

\begin{question}
How do you derive the $d$ and $u$ coefficients from just the residuals?
\end{question}

A three-term recurrence can also directly be derived from the elements
of the Hessenberg matrix:
\begin{equation}
        r_{i+1}h_{i+1i} + r_ih_{ii} + r_{i-1}h_{i-1i} = AM\inv r_i
        \label{eq:three-term}\end{equation}
where from orthogonalization properties we get the expressions
\begin{equation}
\begin{matrix}h_{ii}={r_i^tM\inv AM\inv r_i\over r_i^tr_i},\cr
    h_{ii+1}={r_i^tM\inv AM\inv r_{i+1}\over r_i^tr_i},&
    h_{i+1i}={r_{i+1}^tM\inv AM\inv r_i\over r_{i+1}^tr_{i+1}}.\end{matrix}
        \label{three-term:h:formulas}\end{equation}
Note that the $h_{i+1i}$ are arbitrary normalization factors.

If $A$ is symmetric, the scalar $h_{ii+1}$ can be derived as
\begin{equation}
        h_{ii+1}={r_{i+1}^tr_{i+1}\over r_i^tr_i}h_{i+1i},
        \label{h:both:sides}\end{equation}
which can be shown~\cite{Pa:variantslanczos,Pa:errorlanczos}
to be a more stable variant
of the method.

Factoring the matrix $H=(I-J)D\inv(I-U)$ gives a recurrence
\begin{eqnarray*}
  d_{n+1}\inv&=&h_{n+1n+1}-h_{n+1n}d_nh_{nn+1}\\
    &=&h_{n+1n+1}+\begin{cases}h_{nn+1}\cr h_{n+1n}u_{nn+1}\end{cases}
\end{eqnarray*}

The three-term form (\ref{eq:three-term}) is often written as two
coupled two-term recurrences
\[ r_{i+1}h_{i+1i} + r_ih_{ii} = t_i
        \qquad\hbox{where}\qquad
        t_i = Ar_i - r_{i-1}h_{i-1i}. \]
Although this splitting is of less mathematical significance than the
introduction of search directions in
theorem~\ref{main-theorem}, item~\ref{APD=R(I-J)},
it does
allow for a slight variant of the computation by noting that
$r_i^tAr_i=r_i^tt_i$, which is used in the computation of~$h_{ii}$.
This variant is as stable as the original
method~\cite{Pa:variantslanczos}.

\begin{comment}
\Level 1 {Orthomin and Orthodir}

A popular conjugate gradient method for nonsymmetric systems is
`Orthomin', originally presented by~\cite{Vi:orthomin} (see
also~\cite{YoJe:generalizedcg}). It 
corresponds to orthogonalizing the residuals with respect to some
auxiliary matrix~$Z$; see section~\ref{sec:cg:inprod}. For the
formulas for~$d_{nn}$ and $u_{n-k,n}$ we find
\[ d_{nn}={r_n^tZAr_n\over p_n^tZAp_n}={p_n^tZAr_n\over p_n^tZAp_n} \]
and for~$k<n$
\[ u_{k,n} = -{p_k^tZAr_n+\sum_{i=1}^{k-1}p_k^tZAp_iu_{i,n}
        \over p_k^tZAp_n}
        \]
which can be derived from equation~(\ref{orthomin:recurrence}).
\end{comment}

\Level 1 {Block Conjugate Gradients}
\label{sec:block}

If more than one linear system is to be solved, and the systems are
independent, it is possible to iterate on them simultaneously.
For $n_b$~systems to be solved, each vector then becomes an
$N\times n_b$ matrix, and each scalar (such as $p_i^tAp_i$) becomes
an $n_b\times n_b$ matrix, with division turning into matrix inversion.
Thus we need the formulas (\ref{eq:r-update}) and~(\ref{eq:p-update})
that were derived assuming non-commutativity of multiplication.

A further refinement of these relations is needed. 
The usual orthogonality relations only assure that the $n_b$-wide block vectors
are orthogonal as blocks; the vectors inside each $r_i$~block vector are not.
Also, the component vectors may be of widely differing norms, making the 
matrices $r_i^tMr_i$ and $p_i^tAp_i$ badly scaled. To remedy this,
we transform the equations by introducing a block diagonal scaling.

\Level 2 {Preliminaries}

We first introduce the transformation $p_i=\tilde p_i\gamma_i$, giving
\begin{eqnarray}
    p_{i+1}&=&M\inv r_{i+1}+p_i(r_i^tM\inv r_i)\inv(r_{i+1}^tM\inv r_{i+1})
                                       \nonumber\\
    \tilde p_{i+1}\gamma_{i+1}&=&M\inv r_{i+1}
        +\tilde p_i\gamma_i(r_i^tM\inv r_i)\inv(r_{i+1}^tM\inv r_{i+1})
                                       \label{eq:gamma-from-block-p}
\end{eqnarray}
and
\begin{eqnarray*}
    r_{i+1}&=&r_i-Ap_i(p_i^tAp_i)\inv(r_i^tM\invt r_i)\\
    &=&r_i-A\tilde p_i\gamma_i(\gamma_i^t\tilde p_i^tA\tilde p_i\gamma_i)\inv
                (r_i^tM\invt r_i)\\
    &=&r_i-A\tilde p_i(\tilde p_i^tA\tilde p_i)\inv\gamma_i^{-t}
                (r_i^tM\invt r_i).
\end{eqnarray*}
This is the block CG algorithm as it was given by
O'Leary~\cite{OLeary1980:blockcg}.  We arrive at a specific choice
of~$\gamma_{i+1}$ in \eqref{eq:gamma-from-block-p} by splitting the
equation in two steps, and first computing an auxiliary
vector~$\pi_{i+1}$:
\begin{eqnarray*}
    \pi_{i+1}&=&M\inv r_{i+1}
        +\tilde p_i\gamma_i(r_i^tM\inv r_i)\inv(r_{i+1}^tM\inv r_{i+1})\\
\tilde p_{i+1}&=&\pi_{i+1}\gamma_{i+1}\inv
\end{eqnarray*}
The second step can for instance correspond to a QR decomposition 
of~$\pi_{i+1}$. Note that \eqref{eq:gamma-from-block-p} only
guarantees that the $p_i$ vectors are $A$-orthogonal as blocks;
letting $\gamma_i$ follow from a QR decomposition makes the component
vectors of each $p_i$ block $A$-orthogonal too.

We go another step further, introducing the transformation
$r_i=\tilde r_i\gamma_i$, which gives for the search direction update
\begin{equation}
    \tilde p_{i+1}=M\inv \tilde r_{i+1}
        +\tilde p_i(\tilde r_i^tM\inv \tilde r_i)\inv\eta_i^t
                   (\tilde r_{i+1}^tM\inv \tilde r_{i+1})
    \label{eq:block-p-update}\end{equation}
where $\eta_i\inv=\gamma_i\gamma_{i+1}\inv$, and for the residual update
\[ \tilde r_{i+1}\gamma_{i+1}=\tilde r_i\gamma_i
        -A\tilde p_i(\tilde p_i^tA\tilde p_i)\inv
                (\tilde r_i^tM\invt \tilde r_i)\gamma_i\]
or, splitting the equation by introducing an auxiliary quantity as before:
\begin{eqnarray}
    \rho_{i+1}&=&\tilde r_i
        -A\tilde p_i(\tilde p_i^tA\tilde p_i)\inv
                (\tilde r_i^tM\invt \tilde r_i)
    \label{eq:block-r-update}\\
\rho_{i+1}&=&\tilde r_{i+1}\gamma_{i+1}\gamma_i\inv\nonumber
\end{eqnarray}
A~computationally feasible form of this is
\begin{equation}
 \tilde r_{i+1}=\rho_{i+1}\eta_i\inv,\qquad \gamma_{i+1}=\eta_i\gamma_i;
    \label{eq:r-ortho-from-rho}\end{equation}
the choices of $\eta_i$ now determine the values of~$\gamma_i$; $\eta_i$~can
for instance be chosen to orthogonalise the $r_i$~sequence.

The original, untransformed, residual is no longer computed, but it
can be retrieved as \[ r_{i+1} = \rho_{i+1}\gamma_i. \]

The transformed sequences $\tilde P$ and~$\tilde R$ satisfy a Hessenberg relation
\[ A\tilde P\tilde D=\tilde R(I-\tilde J),\quad
 \tilde P(I-\tilde U)=M\inv \tilde R\]
with
\[ \tilde D=GDG\inv,\quad \tilde J=GJG\inv,\quad \tilde U=GUG\inv.\]

The solution update in terms of the transformed vectors becomes
\begin{equation}
    x_{i+1}=x_i
        -\tilde p_i(\tilde p_i^tA\tilde p_i)\inv
                (\tilde r_i^tM\invt \tilde r_i)\gamma_i.
    \label{eq:block-x-update}\end{equation}

The framework for computing the block Conjugate Gradient algorithm now becomes:
\begin{algorithm}{Block Conjugate Gradient Method, version~1}
In order to solve $Ax=f$ with $f$~an $N\times k$ block vector,
choose~$x_1$ as a block vector of maximal rank. Compute $\rho_1=Ax_1-f$.
Set $\gamma_1=I_k$.
Then perform the following steps for $i=1,\ldots$:\\
Transform the residual:
\begin{equation} r_i=\rho_i\eta_i\inv;
    \qquad \hbox{and set $\gamma_{i+1}=\eta_i\gamma_i$}
    \end{equation}
Apply the preconditioner:
\begin{equation} z_i=M\inv r_i. \end{equation}
If $i=1$, $p_1=z_1$; if $i>1$, compute a new search direction:
\begin{equation} p_i=z_i+p_{i-1}(r_{i-1}^tz_{i_1})\inv\eta_i^t(r_i^tz_i) \end{equation}
Compute $Ap_i$, and $\alpha_i=p_i^tAp_i$; with this, update the solution
\begin{equation} x_{i+1}=x_i - p_i( p_i^tA p_i)\inv( r_i^tz_i)\gamma_i.
    \end{equation}
and the residual:
\begin{equation} \rho_{i+1}= r_i -A p_i( p_i^tA p_i)\inv( r_i^tz_i).
    \end{equation}
\end{algorithm}

\Level 2 {Optimised algorithm}

In the above algorithm, the choice of~$\eta_i$ is free.
For instance, one could let $r_i\eta_i$ be a QR decomposition of~$\rho_i$,
so that~$r_i^tr_i=I_k$.
The choice $r_i^tz_i=I_k$ leads to a slightly different algorithm, in which
we compute $\zeta_i=M\inv\rho_i$, and set $z_i=\zeta_i\eta_i\inv$ after 
$\eta_i$~has been computed from the orthogonality condition%
\footnote{In practice, $z_i$ will most likely be computed simultaneously
with~$r_i$ in the joint orthogonalisation of $\rho_i$ and~$\zeta_i$,
and no separate step is needed}.
These are the ramifications of all $z_i^tr_i$~terms disappearing:
\begin{itemize}
\item In the search direction update, the old direction is multiplied
by a triangular matrix, rather than a full one, saving $k^2/2$ vector updates.
\item The new orthogonalisation of $\zeta_i^t\rho_i$ can be performed
in $k^2/2$ vector updates like that of~$\rho_i^t\rho_i$.
The latter is trivially symmetric; the former is, depending
on the symmetry of~$M$.
\item The dot product calculation~$r_i^tz_i$ disappears, saving $k^2/2$
inner products.
\end{itemize}

\begin{algorithm}{Block Conjugate Gradient Method, version~2}
In order to solve $Ax=f$ with $f$~an $N\times k$ block vector,
choose~$x_1$ as a block vector of maximal rank. Compute $\rho_1=Ax_1-f$.
Set $\gamma_1=I_k$.
Then perform the following steps for $i=1,\ldots$:\\
Apply the preconditioner:
\begin{equation} \zeta_i=M\inv \rho_i \end{equation}
Transform the residual:
\begin{equation} r_i=\rho_i\eta_i\inv;\quad z_i=\zeta_i\eta_i\inv;
    \qquad \hbox{and set $\gamma_{i+1}=\eta_i\gamma_i$}
    \end{equation}
where $\eta_i$ is chosen such that $r_i^tz_i=I_k$.\\
If $i=1$, $p_1=z_1$; if $i>1$, compute a new search direction:
\begin{equation} p_i=z_i+p_{i-1}\eta_i^t\end{equation}
Compute $Ap_i$, and $\alpha_i=p_i^tAp_i$; with this, update the solution
\begin{equation} x_{i+1}=x_i - p_i( p_i^tA p_i)\inv\gamma_i.
    \end{equation}
and the residual:
\begin{equation} \rho_{i+1}= r_i -A p_i( p_i^tA p_i)\inv.
    \end{equation}
\end{algorithm}

\Level 2 {Block BiConjugate Gradients}

\def\gl{\gamma^\ell}\def\gr{\gamma^r}
\def\glt{\gamma^{\ell^t}}\def\grt{\gamma^{r^t}}
\def\glinv{\gamma^{\ell\inv}}\def\grinv{\gamma^{r\inv}}
\def\glinvt{\gamma^{\ell\invt}}\def\grinvt{\gamma^{r\invt}}

We will now derive a block version of the BiConjugate Gradient method.
In this, we will introduce transformed sequences 
\[ p_i=\tilde p_i\gr_i,\quad q_i=\tilde q_i\gl_i,\quad
   r_i=\tilde r_i\gr_i,\quad s_i=\tilde s_i\gl_i. \]
The derivation proceeds as above,
but now using the BiCG scalars of equations
(\ref{eq:dl-lanczos}), (\ref{eq:d-lanczos}), (\ref{eq:u-lanczos})
and~(\ref{eq:ul-lanczos}):
\begin{eqnarray*}
    d^\ell_{n+1}&=&(p_{n+1}^tA^tq_{n+1})\inv (r_{n+1}^tM\invt s_{n+1})\\
    d^r_{n+1}&=&(q_{n+1}^tAp_{n+1})\inv (s_{n+1}^tM\inv r_{n+1})\\
    u^\ell_{nn+1}&=&(r_nM\invt s_n)\inv(r_{n+1}M\invt s_{n+1})\\
    u^r_{nn+1}&=&(s_n^tM\inv r_n)\inv (s_{n+1}^tM\inv r_{n+1})
\end{eqnarray*}

We introduce the following quantities:
\[ \eta^r_i=\gr_{i+1}\grinv_i, \quad \eta^\ell_i=\gl_{i+1}\glinv_i,
    \qquad c_i=\gr_i\glinv_i, \]
(in the symmetric case we will have $\eta^\ell_i\equiv\eta^r_i$
and~$c_i\equiv\nobreak1$),
and vectors
\[ \rho_{i+1}=r_{i+1}\gr_{i+1}\grinv_i, \qquad
    \sigma_{i+1}=s_{i+1}\gl_{i+1}\glinv_i. \]
Now we find 
for the right residual:
\begin{eqnarray*}
    r_{i+1}&=&r_i-Ap_i(q_i^tAp_i)\inv(s_i^tM\inv r_i)\\
    \tilde r_{i+1}\gr_{i+1}&=&\tilde r_i\gr_i-
        A\tilde p_i\gr_i\grinv_i(\tilde q_i^tA\tilde p_i)\inv\glinvt_i
                \glt_i(\tilde s_i^tM\inv \tilde r_i)\gr_i\\
    \tilde r_{i+1}\gr_{i+1}\grinv_i&=&
        \tilde r_i-A\tilde p_i(\tilde q_i^tA\tilde p_i)\inv
                (\tilde s_i^tM\inv \tilde r_i).
    \end{eqnarray*}
The left residual
\begin{eqnarray*}
    s_{i+1}&=&s_i-A^tq_i(p_i^tA^tq_i)\inv(r_i^tM\invt s_i)\\
    \tilde s_{i+1}\gl_{i+1}&=&\tilde s_i\gl_i-
        A^t\tilde q_i\gl_i\glinv_i(\tilde p_i^tA^t\tilde q_i)\inv\grinvt_i
                \grt_i(\tilde r_i^tM\invt \tilde s_i)\gl_i\\
    \tilde s_{i+1}\gl_{i+1}\glinv_i&=&
        \tilde s_i-A^t\tilde q_i\inv
                (\tilde q_i^tA\tilde p_i)\inv(\tilde s_i^tM\inv \tilde r_i).
    \end{eqnarray*}
For the right search direction
\begin{eqnarray*}
    p_{i+1}&=&M\inv r_{i+1}+p_i(s_i^tM\inv r_i)\inv(s_{i+1}^tM\inv r_{i+1})\\
    \tilde p_{i+1}\gr_{i+1}&=&M\inv \tilde r_{i+1}\gr_{i+1}\\
        &&+\tilde p_i\gr_i\grinv_i (\tilde s_i^tM\inv \tilde r_i)\inv
                \glinvt_i\glt_{i+1}
        (\tilde s_{i+1}^tM\inv \tilde r_{i+1})\gr_{i+1}\\
    \tilde p_{i+1}&=&M\inv \tilde r_{i+1}
        +\tilde p_i (\tilde r_i^tM\invt \tilde s_i)\inv \eta^{\ell^t}_i
        (\tilde r_{i+1}^tM\inv \tilde s_{i+1})
\end{eqnarray*}
For the left search direction:
\begin{eqnarray*}
    q_{i+1}&=&M\invt s_{i+1}
        +q_i(r_i^tM\invt s_i)\inv(r_{i+1}^tM\invt s_{i+1})\\
    \tilde q_{i+1}\gl_{i+1}&=&M\invt \tilde s_{i+1}\gl_{i+1}\\
        &&+\tilde q_i\gl_i\glinv_i (\tilde r_i^tM\invt \tilde s_i)\inv
                \grinvt_i\grt_{i+1}
        (\tilde r_{i+1}^tM\invt \tilde s_{i+1})\gl_{i+1}\\
    \tilde q_{i+1}&=&M\invt \tilde s_{i+1}
        +\tilde q_i (\tilde r_i^tM\invt \tilde s_i)\inv \eta^{r^t}_i
        (\tilde r_{i+1}^tM\inv \tilde s_{i+1})
\end{eqnarray*}

As before, the algorithm proceeds by computing
$\rho_{i+1}$ and~$\sigma_{i+1}$ directly,
deriving $\eta^\ell_i$ and~$\eta^r_i$ from the 
orthogonality condition,
and from this
\[ \tilde r_{i+1}\leftarrow\rho_{i+1}\eta^{r\inv}_i,\quad
    \tilde s_{i+1}\leftarrow\sigma_{i+1}\eta^{\ell\inv}_i,\quad
    \gr_{i+1}=\eta^r_i\gr_i,\quad \gl_{i+1}=\eta^\ell_i\gl_i,\]
after which $p_{i+1}$ and~$q_{i+1}$ are computable.

\Level 1 {SYMMLQ}

According to the GMRES paper, SYMMLQ is based on an LQ factorisation
of the (tridiagonal) Hessenberg matrix.

\Level 0 {Minimum and quasi-minimum residual methods:
MINRES, GMRES, QMR, TFQMR}
\label{sec:resmin}

In this section we study those iterative methods that effect a
minimisation of the residual in each iteration over the subspace
generated. Such methods fall in two categories, sometimes only
differentiated by computation details.
\begin{itemize}
\item We recall from section \ref{sec:abstract-true-min}, and in
particular remark~\ref{lemma:ramr-uptri}, that minimisation of
residuals is equivalent to making $R^tAM\inv R$ diagonal\footnote
{Upper triangularity would suffice, but this is never used.}.
Methods such as GCR (section~\ref{sec:gcr}) and
GMRESr (section~\ref{sec:gmresr}) compute sequences of residuals and
search directions to satisfy this condition.
\item Other methods compute a basic sequence, in effect the conjugate
gradient method or a generalisation of it to nonsymmetric systems, and
take combinations of the residuals and iterates of this to obtain the
minimum value.
\end{itemize}

We lay the groundwork for these two approaches in the next two
sections. In section~\ref{sec:minimum-search} we derive conditions for
search directions in norm-minimising methods. In
section~\ref{sec:affine-res} we study in the abstract the process of
taking affine combinations so as to minimise the residual norm of the
result.

\Level 1 {Search directions in norm-minimising methods}
\label{sec:minimum-search}\index{Search directions|(}

We start with the generating equations of all iterative methods that
use search directions: \[P(I-U)=M\inv R,\qquad APD=R(I-J). \]
Multiplying the first by $R^tA$ we find that
\[ R^tAP(I-U)=R^tAM\inv R, \]
which is diagonal, hence $R^tAP$ is diagonal, and
\[ r_i^tAp_i=r_i^tAM\inv r_i.\]
Multiplying the second equation by $P^tA^t$ we get
\[ P^tA^tAPD=P^tA^tR(I-J) \] where the rhs is lower triangular and the
lhs symmetric in structure, hence diagonal. In conclusion we find that
$P^tA^tAP$ is diagonal and \[ (Ap_i)^t(Ap_i)d_ii=p_i^tA^tr_i. \]
The coefficients in~$U$ now have to be chosen to satisfy the
orthogonality of~$AP$.

\index{Search directions|)}

\Level 1 {Affine combinations of residuals}
\label{sec:affine-res}

Several methods have been proposed that generate a vector sequence
satisfying a minimization property by first generating the
sequence~$R$ in the usual way\footnote
{Or at least are equivalent to generating such a vector sequence.},
and subsequently taking combinations of this sequence.
This procedure can be applied both to sequences satisfying Arnoldi
and Lanczos orthogonalisation conditions.
In the Arnoldi case, the combinations can be taken so as to
minimize the $L_2$~norm of the residual; 
in the Lanczos case only a quasi-minimization results.

Let $R$ be a residual sequence satisfying $AM\inv R=RH$,
where $H$~is a Hessenberg matrix with zero column sums,
and construct a sequence~$G$ by taking
affine combinations, specifically, \[G=RV_1\]
with $V_1$~upper triangular with column sums equal to~1.
In lemma~\ref{lemma:combo-res-seq} we saw
that $G$~is then itself a residual sequence.

\begin{block}
We recall \eqref{eq:block-1colsum} for the block form of affine combinations:
\[ \forall_j\colon \sum_iu_{ij}=I_{k\times k}. \]
\end{block}

Since $V_1(J-E_1)$ is a Hessenberg matrix with zero
column sums, by lemma~\ref{H:zero:colsum} it can be written as
$V_1(J-E_1)=(I-J)V_2$. Therefore, for some upper triangular~$V_3$:
  \[ G(J-E_1)=RV_1(J-E_1)=R(I-J)V_2=-RHV_3, \]
where we used the fact that $H=(I-J)V_4$ for some upper triangular~$V_4$.

Thus we arrive at the following formulation for these combining methods:
\begin{equation}
        G(J-E_1)=\begin{cases}-RHV\cr -AM\inv RV\cr\end{cases},\qquad g_1=r_1
        \label{eq:minres}\end{equation}
where $V$ is an upper triangular matrix.

Using $RHV=AM\inv RV$, 
the solution update corresponding to \eqref{eq:minres} is
\begin{equation} Y(J-E_1)=-M\inv RV. \label{eq:minres-x}\end{equation}

The sequence~$G$ is again a residual sequence as already remarked in
lemma~\ref{lemma:combo-res-seq}; in particular $AM\inv G=G\bar H$,
where $\bar H=V_1^{-1}HV_1$ is a Hessenberg matrix with zero column
sums. This follows from $e^tV_1\inv H=e^tH=0^t$.

\begin{block}
We remark that for the transition $R(I-J)V_2=-RHV_3$ we need the fact
that $H$ has zero column sums in the strong sense of
\eqref{eq:block-H-zero-column}.
\end{block}

We could have told the above story in reverse: in \eqref{eq:minres-x}
we recognise the defining equation for polynomial method. In other
words, both $X$ and~$Y$ are methods for the same problem. By
lemma~\ref{lemma:affine-res} we then find that their residual
sequences $R$ and~$G$ are affine combinations of each other.

\begin{ccorollary}{Hessenberg matrices from methods for the same
problem are similar}
Let $X$ and $Y$ be polynomial methods for the same problem with
$x_1=y_1$ and let $R$ and~$G$ be their residual sequences and $H$
and~$\bar H$ the respective Hessenberg matrices, then $H$ and~$\bar H$
are similar.
\end{ccorollary}
\begin{proof}
This follows immediately from $\bar H=V_1^{-1}HV_1$.
\end{proof}

\Level 1 {General theory of $L_2$ minimization methods}
\label{sec:minres-method}

\Level 2 {Derivation of the coefficient minimization problem}
\label{sec:minres-derivation}

From the choice $g_1=r_1$, that is, $GE_1=RE_1$, we find 
in \eqref{eq:minres} that
\[ GJ=RE_1-RHV = R(E_1-HV), \]
that is, the particular choice of~$g_{n+1}$ is induced by 
a choice for~$v_n$.
We now aim at minimising the norm of the~$g_i$ vectors, that is,
we consider the minimisation problem
\begin{equation} \min_{v_n}\|g_{n+1}\|.
    \label{eq:true-min-problem}\end{equation}

Let $\Omega=\diag(R^*M\inv R)^{1/2}$, and define 
\[ N=R\Omega\inv, \qquad\tilde H=\Omega H\Omega^{-1}, \]
so that 
\begin{equation} AM\inv N=N\tilde H,
    \label{eq:AR=RH-normalised}\end{equation}
while $N$ consists of combinations of the
same Krylov sequence as~$R$.

Depending on the nature of the coefficient matrix we have the following
cases regarding~$\Omega$:
\begin{itemize}
\item In the real case with Arnoldi orthogonalisation,
$R^tM\inv R$ is diagonal,
so \[N^*M\inv N=N^tM\inv N=I.\]
\item In the real case with Lanczos bi-orthogonalisation,
$S^tM\inv R$ is diagonal,
but $R^tM\inv R$ is not.
\item In the complex symmetric case, $R^tM\inv R$ is diagonal,
but $R^*M\inv R$ is not.
\item In the complex nonsymmetric case, $S^tM\inv R$ is diagonal,
but neither $S^*M\inv R$ nor $R^*M\inv R$ nor $R^tM\inv R$ are.
\end{itemize}

If $R$~is an orthogonal sequence, that is, if we use Arnoldi orthogonalisation,
we have that $R^*M\inv R$~is diagonal, so we find~$N^*M\inv N=I$.
If $R$~is not orthogonal, as in the case of Lanczos orthogonalisation,
we only have $\diag(N^*M\inv N)=I$, but $N^*M\inv N\not=I$; we still have 
 \begin{equation} \|N_n\|_{M\inv,2}\leq\nobreak\sqrt{n}
 \label{eq:omega-norm}\end{equation}
(where the
$n$~subscript denotes that we take the block consisting of the
first~$n$ columns); see section~\ref{sec:quasi-min}.

If we assume orthogonal~$R$, we get the following derivation.
For $n\geq1$, $g_{n+1}-g_1$ is the $n$-th column of~$-RHV$;
see \eqref{eq:minres}. With $(RHV)_n=R_{n+1}H_nv_n$ this gives
\begin{eqnarray} \|g_{n+1}\|_{M\inv,L^2(C^N)}
&=& \left\| R_{n+1}e_1-R_{n+1}H_nv_n \right\|_{M\inv, L^2(C^N)} 
                                \label{eq:true-min-condition} \\
&=& \left\| \Omega_{n+1}e_1 - \Omega_{n+1}H_nv_n \right\|_{L^2(C^{n+1})} 
                                \label{eq:min-cond-smoothing} \\
&=& \left\| \|r_1\|e_1 - \tilde H_n\Omega_nv_n \right\|_{L^2(C^{n+1})}
                                          \label{eq:min-calculation}
\end{eqnarray}
(In the more general case of non-orthogonal~$R$, the transition from
\eqref{eq:true-min-condition} to \eqref{eq:min-cond-smoothing} is an
inequality $\|\cdot\|\leq\sqrt{n+1}\|\cdot\|$; this will be treated in
more detail in section~\ref{sec:quasi-min}.)

We now find two formulations of the minimisation problem:
\begin{itemize}
\item Observing that $H_n$ can be split as $(J-I)U$, we get from
\eqref{eq:min-cond-smoothing} the minimisation problem
\[ \min_{v_n}\left\| \Omega_{n+1}(e_1 - (J-I)v_n
\right\|_{L^2(C^{n+1})}. \]
This formulation is used in so-called `residual smoothing' methods;
see section~\ref{sec:smoothing}.
\item Making a further substitution $\tilde v_n=\nobreak \Omega_nv_n$,
(and $\tilde V=\nobreak \Omega V$),
we have replaced the minimisation problem~(\ref{eq:true-min-problem}) by
\begin{equation}
    g_{n+1}=g_1-AM\inv N\tilde v_n\qquad\hbox{where
    $\displaystyle \tilde v_n\colon\min_{v_n}\left\| \|r_1\|e_1 
    - \tilde H_n v_n \right\|_{L^2(C^{n+1})}$},
    \label{eq:quasi-min-problem}\end{equation}
and we focus on solving this problem instead.
The minimisation problem involving~$H_n$ is solved by a QR
factorisation; this formulation is used in methods such as MinRes,
GMRES, and QMR.
\end{itemize}

\begin{remark}
\Eqref{eq:quasi-min-problem} is a minimisation problem
that can be solved
regardless of conditions on~$N$. It is only when $N$ is orthonormal
that minimising~$\tilde v_n$ is equivalent to minimising~$g_{n+1}$.
This is the basis for methods such as 
MINRES~\cite{PaSa:indefinite}, see section~\ref{sec:minres},
and GMRES~\cite{SaadSchultz:gmres}, see section~\ref{sec:gmres}.
We consider the general case where $N$ is not orthonormal,
which leads to such methods as QMR~\cite{FrNa:qmr},
in section~\ref{sec:quasi-min}.
\end{remark}

The solution of \eqref{eq:quasi-min-problem} is explained in
section~\ref{sec:minres-solution}

\begin{remark}\label{gmres-from-norm}
By substituting away the $\Omega$ scaling, we have removed
all reference to the $R$~sequence in this problem: it is now
completely expressed in terms of $\tilde H$ and the $N$~sequence,
which are related by~\eqref{eq:AR=RH-normalised}.
These can be generated directly from~$r_1$, without constructing
the rest of the $R$~sequence.
This is the Arnoldi similarity transformation;
see section~\ref{sec:arnoldi-transform}
\end{remark}

\Level 2 {Solution of the coefficient minimization problem}
\label{sec:minres-solution}

We now consider the coefficient minimization
problem~\eqref{eq:quasi-min-problem}:
\[     g_{n+1}=g_1-AM\inv N\tilde v_n\qquad\hbox{where
    $\displaystyle \tilde v_n\colon\min_{\tilde v_n}\left\| \|r_1\|e_1 
    - \tilde H_n\tilde v_n \right\|_{L^2(C^{n+1})}$}. \]

Let $\tilde H_n=Q_nU_n$ be a decomposition into an orthonormal
matrix and an upper triangular matrix, where $Q_n$ is of 
size~\hbox{$(n+1)\times n$} and $U_n$~of size~\hbox{$n\times n$}.
Define
\[ \bar Q_n = \left[\begin{matrix}&*\cr Q_n&\vdots\cr &*\cr\end{matrix}\right],
        \qquad \bar U_n = \left[\begin{matrix}&U_n\cr 0&\cdots&0\cr\end{matrix} \right] \]
where the final column of~$\bar Q_n$ is chosen to make~$\bar Q_n$
a square orthonormal matrix,
then, again, $\tilde H_n=\nobreak \bar Q_n\bar U_n$.
We now find
\[ \left\| \|r_1\|e_1 - \tilde H_n\tilde v_n \right\|_{L^2(C^{n+1})}
   =\left\| \|r_1\|q^{(n+1)} - \bar U_n\tilde v_n \right\|_{L^2(C^{n+1})}, \]
where $q^{(n+1)^t}=(q_{11},\ldots,q_{1n+1})$.
If we solve
\begin{equation}
        \tilde v_n=U_n\inv q^{(n+1)}_{1:n}\|r_1\|,
        \label{minres:usol}\end{equation}
the first~$n$ components of $\bar U_n\tilde v_n-\|r_1\|q^{(n+1)}$ are zero,
so the minimum over all~$\tilde v_n$ is attained for this value\footnote
{QR decompositions are unique up to scaling as $QD$ and $D\inv R$
where $|d_{ii}|=\nobreak 1$. In \eqref{minres:usol} this scaling
drops out again, so $\tilde v_n$~is uniquely determined.}%
\footnote{This argument is still not airtight.},
and the value of the minimum is~$|q_{1n+1}|\|r_1\|$.

Formula~(\ref{minres:usol}) can be summarised for all~$n$
as $\tilde V=U\inv\tilde Q^t$, where $\tilde Q$~is
formed from~$Q$ as
        \[ \tilde Q^t=\begin{pmatrix}q_{11}&q_{11}&\cdots&q_{11}\cr
                        &q_{12}&\cdots&q_{12}\cr
                        &\emptyset&\ddots&\vdots\cr
                        &&&q_{1n}\end{pmatrix}. \]

\Level 2 {Solution vector update for minimization problems}

Above, we found that
\[ \|g_{n+1}\|_{M\inv,L^2(C^N)}=|q_{1n+1}|\|r_1\|, \]
so for the purposes of a stopping test on this norm we do not
explicitly need to form the solution vector or the residual.
Instead, these can be formed {\em in toto} when the decision to 
terminate the iteration is made.

In case formation of the solution is needed, the following applies.

From $G(J-E_1)=-AM\inv N\tilde V$ we first of all find a defining formula
for the iterates sequence:
\begin{equation}
    X(J-E_1)=-M\inv N\tilde V=-M\inv NU\inv \tilde Q^t.
    \label{eq:minres-xconstruct}\end{equation}
For an updating formula for~$X$ we find the even simpler formula
\begin{equation}
    X(J-I) = -M\inv P\,{\rm diag}(q_{1i}),\qquad P=NU\inv
    \label{eq:minres-xupdate}\end{equation}
The elements of~$P$ can be easily updated from the $r_n$~vectors, in the
case of QMR with a three-term recurrence. In GMRES updating~$p_n$
requires all previous such vectors to be stored. Hence, people have
considered truncated or restarted versions of the method.

Furthermore, for the minimised residuals~$G$ we find similarly:
        \[ G(J-I) = -ANU\inv {\rm diag}(q_{1i}) 
                = -N\tilde HU\inv {\rm diag}(q_{1i})
                = -NQ {\rm diag}(q_{1i}). \]

\Level 2 {Relation of residual norm to original iterations}

In~\cite{CuGr:relations} it is shown that
\[ \|r\sp k_F\|_2={\|r\sp k_G\|_2\over
    \sqrt{1-(\|r\sp k_G\|_2/\|r\sp{k-1}_G\|_2)^2}}. \]

\Level 2 {Stagnation of the minimised iterations}

Investigate the meaning of
$\|r\sr{gmres}\sp{n+1}\|=\|r\sr{gmres}\sp{n}\|$.

\Level 1 {MINRES: The Minimum Residual method}
\label{sec:minres}\index{MINRES|(}

In the above we derived how, given a sequence~$R$ satisfying $AM\inv R=RH$,
one derives affine combinations that mimise some norm.
According to section~\ref{sec:affine-res}, taking such combinations
and applying them to a residual sequence, gives again a residual sequence.
In making the transition from 
the residual minimisation problem \eqref{eq:true-min-problem}
to the coefficient minimisation problem \eqref{eq:quasi-min-problem}
we used the orthogonality of~$R$. 

We know from section~\ref{sec:arnoldi-cg} that,
with $A$ and~$M$ symmetric, $R$~can be made orthogonal
with a three-term recurrence.
The resulting minimisation method is known as MINRES~\cite{PaSa:indefinite}.

\begin{question} 
What is LSQR?
\end{question}

\index{MINRES|)}

\Level 1 {GMRES: The Generalised Minimum Residual method}
\label{sec:gmres}\index{GMRES|(}

\begin{question}
what is the relation between the gmres hessenberg matrix and the one
that governs the residuals?
\end{question}
\begin{question}
can we do something with scaling or combinations of the arnoldi
sequence, instead of the minimisation?
\end{question}

\Level 2 {The Saad and Schultz algorithm}

The GMRES, for Generalized Minimal Residual~\cite{SaadSchultz:gmres}, method
is found by applying the above minimisation and using the
substition of remark~\ref{gmres-from-norm}.
Thus the algorithm proceeds as follows:
\begin{enumerate}
\item\label{step:gmres-arnoldi}
 Derive an orthonormal sequence $N$ and upper Hessenberg matrix satisfying
\[ AM\inv N=NH,\qquad N^tM\inv N=I,\qquad n_1=r_1/\|r_1\|. \]
More about this in section~\ref{sec:arnoldi-transform}.
\item\label{it:gmres-error}
Make a QR decomposition: $H_i=Q_iU_i$ where $Q_i$ is 
of size~\hbox{$i+1\times i$}.
Generate an $i+1$-st column $q_{i+1}$ of $Q_i$, such that
the extended $Q_i$ is again orthonormal;
then the residual error is~$|q_{1i+1}|\|r_1\|$.
\item\label{it:gmres-solve-v} If this error is deemed small enough, solve
\[ v_i=U_i\inv (q_{11},\ldots q_{1i})^t \|r_1\| \]
and update residual and solution
\[ x_i=x_1-M\inv N_{*,1:i}v_i; \qquad r_i=r_1-AM\inv N_{*,1:i}v_i. \]
\item If the restart parameter is reached, perform the above update
regardless the error estimate, let
\[ r_1\leftarrow r_i,\qquad x_1\leftarrow x_i, \]
and restart the whole algorithm.
\end{enumerate}

A matlab implementation can be found in section~\ref{sec:matlab-gmres}.

\Level 2 {Breakdown of GMRES}\label{sec:gmres-breakdown}

In the above algorithm, there are two opportunities for breakdown:
\begin{itemize}\item if step~\ref{step:gmres-arnoldi}
breaks down, that is the sequence~$N$ can not be continued since we
have found an invariant subspace, or,
\item if in step~\ref{it:gmres-solve-v} the matrix~$U_i$ is singular.
\end{itemize}
By the analysis in section~\ref{sec:arnoldi-transform}, case~1
implies that we can form the exact solution of the linear system.
Case~2 implies that $H$~is not an irreducible upper Hessenberg matrix,
which also implies (by theorem~\ref{cor:next-r})
that we have found an invariant subspace.

We conclude that GMRES does not break down, other than by finding the
true solution.

\Level 2 {Implementation of the QR decomposition}

In the original GMRES paper~\cite{SaadSchultz:gmres}, Saad and Schultz
derived \eqref{minres:usol}, and the error estimate in
item~\ref{it:gmres-error} above, from a specific implementation of the
QR decomposition by Givens rotations.  However, it follows from the
exposition in section~\ref{sec:minres-solution} that this is merely an
implementation detail.  A~decomposition by
Gram-Schmidt\index{Gram-Schmidt} orthogonalisation is equally
feasible; see section~\ref{sec:gram-schmidt}.

\Level 2 {Direct computation of the GMRES vectors}
\label{sec:gmresr}

The GMRES method used an indirect method to construct the residuals:
it did not use either a recurrence in terms of older residuals, or
an update formula using search directions. This does not mean it is
not possible to do so; see section~\ref{sec:gcr}

\Level 2 {Left-preconditioned GMRES}

Based on corollary~\ref{left-gmres-basic} we can also generate the
Arnoldi vectors from
\[ v_1=M\inv(Ax_1-b),\quad n_1=v_1/\|v_1\|; \qquad M\inv AN=VN. \]

\index{GMRES|)}

\Level 1 {Residual-minimising Arnoldi methods for the non-symmetric
case: GCR, Orthodir, GMRESr}
\label{sec:gcr}

There are several algorithms based on Arnoldi orthogonalization
for the nonsymmetric case that minimise the residual norms in each iteration.
This makes them mathematically equivalent to GMRES
(section~\ref{sec:gmres}) in the sense that
they derive the same iterates and residuals.
However, unlike GMRES (see section~\ref{sec:gmres-breakdown})
they are subject to breakdown.

The Generalized Conjugate Residual method (GCR; see next)\index{GCR},
OrthoDir~\cite{YoungJea1980:generalizedcg}\index{OrthoDir},
OrthoMin~\cite{Vi:orthomin}\index{OrthoMin}, Axelsson's
method~\cite{Ax:unsymmetricinconsistent}\index{Axelsson's method},
were derived in the 1970s and 1980s but that have by now largely
slid into obscurity.

The GCR algorithm~\cite{Elman:thesis} can be described as 
in figure~\ref{fig:cgr}.
\begin{rfigure}
\begin{tabbing}
$r_1=p_1=Ax_1-b$\\
for \=$i=1,\ldots$\\
\>let $\alpha_i=r_i^tAp_i/(Ap_i)^t(Ap_i)$.\\
\>update \=$x_{i+1}=x_i-\alpha_ip_i$\\
\>\>$r_{i+1}=r_i-\alpha_iAp_i$\\
\>let \=$p_{i+1}=r_{i+1}+\sum_{j\leq i}p_j\beta_{ji}$\\
\>\>where $\{\beta_{ji}\}$ are chosen\\
\>\>such that $(Ap_{i+1})^t(Ap_j)=0$ for $j\leq i$.
\end{tabbing}
\caption{Generalized Conjugate Residual method}\label{fig:cgr}
\end{rfigure}
Since this algorithm orthogonalises the $Ap_i$ series to itself,
we know from section~\ref{sec:min-general-inprod}
that this is equivalent to making the residuals $A$-orthogonal
($AM\inv$-orthogonal for preconditioned methods),
which by corollary~\ref{th:gmres} minimises the residuals.

A simple example of the breakdown of this method is given in~\cite{SaadSchultz:gmres}:
with \[ A=\begin{pmatrix}0&1\cr -1&0\end{pmatrix},\qquad b=\begin{pmatrix}1\cr 1\end{pmatrix} \]
we get $\alpha_1=0$, which leads to $r_2=r_1$.
Continuing this, $p_2=0$, so computing $\alpha_2$ gives a 
divide-by-zero error.

Van der Vorst and Vuik~\cite{VoVuik:gmresr}
describe a method along the following lines\index{GMRESr}:
\begin{itemize}
\item let $p_n=M\inv r_n$ and $q_n=AM\inv r_n=Ap_n$;
\item modify $p_n\leftarrow p_n-\sum_{k<n}c_kp_k$,
$q_n\leftarrow q_n-\sum_{k<n}c_kq_k$, such that the $q_i$~family
is orthogonal;
\item normalize by dividing $q_n$ and $p_n$ by~$\|q_n\|$;
\item update \[ r_{n+1}=r_n-q_n (q_n^tr_n). \]
\end{itemize}
In this we recognise the basic GCR scheme with preconditioning
incorporated\footnote{It should be noted that van der Vorst and Vuik's
derivation of the method is along completely different
lines.}. A~practically interesting proposition is to let the
preconditioner be an embedded iterative method, such as GMRES
itself. In van der Vorst and Vuik's terminology that makes it a
`recursive GMRES' method; Saad calls it `flexible GMRES', but there is
a difference between GMRESr and fGMRES.

Van der Vorst and Vuik propose to solve the breakdown problem by
incorporating a `LSQR-switch', that is, in case of breakdown they
propose to let~$p_n\leftarrow\nobreak A^tr_n-\sum\ldots$. Van der
Vorst and Chan~\cite{VorstChan:iterative} observe that a sufficient
condition for avoiding breakdown ($\|c\|_2=0$) is that the norm of the
residual at the end of an inner iteration is smaller than the
right-hand residual: $\|Az^{(m)}-r_i\|_2<\|r_i\|_2$.

\Level 1 {Quasi-minimisation}\label{sec:quasi-min}

The simplifying step from the condition~(\ref{eq:true-min-condition})
to the practically computable form~(\ref{eq:min-calculation})
uses the $M\inv$-orthogonality of~$R$.
In methods such as BiCG, which are based on Lanczos bi-orthogonalisation,
$R$~is not itself orthogonal.
However, defining as before $\Omega=\diag(R^*M\inv R)^{1/2}$, and
$N=R\Omega\inv$ (which satisfies $\diag(N^*M\inv N)=\nobreak I$) 
we have the inequality
\[ \|R_{n+1}x\|_{M\inv,L^2} \leq \|N\|_{M\inv,L^2}\|x\|_{L^2}
    \leq \sqrt{n+1}\|x\|_{L^2}, \]
where we used the following lemma
\begin{lemma}
If $N$ of size $m\times n$ satisfies $\diag(N^*M\inv N)=I$,
then $\|N\|_{M\inv}\leq\sqrt m$.
\end{lemma}
\begin{proof}
Recall the definition of the matrix norm:
\[ \|N\|=\min_{\|x\|=1}|Nx|, \]
and, using the Cauchy-Schwarz inequality and the elementary
fact that $2xy\leq\nobreak x^2+\nobreak y^2$ for all $x$ and~$y$,
\begin{eqnarray*}
    |Nx|_{M\inv}^2 &=& \sum_{i,j}x_ix_j\bar n_i^*M\inv \bar n_j \\
    &\leq& \sum_i x_i^2+2\sum_{i<j}x_ix_j \\
    &\leq& \sum_i x_i^2 +\sum_{i<j}(x_i^2+x_j^2) = n\sum_ix_i^2 = n.
\end{eqnarray*}
\end{proof}

This $\sqrt{n}$ term also appears when we consider the complex symmetric
conjugate gradient method, where $R^tM\inv R$ is diagonal, but the
vector space would demand $R^*M\inv R$ to be diagonal for
true minimisation.

\Level 1 {QMR}
\label{sec:qmr}\index{QMR|(}

The Quasi-Minimum Residual method of Freund and Nachtigal~\cite{FrNa:qmr}
arises from applying quasi-minimisation to the BiCG method.

\index{QMR|)}

\Level 1 {Residual smoothing}
\label{sec:smoothing}\index{Residual smoothing|(}

Several authors 
\cite{Sch:scientific-vector,We:convergence-cg,ZhWa:residual-smoothing}
have investigated methods that take an iterate sequence~$\{x_i\}$
and improve on it by taking affine combinations of the form
\[ y_1 = x_1,\qquad y_{i+1} = (1-\eta_i)x_i+\eta_iy_{i+1}. \]
The scalars~$\eta_i$ are determined in such a way that the residuals
are minimised in some sense.

\begin{lemma}
The residuals $g_i=Ay_i-b$ corresponding to the smoothed sequence
are a residual sequence iff $r_i=Ax_i-b$ is a residual sequence.
\end{lemma}
\begin{proof}
The $g_i$ residuals are affine combinations of the $r_i$ residuals,
hence by lemma~\ref{lemma:combo-res-seq} also a residual sequence.
To see this,
we find the following relation between the $G$~and~$R$ sequences:
\[ G\begin{pmatrix}1&\eta_1-1\cr &1&\eta_2-1\cr &&\ddots&\ddots\cr\end{pmatrix}
        = R\begin{pmatrix}1\cr &\eta_1\cr &&\eta_2\cr&&&\ddots\end{pmatrix} \]
or $GN=RD$ where $e^tN=e^tD$, i.e., $N$~and~$D$ have the same
column sums. 
It follows that $G=RDN\inv$ where $DN\inv$ is an upper triangular
matrix with column sums~${}\equiv1$. Thus, $G$~is a affine combination
of residuals as described in section~\ref{sec:affine-res}, and
it is itself again a residual sequence.\end{proof}

\begin{block}
In the case of block vectors, the scalars $\eta_i$ become
$k\times\nobreak k$ blocks.
If they are of diagonal form, the block method reduces to a number of
scalar methods executed at the same time.
In the general case, $e^tN=e^tD$ still holds, so $DN\inv$~has 
colum sums~${}\equiv1$ in the stronger sense of \eqref{eq:block-1colsum}.
\end{block}

In practice, residual smoothing is used to minimise the
norm of the newly created $g_i$~residuals. We have already seen
methods for this purpose, and in fact they can be reproduced by
residual smoothing methods.

The following discussion will pertain both to symmetric and nonsymmetric
real and complex systems.
Let $R$ be the sequence of right residuals of the conjugate
gradient method, and $S$~the left residuals of the bi-conjugate
gradient method for nonsymmetric system; for symmetric systems
we will implicitly assume~$S\equiv\nobreak R$.

In section~\ref{sec:affine-res} we showed that any affine combination
vector~$g_n$ of the residuals~$r_1,\ldots,r_n$ satisfies the following
formula: \[ g_{n+1}-g_1=-AM\inv R_nv_n=-R_{n+1}H_nv_n \] for some
vector~$v_n$, where $H_n=H_{[n+1,n]}$~is the $(n+1)\times
n$~Hessenberg matrix describing the iterative method, and
$R_n=(r_1,\ldots,r_n)$.  Since $g_1=r_1=R_ne_1$ (for any~$n$), this
leads to the minimisation problem
\[\min\|g_{n+1}\|=\min\|R_{n+1}(e_1-H_{[n+1,n]}v_n)\|.\]
This is used to generate minimising or quasi-minimising residuals, as
we have seen in section~\ref{sec:minres-derivation}

Since $H_n$ can be factored as $H_n=(J-I)_{[n+1,n]}U$
with
 \[ J-I=\begin{pmatrix}-1\cr 1&-1&&\emptyset\cr \emptyset&\ddots&\ddots\cr\end{pmatrix},\qquad
    \hbox{$U$ upper triangular}, \]
there is likewise a vector~$z_n$ such that
 \begin{equation} g_{n+1}=R_{n+1}(e_1-(J-I)_{[n+1,n]}z_n).
 \label{eq:g-direct}\end{equation}
The minimisation problem to be solved is then
\begin{equation} \min_{z_n}\|g_{n+1}\|=\min_{z_n}\|R_{n+1}v_{n+1}\|.
     \label{eq:g-min}\end{equation}
where we define $v_{n+1}=e_1-(J-I)z_n$.

We now consider the minimisation problem~(\ref{eq:g-min})
in more detail.
In the symmetric case,
 \[ \|g_{n+1}\|_{M\inv,2}=\|R_{n+1}v_{n+1}\|_{M\inv,2}=
        \|N_{n+1}\Omega^{1/2}v_{n+1}\|_{M\inv,2}=
        \|\Omega^{1/2}v_{n+1}\|_2. \]
In the nonsymmetric case, we get from~(\ref{eq:omega-norm}):
 \begin{eqnarray} \|g_{n+1}\|_{M\inv,2}&=&\|R_{n+1}v_{n+1}\|_{M\inv,2}=
    \|N_{n+1}\Omega^{1/2}v_{n+1}\|_{M\inv,2}            \nonumber\\
    &\leq& \|N_{n+1}\|_{M\inv,2}\,\|\Omega^{1/2}v_{n+1}\|_2 \nonumber\\
    &\leq& \sqrt{n+1}\|\Omega^{1/2}v_{n+1}\|_2.
    \label{eq:quasi-bound}\end{eqnarray}

Hence, solving the minimisation problem
 \begin{equation} \min_{z_n}\|\Omega^{1/2}v_{n+1}\|_2
     \label{eq:omega-min}\end{equation}
gives the optimal solution in the Krylov basis,
exactly in the symmetric case,
up to a factor of~$\sqrt{n}$ in the nonsymmetric case.
The minimisation problem~(\ref{eq:omega-min}) is also
essentially the one considered in~\cite{Fr:cgcomplexsymmetric}
for the complex symmetric case,
although this theoretical justification was not given there.

In the traditional derivation of the QMR method, the minimisation
problem~(\ref{eq:omega-min}) was solved by $QR$ factorisations, as
in~\cite{Fr:cgcomplexsymmetric,FrNa:qmr}.
Here we are deriving the residual smoothing method of solving 
the same minimisation problem, hence deriving the same iterates.
We follow the discussion in~\cite{ZhWa:residual-smoothing}.

Note that the minimisation problem
 \[ \tau_k^2=\min_{z_n}\|\Omega^{1/2}(e_1-(J-I)z_n)\|_2 \]
is of the form~$\min_x\|Ax-b\|$.
In the complex case, $z_n$~can be complex, but $A$~and~$b$ are real,
so the solution, given by $x=(A^*A)\inv A^*b$ is real too.

An explicit form for the minimum, and the value of~$v$ for
which it is taken, can be given.
With $z_n=(\zeta_1,\ldots,\zeta_n)^t$,
 \[\tau_k^2=\min_z\omega_1(1-\zeta_1)^2+\omega_2(\zeta_1-\zeta_2)^2
    +\cdots+\omega_n(\zeta_{n-1}-\zeta_n)^2+\omega_{n+1}\zeta_n^2.
 \]
Changing variables $\xi_1=1-\zeta_1$, $\xi_k=\zeta_{k+1}-\zeta_k$ for
$k=2,..,n$, $\xi_{n+1}=\zeta_n$, the minimisation problem becomes
 \[ \tau_n^2=\min_{\sum\xi_i=1}\sum\omega_i\xi_i^2 \]
for which the unique minimiser is given by
 \[ \xi_i = {1/\omega_i\over \sum_j {1\over\omega_j}} \]
and the resulting value of the minimum is
 \[ \tau_n=\sqrt{1\over\sum_i^n1/\omega_i}. \]

\begin{lemma}
The minimimum of $f(x)=\sum\omega_i^2x_i^2$ under the constraints
$\omega_i>0$, $\sum x_i=1$ is given by
$x_i=\omega_i\inv/\sum_j\omega_j\inv$.
\end{lemma}
\begin{proof}
The constraint equation translates to $\sum_i\partial x_i/\partial x_j=0$ for
all~$j$. The case $\omega_ix_i\equiv c$ gives $x_i=c/\omega_i$. With
\[ 1=\sum x_i=c\sum\omega_i\inv \Rightarrow c=1/\sum_i\omega_i\inv \]
this becomes $x_i=\omega_i\inv/\sum_i\omega_i\inv$.
In this value of~$x$, 
\[ {\partial f\over \partial x_j}=\sum_i\omega_i^2{\partial x_i^2\over \partial x_j}
  = {2\over \sum_i\omega_i\inv}\sum_i\omega_i{\partial x_i\over \partial x_j}
  \begin{cases}\leq {2\over \sum_i\omega_i\inv}
               \omega\sb{max}\sum_i{\partial x_i\over \partial x_j}=0\cr
         \geq {2\over \sum_i\omega_i\inv}
               \omega\sb{min}\sum_i{\partial x_i\over \partial x_j}=0\cr\end{cases}
  =0.\]
\end{proof}


We thus find that $L_2$ quasi-minimisation of the 
norm of the coefficient
vector~$z_k$ gives coefficients~$\tau_k$ satisfying
 \begin{equation}
     {1\over \tau_k^2}={1\over \tau_{k-1}^2}+{1\over\omega_k},\quad
    \tau_1^2=\omega_1,\qquad
    \hbox{where $\omega_k=\|s_k^tM\inv r_k\|_2$.}
        \label{eq:smooth-h}\end{equation}
From the discussion in~\cite{ZhWa:residual-smoothing} we know
that these $\tau_k$~coefficients generate the (quasi) minimising
residuals:
 \[ \tau_{k+1}^{-2}g_{k+1}=\tau_{k}^{-2}g_k+\omega_k^{-1}r_{k+1}.\]

We summarise the resulting algorithms in figure~\ref{fig:cg-rs}.

\begin{figure}
    \begin{tabbing}
Let $A$ be a matrix, and let $M$ be a preconditioner for~$A$,
and let $b$ be given;\\
let $x_1$ by arbitrary, and $r_1=Ax_1-b$;\\
let $s_1=r_1$ in the symmetric case, or arbitrary in the nonsymmetric case.\\
Iterate \=for $i=1,\ldots$:\+ \\
 otherwise, \=\kill
 let $\rho_i=s_i^tM\inv r_i$ and $\omega_i=r_i^*M\inv r_i$\\
 calculate the smoothed solution and residual:\\
 if $i=1$, $\tau_1=\rho_1$, $g_1=r_1$, $y_1=x_1$\\
 otherwise, \>$\tau_i=1/(\tau_{i-1}\inv+\omega_i\inv)$ and\\
            \>$y_i=\tau_i(\tau_{i-1}\inv y_{i-1}+\omega_i\inv x_{i-1})$\\
            \>$g_i=\tau_i(\tau_{i-1}\inv g_{i-1}+\omega_i\inv r_{i-1})$\\
 calculate search direction and update the original residual:\\
 if $i=1$, $p_i=r_i$, ($q_1=s_1$ in the nonsymmetric case), $\tau_1=\omega_1$\\
 otherwise, \>$\beta=\rho_i/\rho_{i-1}$\\
           \> $p_i=r_i+\beta p_{i-1}$\\
           \> $q_i=s_i+\beta q_{i-1}$ (nonsymmetric only)\\
 and with \= \kill
 with \>$\alpha=\rho_i/p_i^tAp_i$\\
 update \=$x_{i+1}=x_i-\alpha p_i$\\
 and \>$r_{i+1}=r_i-\alpha Ap_i$\\
 \> $s_{i+1}=s_i-\alpha A^tq_i$
    \end{tabbing}
\caption{The residual smoothed conjugate or bi-conjugate gradient algorithm,
for real and complex systems.}
\label{fig:cg-rs}
\end{figure}

\begin{lemma}
Let $G$ be a residual smoothing sequence, that is, let $G=RB^{-1}$,
where $R$~is a residual sequence and $B$~is upper triangular.
If every~$g_i$ is the minimum norm combination of~$r_j$ ($j\leq i$),
then $B$~is bidiagonal.
\end{lemma}
\begin{proof} Let $AR=RH$ be the defining relation for the residual sequence~$R$,
let~$\Omega$ be the diagonal matrix with~$\omega_i=\|r_i\|$,
let $N=R\Omega^{-1}$~be the normalised residuals, and
let $\tilde H=\Omega H\Omega^{-1}$~be the Hessenberg matrix
satisfying~$AN=N\tilde H$.
Let $\tilde H=QU$ and~$H=\hat Q\hat U$ be $QR$~decompositions,
and define~$D_Q={\rm diag}(\omega_1q_{qi})$.

With the results from section~\ref{sec:minres-derivation}
and equation~\ref{eq:H-Om-U},
we find that for a minimised sequence~$G$:
\begin{eqnarray*} G(J_I)
        &=& AR\Omega^{-1}U^{-1}D_Q\|r_1\| \\
        &=& RH\Omega^{-1}U^{-1}D_Q\|r_1\| \\
        &=& R(J-I)B_1^{-1}D_Q\|r_1\|
        \end{eqnarray*}

We now have that
$G(J-I)D_Q^{-1}B_1=R(J-I)$. Clearly, $D_Q^{-1}B_1$~is upper bidiagonal.
Using equation~\ref{eq:B1-def}:
\[ D_Q^{-1}B_1e=D_Q^{-1}Q^t\Omega(J-I)e =
        \begin{pmatrix}1&*\cr 1&*&*\cr \vdots&\vdots&\vdots&\ddots\cr\end{pmatrix}
                \begin{pmatrix}\omega_1\cr 0\cr \vdots\cr\end{pmatrix} =
        \omega_1e \]
it follows that $D_Q^{-1}B_1$~also has constant row sums.
By lemma~\ref{lemma:JI-right-fac} there is then an upper bidiagonal~$B_3$
such that~$GB_3(J-I)=R(J-I)$. From this it follows that $G$~is a 
residual smoothing method.\end{proof}

\index{Residual smoothing|)}

\Level 1 {TFQMR}

Applying residual smoothing to the CGS method (see section~\ref{sec:cgs})
gives the Transpose-Free QMR method of Freund~\cite{Fr:tfqmr}.

\input cgs

\Level 0 {Arnoldi methods}

In this monograph we have repeatedly considered the Arnoldi
orthogonalisation, that is, generating a sequence~$R$ that
is orthogonal to itself under some inner product.
Usually we considered the case where $R$ was a residual sequence.

There is another application for the Arnoldi method. If
\[ AV=VH\qquad\hbox{and}\qquad V^tV=I \]
then
\begin{equation} V^tAV=H \label{eq:arnoldi-sim}\end{equation}
is a similarity transformation to upper Hessenberg
(or in case of symmetry, tridiagonal)
form. Many eigenvalue methods perform this reduction as
a preliminary step.

In this section we  will investigate
what is usually termed the `Arnoldi method', which is the above similarity
transformation.
Methods based
on Arnoldi orthogonalisation that generate residual sequences
are considered in section~\ref{sec:resmin}.

\Level 1 {The Arnoldi algorithm}
\label{sec:arnoldi-transform}

Here we will give the algorithm for the Arnoldi similarity 
transformation \eqref{eq:arnoldi-sim}; we will immediately formulate
it in block form, where each vector is of size~$n\times\nobreak b$.
\begin{algorithm}{Arnoldi reduction to Hessenberg form}
Let a matrix $A$ and a vector $v_1$ be given.
\begin{enumerate}
\item Assume inductively that
\[ v_i^tv_i=I_{b\times b}\qquad\hbox{and}\qquad
   \forall_{j<i}\colon v_j^tv_i=0_{b\times b}. \]
\item Let $u\leftarrow Av_i$.
\item\label{arnoldi-make-ortho} For all $j\leq i$ modify
\[ u\leftarrow u-v_j(v_j^tu)\inv. \]
\item\label{arnoldi-make-normal} Let $w$ be square of size $b\times\nobreak b$
such that $u^tu=(ww^t)\inv$, and define \[ v_{i+1}\leftarrow uw. \]
\end{enumerate}
\end{algorithm}

In sum, \[ v_{i+1}w = Av_i-\sum_{j\leq i}v_j(v_j^tu)\inv
\quad\Rightarrow\quad Av_i = v_{i+1}w + \sum_{j\leq i}v_j(v_j^tu)\inv, \]
which gives us the coefficients of the Hessenberg matrix relating the
various quantities in $AV=VH$.

One easily sees that step~\ref{arnoldi-make-ortho} guarantees that $v_{i+1}$
is inductively orthogonal to all $v_j$ with~$j\leq\nobreak i$;
step~\ref{arnoldi-make-normal} guarantees that 
$v_{i+1}^tv_{i+1}=\nobreak I_{b\times b}$.
In the scalar case, $w$~is simply $\pm1/\sqrt{u^tu}$; 
in the block case we make a QR decomposition $u=\nobreak qr$,
and choose~$w=\nobreak r\inv$.
QR decompositions are uniquely determined up a diagonal matrix~$D$
satisfying~$D^2=\nobreak I$.

This process is needed in the GMRES algorithm (section~\ref{sec:gmres}).
We can analyze its breakdown as follows:
\begin{itemize}
\item The orthogonalisation in step~\ref{arnoldi-make-ortho} can be
omitted if $v_j^tu=0$, so this step can not break down.
\item In step~\ref{arnoldi-make-normal} the occurrence of~$ww^t=\nobreak0$
implies that we have found an invariant subspace,
so by corollary~\ref{cor:solution} we can form the solution to the linear
system.
\end{itemize}

\Level 1 {Characterisation of the Arnoldi sequence}

Based on a Krylov sequence~$K$, many sequences can be formed by taking
combinations~$R=\nobreak KU$.  The sequences are completely characterised by
the upper triangular matrix~$U$; for the subset of the residual
sequences the only restriction on~$U$ is that $u_{1j}\equiv\nobreak1$.
We now address the question how to characterise the Arnoldi sequence
--~which is not a residual sequence~--
in terms of its matrix~$U$.  Since the Arnoldi sequence is uniquely
determined (up to signs) from the first vector
(section~\ref{sec:arnoldi-transform}), there has to be a matrix~$U$,
unique up to sign scaling, derivable from the Krylov
sequence.

The determining characteristic of the Arnoldi sequence is that it
satisfies $V^tV=\nobreak I$. 
Since the Arnoldi sequence is a combination $V=KU$ of the Krylov sequence,
we have \[ U^tK^tKU=I\quad\Rightarrow\quad U\invt U\inv=K^tK, \]
\index{moments}
that is, $U$~is the inverse Cholesky factor of the moments matrix~$K^tK$.

For the Arnoldi sequence, $K=VU\inv$ is a QR factorisation of the 
Krylov sequence.  This is unique up to scaling as $V\leftarrow VD$
and $U\inv\leftarrow DU\inv$ by a diagonal matrix~$D$ with $D^2=I$.
In $K^tK=U\invt U\inv$ this diagonal scaling drops out.

The Hessenberg matrix $H$ in $AV=VH$ is $H=U\inv JU$.
This makes $H$ determined up to scaling~$DHD$.
In particular, the diagonal of~$H$ is invariant.

\begin{llemma}{Uniqueness of Arnoldi method}
If $AV_1=V_1H_1$ with $V_1^tV_1=I$ and $AV_2=V_2H_2$ with $V_2^tV_2=I$,
then there is a diagonal matrix~$D$ with $D^2=I$ such that
\[ V_1=V_2D,\qquad H_1=DH_2D. \]
\end{llemma}

Alternative deriviation.
The Arnoldi sequence $V$ satisfies an equation $AV=VH$ just like all
sequences that are derived by taking combinations of a Krylov sequence;
see lemma~\ref{Hess-Krylov}.
From this, we find $V^tAV=H$. Now use \eqref{eq:hess-from-u}: $H=U\inv JU$,
where $V=KU$, to find
\[ V^tAV=\begin{cases}H=U\inv JU\cr U^tK^tAKU=U^tK^tKJU\cr\end{cases} \]
from which
\[ K^tK=U\invt U\inv. \]
This requires a little care. We can only partly `divide out' the
singular matrix $JU$. At first it gives us that the result holds from
the second column onward. Because of symmetry it then also holds from
the second row down. And now for the $(1,1)$ component\dots

\Level 1 {Orthogonalisation of the Krylov sequence}

Iterative methods based on Arnoldi orthogonalisation perform
a substantial number of orthogonalisation steps in each iteration.
The Blas1 operations associated with this are quite inefficient
on most computers. We will discuss two strategies of improving
the performance of Arnoldi-based methods.

Apart from the Gram-Schmidt-based\index{Gram-Schmidt} strategies
below, there is also the possibility of using Householder
reflections~\cite{Walker:householder-gmres}.

\Level 2 {Gram-Schmidt and modified Gram-Schmidt in Arnoldi methods}
\label{sec:gram-schmidt}\index{Gram-Schmidt|(}

The orthogonalisation in each iteration of GMRES or OrthoDir
can be desribed  as in figure~\ref{fig:mod-gs},
\begin{rfigure}
\begin{tabbing}
$w\leftarrow Av_i$\\
for \=$j<i$\\
\>let $\delta_j=(w,v_j)$\\
\>update $w\leftarrow w-\delta_jv_j$.\\
$v_{i+1} = w/\|w\|$
\end{tabbing}
\caption{Modified Gram-Schmidt orthogonalisation}
\label{fig:mod-gs}
\end{rfigure}
which is a Modified Gram-Schmidt process.
The inner products and vector updates performed here are all
Blas1 operation, hence relatively inefficient on most computers.
Also, their interdependence implies a large number of synchronisation
points in a parallel context

An easy way to increase the efficiency of the orthogonalisation
process is to use  a classical Gram-Schmidt process
as in figure~\ref{fig:class-gs}.
\begin{rfigure}
\begin{tabbing}
$w\leftarrow Av_i$\\
for \=$j<i$\\
\>let $\delta_j=(w,v_j)$\\
for \=$j<i$\\
\>update $w\leftarrow w-\delta_jv_j$.\\
$v_{i+1} = w/\|w\|$
\end{tabbing}
\caption{Classical Gram-Schmidt orthogonalisation}
\label{fig:class-gs}
\end{rfigure}
Performing the inner products and updates in a block manner
raises the execution efficiency to Blas2 level, and all inner products
can be combined in a single global reduction operation.
Unfortunately, this method is less numerically stable;
see~\cite{Bjorck:mod-gs,BjorckPaige:mod-gs,DanielGKS:stable-gs}.

\begin{question}
How do the Hessenberg matrices relate for the two variants?
\end{question}

A solution, proposed in~\cite{DanielGKS:stable-gs}, is to
re-orthogonalise in case of proven large numerical errors;
see figure~\ref{fig:twice-gs}.
\begin{rfigure}
\begin{tabbing}
$w\leftarrow Av_i$\\
{\tt label:ortho}\\
for \=$j<i$\\
\>let $\delta_j=(w,v_j)$\\
for \=$j<i$\\
\>update $w\leftarrow w-\delta_jv_j$.\\
if $\|w\|<\eta \|(\ldots \delta_j\ldots)\|$ \\
\>repeat once from label {\tt ortho}.\\
$v_{i+1} = w/\|w\|$
\end{tabbing}
\caption{Classical Gram-Schmidt orthogonalisation}
\label{fig:twice-gs}
\end{rfigure}
The tolerance parameter~$\eta$ to base the re-orthogonalization
decision on can be fairly modest, e.g.,~$1/\sqrt 2$. The $\eta$~test
tests for the occurence of cancellation.

Since the coefficients arising here have to be
stored in a Hessenberg matrix, we derive the effect of this double
orthogonalisation. Let $v_i,i=1\ldots n$ be orthogonal, and let
$u=Av_n$. Then:
\begin{eqnarray*}
u'&=&u-\sum_i^n \alpha_iv_i,\quad \alpha_i={v_i^tu\over v_i^tv_i}v_i\\
u''&=&u'-\sum_i^n \beta_iv_i,\quad \beta_i={v_i^tu'\over
v_i^tv_i}v_i\\
&=&u-\sum_i^n(\alpha_i+\beta_i)v_i
\end{eqnarray*}
After scaling $v_{n+1}=u''/\gamma_n$ (we leave open the possibility
that the $v_i$~vectors are not normalised), we get
\[ Av_n = v_{n+1}\gamma_n+\sum_i^n(\alpha_i+\beta_i)v_i. \]

An approach in between Classical and Modified Gram-Schmidt
is the Block Gram-Schmidt 
algorithm~\cite{JalbyPhilippe:block-gs}; figure~\ref{fig:block-gs}.
\begin{rfigure}
\begin{tabbing}
$W\leftarrow AV_i$\\
for \=$j<i$\\
\>$\Delta_j=V_i^tW$\\
\>$W\leftarrow W-V_i\Delta$
\end{tabbing}
\caption{Block Gram-Schmidt orthogonalisation}
\label{fig:block-gs}
\end{rfigure}
Here all vectors are block vectors.
If $V_i$ is a true block vector, this algorithm uses Blas3 operations,
otherwise the inner products and updates are still of Blas2 level.

Vanderstraeten~\cite{Vanderstraeten:block-gs} proposes to
let the size of the $V_i$ blocks be dynamically determined
by their condition, which can be efficiently estimated
from the $R$~matrices of their $QR$ 
factorisations;~\cite{Bi:incremental-condition}.
The algorithm is given in figure~\ref{fig:dyn-block-gs}.
\begin{rfigure}
\begin{tabbing}
$w\leftarrow Av_i$\\
for \=$j<p$\\
\>orthogonalise $w$ again block $V_j$\\
$V_p\leftarrow[V_p,w]$\\
if \=the condition of $V_p$ is too large,\\
\>start building a new $V$ block,\\
\>otherwise, continue adding to this block
\end{tabbing}
\caption{Dynamic Block Gram-Schmidt orthogonalisation}
\label{fig:dyn-block-gs}
\end{rfigure}
\index{Gram-Schmidt|)}

\Level 2 {After-the-fact orthogonalisation}
\label{sec:sstep-cg}

All algorithms discussed in this monograph alternate the matrix-vector
product and the orthogonalisation of the result to the previously
generated vectors.
It is possible to generate an initial part $K_n$ of the
Krylov sequence and orthogonalise this as a whole, using
a QR decomposition. Writing this decomposition as \[ K_n=V_nU_n\inv \]
we thus derive an orthonormal~$V_n$, which can be scaled
to a residual sequence (section~\ref{sec:scaled}),
or used in a GMRES method (section~\ref{sec:gmres}).
The Hessenberg matrix can be constructed as $H=U\inv JU$;
see \eqref{eq:hess-from-u}.

This procedure is not recommended for any large values of~$n$,
since round-off will steer the $k_i$ vectors towards the largest
eigenvector.
However, for moderate values there may be a performance advantage to
this approach:
\begin{itemize}
\item The construction of~$U_n$ is now done as a whole, and not
by updating, column by column. Thus, a Blas~3 kernel can be used
for the QR decomposition. 
Experience shows that, because of register blocking,
already for $n=4$ Blas3 routines may outperform Blas2 ones.
\item In parallel it is possible to pipeline the matrix-vector products.
For this, the processors exchange enough boundary information
that several products can be performed by purely local operations.
This procedure does involve some redundant operations.
See~\cite{OpJo:improved-sssor} for details.
\item There are other considerations of cache reuse that
make this algorithm attractive; see~\cite{ChGe:sstep}.
\end{itemize}

Instead of deriving the Krylov sequence itself, one could use an inner
product-free method such as Chebyshev semi-iteration prior to the
block orthogonalisation. This was proposed by de
Sturler~\cite{Sturler:restructured}.

\Level 1 {Reduction to Hessenberg form by Householder reflections}
\label{sec:householder}
\FurtherReading

In this monograph we extensively studied the reduction of a matrix
to Hessenberg form by forming the Lanczos basis.
Such a reduction can also be effected by Housholder reflections.
The two reductions are related, and in this section we study precisely how.

The matlab code for various algorithms
is in section~\ref{sec:matlab-householder}.

\Level 2 {Basic ideas of Householder reflectors}

\input householder.tex

\Level 2 {Relation to Lanczos vectors}

We now study the relation between the above reduction to Hessenberg
form by Householder reduction, and the reduction by forming
an Arnoldi basis as studied in most of this monograph.

First of all,
we note from \eqref{eq:Hess-Q-form}
that in the reduction $AV=VH$ by Householder reflectors $v_1=e_1$,
so this reduction is equivalent to applying an Arnoldi process 
with the first unit vector as starting point.

Conversely, let $u$ and $v$ be normalised vectors and $Q$~a~Householder
reflector with~$v=Qu$, and define $\hat A=\nobreak QAQ$, 
which is a similarity transformation, then
\[ Av=AQu=Q\hat Au,\quad\ldots\quad, A^nv=Q\hat A^n u \]
and
\[ Au=Q\hat AQu=Q\hat Av,\quad\ldots\quad, A^nu=Q\hat A^nv \]
In other words, the Krylov space $K=\KmethodAv Av$ can be found
by applying a reflector to $\hat K=\KmethodAv {\hat A}u$, and vice versa.

Applying this converse statement to the Lanczos starting vector~$q_1$,
we note that
\[ Q^tA^kq_1=(Q^tAQ)^kQ^tq_1=H^ke_1, \]
so \[ Q^t\KmethodAv{A}{q_1}=\KmethodAv H{e_1}. \]

\begin{comment}
This can be carried over to residuals:
 \[ \begin{cases}K=E\hat K\cr R=KU\end{cases}\Rightarrow
    \hbox{$R=E\hat R$ where $\hat R$ is}. \]
\end{comment}

Another relation statement. Suppose $AQ=QH$ is any reduction
to Hessenberg form by an orthonormal~$Q$, for instance as a Lanczos basis.
The $Q$~matrix obtained from the reduction by Householder reflectors
has the special form of \eqref{eq:Hess-Q-form}, so let $Q_e$~be a reflector
that transforms~$q_1$ into~$e_1$.
Define $\hat Q=Q_eQ$, which is of the desired form,
and which is orthonormal again.
Then \[ AQ=QS\quad\Rightarrow\quad Q_eAQ_e\hat Q=\hat QH. \]
It would have been more interesting if
we could have transformed the Lanczos basis reduction of~$A$ into
a Householder reflector basis of~$A$. However, this would imply
that after solving one linear system we could solve any other system
by a simple transformation of the Lanczos basis.
It is unlikely that this can be done.

\Level 2 {Retrieving Householder vectors from the Lanczos basis}

We will now consider the problem of reconstructing the Householder
vectors from the Lanczos basis.

In our first approach we assume that the whole Lanczos basis
is available.

Let~$Q$~be an orthonormal matrix of the form~(\ref{eq:Hess-Q-form}).
From the observation that the second column is~$e_2-u_{11}u_1$,
where $u_1$~is the first Householder vector, we can easily
compute~$u_1$.
Since $Q=Q_1Q_2\cdots$ and $Q_i=Q_i\inv=Q_i^t$, we get $Q_2Q_3\cdots=Q_1Q$.
Now we can iterate this process to find the second Householder
vector,~$u_2$, and eliminate~$Q_2$ from the product.

It is also possible to reconstruct the Householder vectors incrementally.

Let \[ Q=[q_1,\ldots,q_n] \] be the Lanczos basis.
For~$Q$ we also have $Q=\prod Q_i$ where $Q_i=I-u_iu_i^t$.
Summarise the $u_i$~vectors in \[ U=[u_1,\ldots,u_n] \]
which is strictly lower triangular.
The $Q_i$ matrices are of the form
\[ Q_i=\begin{pmatrix}1\cr &\ddots\cr &&1\cr
     &&&*&\cdots&*\cr &&&\vdots&&\vdots\cr &&&*&\cdots&*\cr\end{pmatrix} \]

The first Householder vector, $u_1$, can be found from the second
column of~$Q_1$, which stays intact in the final matrix~$Q$.

For the second Householder vector we consider the third column of~$Q$,
which is also the third column of~$Q_1Q_2$. It is formed from the
third column of~$Q_2$, which is $-u_2u_{22}+e_3$, and the 
rows of~$Q_1$, which are
\[ (Q_1)_{k,*}=-u_{k1}u_1^t+e_k^t. \]
As a result we get
\begin{eqnarray*} q_3&=&(-u_{*1}u_1^t+e*^t)(-u_2u_{22}+e_3)\\
    &=&u_{*1}u_1^tu_2u_{22}-e_*^tu_2u_{22}-u_{*1}u_1^te_3+e_*^te_3\\
    &=&u_{*1}(u_1^tu_2u_{22}-u_{13})-u_{*2}u_{22}+\delta_{*3}.
\end{eqnarray*}
In general we find $q_{n+1}=\sum_k^nu_{*k}c_{kn}+e_{n+1}$,
which obviously can be inverted to
\[ q_{n+1}=\sum_k^nq_{*k}c_{kn}+u_n\alpha_n+e_{n+1}. \]
This looks suspiciously like the Lanczos update formula.

\SetBaseLevel{1}
\input arpack
\SetBaseLevel{0}

\Level 0 {Non orthogonality-based methods}
\FurtherReading

There are various methods that do not base their workings on some form
or orthogonality. In this section we discuss
\begin{itemize}
\item Stationary iteration, which reduces the residual in norm by a
  fixed factor in each iteration.
\item Steepest descent, which in each iteration minimizes the residual
  along the search direction, which is taken as the gradient.
\item Chebyshev semi-iteration, which achieves the maximal reduction
  of the residual in each iteration, based on an estimate of the
  spectrum.
\end{itemize}

\SetBaseLevel 1
\input stationary
\input other-stationary.tex
\SetBaseLevel 0

\Level 1 {Chebyshev semi-iteration}

If estimates of the spectrum of the coefficient matrix are known, that
is $0<a<b$ are known such that the spectrum is contained in~$[a,b]$,
it is possible to construct the iteration polynomials such that the
residual is minimized in each iteration. 

The theory of this minimization process can be found in
section~\ref{sec:cheby}.

The advantage of the Chebyshev method is that it does not use inner
products during the iteration process.

Extension of the Chebyshev method to nonsymmetric systems was
discussed by Manteuffel~\cite{Ma:chebyshev}. A~procedure for
adaptively estimating the spectrum parameters was discussed by
Manteuffel~\cite{Ma:adaptive}.

\Level 0 {Polynomials}

In most of this monograph, we do not talk much about polynomials
since most of the time any statement regarding them can be expressed
in terms of matrices and vectors. This section will gather those results
that are better expressed directly in terms of polynomials, or that
translate between the polynomial results and the corresponding
linear algebra expressions.

This is also where we discuss the convergence speed of conjugacy-based
methods; the basic theory of Chebyshev polynomials is in the last
section of this chapter (section~\ref{sec:cheby}).

\Level 1 {Recap of earlier results}

Lemma~\ref{Poly-4}:\\
Let $X$ be a sequence, and let the matrix~$A$ and the vector~$f$ be
given. Define $k_1=Ax_1-f$, and let $K=\Kmethodv{k_1}$.
Then the following statements are equivalent.
\begin{enumerate}
\item $X=\Pmethod$
\item
 There are polynomials $\{\pi_i\}_{i\geq1}$ such that
        \[ X(J-I)=(\pi_1(A)k_1,\pi_2(A)k_1,\ldots). \]
\item
 There are polynomials $\{\pi_i\}_{i\geq1}$ such that
        \[ X(J-E_1)=(\pi_1(A)k_1,\pi_2(A)k_1,\ldots). \]
\item
 There is an upper triangular matrix $U$ such that
        \[ X(J-I)=KU. \]
\item
 There is an upper triangular matrix $U$ such that
        \[ X(J-E_1)=KU. \]
\end{enumerate}

Lemma~\ref{R-Krylov-combo}:\\
Let a matrix $A$ a~vector~$f$ and a sequence~$X$ be given. Then
\begin{eqnarray*}
    &&\exists_{\{\pi_i\in\Pn\}}\colon X=\Pmethod\\
    &\Leftrightarrow&
    \exists_{\{\phi_i\in\Pnn\}}\colon\Rmethod=[\phi_i(A)r_1]
 \end{eqnarray*}
and the $\phi_i$ and $\pi_i$ polynomials are related by
\[ \phi_{i+1}(x)=1+x\pi_i(x). \]

Theorem \ref{main-theorem}, item~\ref{R=pi(AMinv)}:\\
There are a nonsingular matrix~$M$ and polynomials~$\Pset$
such that
        \[ r_i=\pi_i(AM\inv )r_1,\qquad
                {\rm deg}(\pi_i)=i-1,\qquad
                \pi_i(0)=1. \]

\Level 1 {Orthogonal polynomials}

\input ortho_poly

This lemma for orthogonal polynomials
has an exact counterpart for residual sequences.
\begin{llemma}{Residual sequences generated by symmetric matrix
satisfy three-term recurrence}
Let $r_i$ be a sequence generated from
\[ r_{n+1}=\alpha^{(n)}Ar_n-\sum^n_i\beta^{(n)}_ir_i, \]
where $A$ is symmetric. Then the vectors satisfy a three-term
recurrence
\[r_{n+1}=(\alpha^{(n)}A-\beta^{(n)}_n)r_n-\beta^{(n)}_{n-1}r_{n-1}.\]
\end{llemma}
\begin{proof}
Analogous to the above proof we determine the~$\beta$ coefficients
as \[ \beta_k/\alpha = r_k^tAr_n/\|r_k\|^2. \]
We then observe that if $A$~is symmetric and~$k<n-1$,
\[ A^tr_k=Ar_k\in\Span{r_1,\ldots,r_{k+1}}, \]
so $r_k^tAr_n=0$ for~$k+1<n$.
\end{proof}

This proof is essentially that of lemma~\ref{lemma:cg-tri}: the full
recurrence is expressed as~$AR=RH$, from which by orthogonality of the
$r_i$~vectors
\[ h_{kn}={r_k^tAr_n\over \|r_k\|^2}. \]
If $A$ is symmetric, for~$k<n-1$
\[ h_{kn}={\|r_n\|^2\over \|r_k\|^2}h_{nk}=0.\]

\begin{question}
Where does symmetry come into the polynomials proof?
\end{question}

\Level 1 {CG polynomials}

The update relations for CG are
\begin{eqnarray*}
M\inv r_{i+1}&=&M\inv r_i- M\inv Ap_i\alpha_i\\
p_i&=&M\inv r_i-p_{i-1}\beta_i
\end{eqnarray*}
Since all $\alpha_i>0$,
\[M\inv r_{i+1}=\rho_{i+1}(M\inv A)M\inv r_1\]
where \[ \rho_{i+1}(x)=(-1)^i|c^{(i+1)}_{i+1}+\cdots \]
That is, the coefficients of the highest exponent are of alternating
sign. Contrast this with the Arnoldi procedure, where 
\[ z_{i+1}=c_{i+1}(Az_i-c_iz_i-\cdots \]

\Level 1 {Polynomials of search directions}

We gather some results on the polynomials associated with residuals
and search directions.

\begin{llemma}{Residual polynomials satisfy recurrence with 
coefficients in $H$}
\label{lemma:polynomial:recurrence}
Let $R$ be a residual sequence satisfying $AR=RH$,
and let polynomials~$\rho_i$ be such that $r_i=\rho_i(A)r_1$. Then:
\[h_{n+1n}\rho_{n+1}(t)=
  (t-h_{nn})\rho_n(t)-\sum_{k=1}^{n-1}h_{kn}\rho_k(t) \]
\end{llemma}

\begin{comment}
\begin{proof}
Let $U$ be the upper triangular matrix containing the polynomial coefficients.
From the relation $UH=JU$ relating the upper
Hessenberg matrix and the upper triangular matrix corresponding to
certain spans of Krylov spaces we see that $U$ can be solved one
column at a time. Suppose the $n$-th column $u_{*n}$ has been solved,
then 
\begin{equation}
        u_{in+1}h_{n+1n}=u_{i-1n}-\sum_{k=1}^n u_{ik}h_{kn}
        \qquad\hbox{for $i=1,\ldots,n+1$.}
        \label{eq:HtoU}\end{equation}
This relation implies the recurrence for the polynomials~$\rho_n$.
\end{proof}
\end{comment}

We present the following results 
in the context of Lanczos orthogonalization.
For Arnoldi orthogonalization, the left sequence can simply be ignored.

\begin{llemma}{Polynomials for residuals and search directions}
\label{lemma:PR-polynomials}
Let residuals~$r_i$ and search directions~$p_i$,
and corresponding left sequences $\tilde r_i$ and~$\tilde p_i$,
be defined by coupled two-term recurrences
(theorem~\ref{main-theorem}, item~\ref{APD=R(I-J)}):
\[ r_{i+1}=r_i-Ap_i\ar_i,\qquad p_i=M\inv r_i-p_{i-1}\br_i \]
and
\[ \tilde r_{i+1}=\tilde r_i-A^t\tilde p_i\al_i,\qquad
    \tilde p_i=M\invt \tilde r_i-\tilde p_{i-1}\bl_i .\]
There are polynomials $\rho_i$, $\tilde \rho_i$, $\pi_i$, and~$\tilde\pi_i$
of degree~$i-\nobreak1$, such that
\begin{equation} \left\{\begin{matrix}
    r_i = \rho_i(AM\inv)r_1,\qquad p_i = M\inv\pi_i(AM\inv)r_1\cr
    \tilde r_i = \tilde \rho_i(A^tM\invt)\tilde r_1,\qquad 
        \tilde p_i = M\invt\pi_i(A^tM\invt)\tilde r_1\cr
    \end{matrix}\right. \label{eq:rp-polynomials}\end{equation}
The polynomials satisfy recurrences
\begin{equation}
    \begin{cases}\rho_{i+1}(x)=\rho_i(x)-x\pi_i(x)\ar_i\cr
           \tilde\rho_{i+1}(x)=\tilde\rho_i(x)-x\tilde\pi_i(x)\al_i\cr
           \pi_{i+1}(x)=\rho_{i+1}(x)-\pi_i(x)\br_i\cr
           \tilde\pi_{i+1}(x)=\tilde\rho_{i+1}(x)-\tilde\pi_i(x)\bl_i\cr\end{cases}
    \label{eq:rp-poly-recurrence}\end{equation}
\end{llemma}
\begin{proof}
This follows immediately from
\[ r_{i+1} = \rho_i(AM\inv)r_1-AM\inv\pi_i(AM\inv)\ar_ir_1 \]
and
\[ p_{i+1} = M\inv\rho_{i+1}(AM\inv)r_1
        -M\inv\pi_{i-1}(AM\inv)\br_ir_1, \]
(and similar for the left sequence)
from which
\[
 \left\{\vcenter{\halign{$#$\hfil\cr
    \rho_{i+1}(\AM)=\rho_i(\AM)+\AM\pi_i(\AM)\ar_i;\cr
    \tilde\rho_{i+1}(\AMt)=\rho_i(\AMt)+\AMt\pi_i(\AMt)\al_i\cr
%    \Rightarrow
%    \tilde\rho_{i+1}(\MA)=\tilde\rho_i(\MA)+\MA\tilde\pi_i(\MA)\al_i,\cr
    M\inv\pi_{i+1}(\AM)=M\inv\rho_{i+1}(\AM)+M\inv\pi_i(\AM)\br_i\cr
    M\invt\tilde\pi_{i+1}(\AMt)=M\invt\tilde\rho_{i+1}(\AMt)
                                      +M\invt\tilde\pi_i(\AMt)\bl_i\cr
%    \Rightarrow
%    \tilde\pi_{i+1}(\MA)=\tilde\rho_{i+1}(\MA)+\tilde\pi_i(\MA)\bl_i\cr
    }}\right. 
\]
\end{proof}

\Level 1 {Constructing a sequence from a polynomial}

Consider the scheme
\begin{eqnarray*}
r_i&=&Ax_i-b\\
x_{i+1}&=&x_i-\alpha_ir_i
\end{eqnarray*}
The second line gives $r_{i+1}=(I-\alpha_iA)r_i$. Continuing,
\[ r_{i+1}=P_i(A)r_1,\qquad P_i(x)=\Pi(1-\alpha_ix). \]
This means that if we have a suitable polynomial and its
zeros~$-\alpha_i\inv$, we can apply it to construct a polynomial
sequence. This is the basic idea behind Richardson iteration;
section~\ref{sec:richardson}.

This could be a cool idea if more than one system with the same
coefficient matrix is to be solved. The coefficients~$\alpha_i$ can be
computed from the first system at some expenditure (for instance with
a GMRES method), after which the other sequences can be solved at less
expense.

\Level 1 {Chebyshev polynomials}
\label{sec:cheby}

\input chebyshev

\Level 0 {Convergence theory}
\label{sec:convergence}

Combining lemma~\ref{R-Krylov-combo}, which effectively says that all
possible choices for the $i$-th residual are in the same subspace, and
corrolary~\ref{minimum:A-1} which states that the orthogonal residual
in minimal in that subspace, we get the statement that
\[ \|r_n\|_{A\inv}=\min_{\pi_n\in\Pn} \|\pi_n(AM\inv)r_1\|. \]

For any given polynomial, we can bound the residual size by a bound on
the spectrum.
\begin{llemma}{Convergence is dominated by estimate of polynomial on
    spectrum}
Let $S$ be a set that contains the spectrum of~$AM\inv$, 
let $\{\pi_i\}$ be the polynomials of some polynomial iterative
method, and let $M_i$ be such that
\[ \lambda\in S\colon|\pi_i(\lambda)|\leq M_i, \]
then
\[ \|r_n\| \leq M\|r_1\| \]
\end{llemma}
\begin{proof} This follows from $r_n=\pi_n(AM\inv)r_1$. \end{proof}

This leads to the basic theorem of CG convergence.
\begin{ttheorem}{Convergence of CG determined by best polynomial bound
    on spectrum}
\label{th:spectrum-bound}
Let $R$ be the residuals of the CG method, let $S$ contain the
spectrum of~$AM\inv$, and define a polynomial-specific bound~$M_\pi$
such that \[ \forall_{\lambda\in S}\colon|\pi(\lambda)|\leq M_{\pi} \]
then
\[ \|r_n\|_{A\inv}=\min_{\pi_n}M_{\pi_n}\|r_1\|_{A\inv}. \]
\end{ttheorem}
This means that we can bound the residual size by bounding various
polynomials on (a~set containing) the spectrum.

\Level 1 {Speed of convergence of Conjugate Gradients}

For the symmetric conjugate gradients method a convergence bound is
easy to derive. From theorems \ref{th:spectrum-bound}
and~\ref{th:cheby-optimal} it follows that we need to bound
\[ T_n\left({b+a-2x\over b-a}\right)\mid/\left({b+a\over b-a}\right)
\]
with $a=\lambda_{\min}$, $b=\lambda_{\max}$. The denominator is bound
by~1, so we can ask what the smallest~$n$ is for which
$T_n(b+a/b-a)>1/\epsilon$. Let $\kappa=\lambda_{\max}/\lambda_{\min}$
be the spectral condition number, then
\[ T_n\left({b+a\over b-a}\right)=T_n\left({\kappa+1\over\kappa-1}\right)  
  ={1\over2}\left[\left({\sqrt\kappa+1\over\sqrt\kappa-1}\right)^n +
                  \left({\sqrt\kappa-1\over\sqrt\kappa+1}\right)^n\right]
  \geq \left({\sqrt\kappa+1\over\sqrt\kappa-1}\right)^n \]
Use the fact that
\[ \ln[(\sqrt\kappa+1)/(\sqrt\kappa-1)]>2/\sqrt\kappa \]
to see that
\[ n>(1/2)\sqrt\kappa \ln(2/\epsilon) \Rightarrow
    T_n(b+a/b-a)>1/\epsilon . \]

\Level 1 {Error reduction with isolated eigenvalues}

Most convergence statements estimate speed of convergence in terms of
the size of some eigenvalue cluster. Outlying eigenvalues are
generally considered to delay convergence. Results to this effect were
proved by Campbell et al~\cite{Campbell:gmres-minimalp}:
\begin{itemize}
\item If there is a single eigenvalue cluster with radius~$\rho$, and
  $d$ is the sum of the degrees of the minimal polynomials of the
  outliers, then
  \[ \|r_{d+k}\|\leq C(\rho)\rho^k\|r_0\|. \]
\item In the case of $P$ cluster, a similar estimate
  \[ \|r_{d+kP}\|\leq C(\rho)(\sigma^{P-1}\rho)^k\|r_0\| \]
  holds, where $\sigma$~is the maximal distance between clusters.
\end{itemize}

\Level 1 {Stopping tests}

Many people use the test $\|r_i\|\leq\epsilon\|r_1\|$. This is not a
good idea~\cite{Ba:templates,Kaas:termination,Ashby:termination}.

\Level 2 {Backward error analysis}

Ultimately, in error analysis we want to bound the distance between
the computed and the true solution. For instance, if 
\[ {\|x-\hat x\|\over \|x\|}<10^{-n} \]
then the computed solution has $n$ correct digits.
Of course, this \index{forward error}forward error can not be
computed.

However, we can bound it if we take a small detour.

Backward error analysis considers that the computed solution is the
exact solution of a perturbed system 
\begin{equation}
    (A+\delta A)x=b+\delta b.
    \label{eq:backwardsystem}\end{equation}
Clearly, we want the relative perturbations $\|\delta A\|/\|A\|$,
$\|\delta b\|/\|b\|$ to be small, ideally machine precision, because
that would correspond to any normal contamination of the
representation of a continuous problem as a matrix equation.

The normwise \index{backward error}backward error is then defined as
the minimum~$\epsilon$ such that 
\begin{equation}
     \max\{{\|\delta A\|\over \|A\|}, {\|\delta b\|\over\|b\|}
     \leq\epsilon
    \label{eq:backwarderror}\end{equation}
where $\delta A$, $\delta b$ satisfy \eqref{eq:backwardsystem}.

The following theorem~\cite{RigalGaches:compatibility} has a more
practical form of the backward error.
\begin{ttheorem}{Backward error can be computed from residual and
    solution}
The normwise backward error defined in \eqref{eq:backwarderror}
satisfies
\begin{equation}
    \epsilon={\|r\| \over \|A\|\|x\|+\|b\|}
    \label{eq:backwardcompute}\end{equation}
\end{ttheorem}

Using the backward error, we can bound the forward error. Consider the
perturbed solution of a perturbed system:
\[ (A+\delta A)(x+\delta x)=b+\delta b. \]
Our goal is to bound $\|\delta x\|/\|x\|$, assuming that we satisfy a
backward error bound as in \eqref{eq:backwarderror}.

Using $Ax=b$ we easily get
\[ \delta x=A\inv (\delta b-\delta Ax-\delta A\delta x). \]
Taking norms gives
\[ \|\delta x\|(1-\|A\inv\|\|\delta A\|)
    \leq \|A\inv\|(\|\delta b\|+\|\delta A\|\|x\|)
\]
which, using $\|b\|\leq\|A\|\|x\|$ becomes
\[ \|\delta x\|(1-\|A\inv\|\|\delta A\|)
    \leq \|A\inv\| ( {\|\delta b\|\over\|b\|}\|A\|\|x\|
                     + \|\delta A\|\|x\| )
\]
Introducing $\kappa=\|A\|\|A\inv\|$, this becomes
\begin{equation}
    {\|\delta x\|\over \|x\|} \leq
    {\kappa\over 1-\kappa{\|\delta A\|\over\|A\|}}
        \left(
          {\|\delta b\|\over \|b\|}+{\|\delta A\|\over \|A\|}
        \right)
    \leq {2\kappa\epsilon\over 1-\kappa\epsilon}
    \label{eq:forwardestimate}\end{equation}
where we assume the backward error bound to hold, and we assume that
$\kappa\epsilon<1$.

The theory of Oettli and Prager~\cite{OettliPrager:compatibility} that
for with $r=Ax-b$ and
\[ \epsilon=\max_i{|r_i|\over (E|x|+b)_i} \]
we have that
\[ (A+\delta A)x=b+\delta b,\qquad 
    \hbox{$|\delta A|\leq\epsilon E$ and $|\delta b|\leq\epsilon b$}
\]
This means that by minimizing~$\epsilon$ we obtain the solution to a
nearby system.

Arioli et al.~\cite{ArDuRu:stopping} proposed a variant on this:
\[ \epsilon={\|r\|\over \|A\|\|x\|+\|b\|} \]
(Check this!)
\Level 2 {Eigenvalue based estimates}

Ultimately, we want
\[ \|x-x_i\|/\|x\|\leq\epsilon. \]
For this it is enough to have
\[ \|x-x_i\|/\|x_i\|\leq\epsilon/(1+\epsilon), \]
and, since $x-x_i=A\inv(b-Ax_i)=-A\inv r_i$, we get for symmetric
matrices the stopping criterion
\[ \|r_i\|\leq\mu_1\|x_i\|\epsilon/(1+\epsilon), \]
where $\mu_1$ is the smallest
eigenvalue. Kaasschieter~\cite{Kaas:termination} proposes an efficient
way of computing this smallest eigenvalue from the CG polynomials by
using bisection with the previous estimate as starting guess.

\Level 1 {Breakdown conditions}
\label{sec:breakdown}

In this section we investigate the breakdown of iterative methods,
that is, conditions under which an inner product appearing
in the nominator of a scalar expression is zero.

\Level 2 {General theory}

In section~\ref{sec:cg:inprod} we considered conjugacy-based methods
under a general inner product that makes the residuals satisfy
\[ \hbox{$R^tN\inv R$ upper triangular.} \]
We did not consider whether, given an inner product~$N\inv$,
this construction is well-defined a priori.
For this, recall that for polynomial iterative methods,
\[ R=KU, \qquad\hbox{with $U\in\Un$ and $K=\Kmethod{AM\inv,r_1}$}. \]
Constructing $R$ is then equivalent to constructing~$U$.
In the next lemma we will show that $U$ can be constructed inductively,
as long as $K$~is independent under~$N\inv$.

\begin{llemma}{Arnoldi orthogonal $R$ constructable from independent $K$}
Let $K=\KmethodAv{AM\inv}{r_1}$ and $N$~a symmetric (general)
nonsingular matrix
It is possible to
construct~$U_{n+1}$ such that $R_{n+1}^tN\inv R_{n+1}$
is diagonal (upper triangular),
where the sequence~$R$ is
constructed from~$R=KU$, iff the first $n+1$ columns of~$K$ are independent.
\end{llemma}
\begin{proof} Let $R_n$, $U_n$ be the initial $n$~columns of~$R$,~$U$.
Suppose inductively that $R_n^tN\inv R_n$ is diagonal.
In order to let $r_{n+1}$ be $N\inv$-orthogonal to~$R_n$, we need to solve
the $n+1$-st column,~$u_{n+1}$, of~$U$ from the overdetermined system
\[ U_n^t\bar Nu_{n+1}=\left.%\{
        \begin{pmatrix}0\cr \vdots\cr 0\cr\end{pmatrix}\right\}n \]
where $\bar N$ is the $n\times n+1$ primary subblock of~$K_n^tN\invt K_{n+1}$.
This determines~$u_{n+1}$ up to scaling.
We scale it so that~$u_{1,n+1}=1$, so the system to be solved now
becomes
        \[ \bar N\begin{pmatrix}0\cr u_{2n+1}\cr \vdots\cr u_{n+1n+1}\end{pmatrix}
                = \bar N\begin{pmatrix}1\cr 0\cr \vdots\cr 0\end{pmatrix}. \]
Now it follows that
$R_n^tNr_{n+1}=0$, and for symmetric~$N$, $R_{n+1}^tNR_{n+1}$ is diagonal.

The reverse assertion is trivial: if $K$~has dependent columns,
clearly no orthogonal sequence can be constructed from it.
\end{proof}

The requirement for the Krylov sequence to be independent shows
up in a different guise in section~\ref{sec:scaling-theory}.

For the case of Lanczos orthogonalisation, we have an added condition
on the existence of the residual sequence.
\begin{llemma}{Lanczos orthogonal $R$ exists if $K$ independent
and no accidental $s_i^tr_i=0$}
Show this
\end{llemma}

The Lanczos breakdown condition can be prevented with a look-ahead
strategy.
\begin{comment}
\begin{theorem}
Let a sequence~$X$, a vector~$f$, and a matrix~$A$ be
given. Define residuals by $r_i=Ax_i-f$ and search directions by
$p_i=x_{i+1}-x_i$, and let transformations~ $M$,~$N$ be such that
$R=M\inv PN$, then $R$~consists of combinations of a Krylov space
of~$AM$ if and only if $N$~is upper triangular.
\end{theorem}
\begin{proof} From $X(J-I)=P$ it follows that
        \[ R(J-I)=AP=AMRN\inv \]
so      \[ R(J-I)N=AMR. \]
If $R$ is constructed from a Krylov sequence of~$AM$, $(J-I)N$~has to
be upper of Hessenberg form, from which it follows that $N$~is upper
triangular.
\end{proof}

\end{comment}

\Level 2 {Iterative forms of the breakdown condition}

In remark~\ref{H-red} we observed that breakdown
is associated with the Hessenberg matrix becoming reducible,
that is, with $h_{n+1n}=0$ for some~$n$.
In the search-direction formulation of Krylov methods,
this corresponds to
\[ h_{n+1n}=-d_n\inv=-{p_n^tAp_n\over r_n^tM\inv r_n}. \]


\Level 0 {Eigenvalues}

Arnoldi methods perform a reduction $V^tAV=H$ with $H$~of upper
Hessenberg form (in the symmetric case $H=T$ of tridiagonal form),
and $V^tV=I$, so this
is a similarity transformation, and the spectrum of~$H$ is that
of~$A$.

The Lanczos method uses different sequences $V$ and $W$ with $W^tAV=T$
of tridiagonal form and $W^tV=I$. Observing that $W^tVW^t=W^t$, when
also $VW^t=I$, we can transform an eigenequation
\[ Au = \mu u \Rightarrow W^tAVW^t u = \mu W^tu \Rightarrow
    TW^tu=\mu W^tu \]
and again we see that the spectrum of~$T$ is that of~$A$.

\Level 1 {Ritz values and Harmonic Ritz values}

In the finite case, let $V_n$~be the block containing the first~$n$
Arnoldi vectors, and $A_n=V_n^tAV_n$, then the eigenvalues and
eigenvectors of~$A_n$ are called the \index{Ritz values}Ritz values
and \index{Ritz vectors}Ritz vectors.

With $W=AV$ and using an Arnoldi method that makes $W^tW=I$, we get
\[ W^tA\inv Wy=V^tAVy=\lambda\inv V^tA^tAVy=\lambda\inv
W^tWy=\lambda\inv y. \] In other words, making $AV$ orthonormal makes
that the eigenvalues of $V_n^tAV$ approximate~$\lambda\inv$ where
$\lambda$~is an eigenvalue of~$A$. The $\lambda$ values thus obtained
are called \index{Ritz values!harmonic}harmonic Ritz
values~\cite{PagParVdV:approx,Mor:interior,Fr:cgcomplexsymmetric}.

\begin{question}
Here is another definition: for a set~$S$, $x$~is a Ritz vector with
corresponding Ritz value~$\theta$ if 
\[ x^t(Ax-\theta x)=x\qquad\forall_{x\in S}. \]
Is this the same?
\end{question}

\Level 1 {Eigenvalues of the Hessenberg matrix}

In the following, read ``$AM\inv$'' any time ``$A$'' is used.

\Level 2 {Relationship between eigenvalues of $A$ and $H$}

After $n$ steps of an Arnoldi process we have an equation
$AV_n=V_{n+1}H_{[n+1,n]}$ or 
\[ AV_n=V_nH_n+v_{n+1}[0,\ldots,h_{n+1,n}]. \]
Let $H_nx=\lambda x$ be an eigen equation and let $y=V_nx$, then
\begin{eqnarray*} Ay&=&AV_nx=V_nH_nx+v_{n+1}[0,\ldots,h_{n+1,n}]x\\
    &=&\lambda y+v_{n+1}h_{n+1,n}x_n
\end{eqnarray*}
With $r_x=Ay-\lambda y$, the size of this residual is
\[ |r_x|=\|v_{n+1}\|\,|h_{n+1,n}|\,|x_n|. \]
If we normalise $x$, so that~$x_n\leq1$, and we orthonormalise the
$v_i$~vectors, we see that $\|r_x\|\leq|h_{n+1n}|$, so we can monitor
the convergence by the subdiagonal elements of the Hessenberg matrix.

\Level 0 {Other aspects of the iterative solution process}

\Level 1 {Inventory of all upper triangular matrices}

Let a preconditioned polynomial iterative method
$X=\PPmethodAf{AM\inv}{M\inv r_1}$,
its residual sequence $R=\Rmethod$, and the Krylov sequence of the
first residual $K=\Kmethod$ be given. Let $U\in\Un$~be such that \[
R=KU, \] then lemma~\ref{lemma:xr-uptri}
\[ X(J-E)=KU_2\qquad\hbox{with $U_2=U_{[2:*,2:*]}$}. \]
Let $H$~be the upper Hessenberg matrix (with zero column sums by
lemma~\ref{H:zero:colsum})
\[ H=U\inv JU\qquad\hbox{then}\qquad AR=M\inv RH. \]
Search directions~$P$ are defined as $PU_3=M\inv R$ where
\[H=(I-J)D\inv U_3\inv.\]

\Level 1 {The symmetrisable case}
\FurtherReading

In the foregoing we have repeatedly seen how nonsymmetric systems
need different methods than symmetric ones. There is a simple
exception, namely for the case where $A$ might be nonsymmetric,
but $\hat A=G^{1/2}AG^{-1/2}$ is symmetric for a symmetric matrix~$G^{1/2}$.
In that case we draw up the equations for
an unpreconditioned conjugate gradient method with matrix~$\hat A$:
\begin{equation} \hat APD=R(I-J),\qquad P(I-U)=R,\qquad
     r_1=\hat Ax_1-G^{1/2}f
    \label{eq:cg_symmable}\end{equation}
which defines a polynomial method~$X$ for $\hat A\inv G^{1/2}f=G^{1/2}A\inv f$.
The sequence $\bar X=G^{-1/2}X$ is then a method for~$A\inv f$.

Since $R=G^{1/2}(A\hat X-f)$, we will be interested in
transformed sequences
\begin{equation} \tilde R=G^{-1/2}R,\qquad \tilde P=G^{-1/2}P.
    \label{eq:symble-trans1}\end{equation}
In terms of these we get 
\begin{equation}
     A\tilde PD=\tilde R(I-J),\qquad \tilde P(I-U)=\tilde R,
        \label{eq:cg_symmable1}\end{equation}
which we use as a basis for computation in terms of~$A$.

It remains to compute the quantities
\[ d_{ii}=r_i^tr_i/p_i^t\hat Ap_i,\qquad
    u_{ii+1}=r_{i+1}^tr_{i+1}/r_i^tr_i, \]
for which we use
\[ p_i^t\hat Ap_i=\tilde p_i^tGA\tilde p \qquad\hbox{and}\qquad
   r_i^tr_i=\tilde r_iG\tilde r.\]
In these last identities we use the symmetry of~$G^{1/2}$.

To update the solution, we note (see above) that we are really interested
in $G^{-1/2}X$, and the update for this we can easily read from
that for $\tilde R=G^{-1/2}R$.

The method now becomes:
\begin{algorithm}{Symmetrised Conjugate Gradient Method}
\label{alg:symable}
Let $x_1$ be given, and compute $r_1=Ax_1-f$.
Then iterate:
\begin{itemize}
\item Compute \[\rho_i=r_i^tGr_i.\]
\item For $i>1$, update \[ p_i=r_i+\rho_i/\rho_{i-1}p_{i-1}.\]
\item Compute \[\pi_i=p_i^tGAp_i.\]
\item Update \[ r_{i+1}=r_i-Ap_i(\rho_i/\pi_i) \qquad\hbox{and}\qquad
                x_{i+1}=x_i-p_i(\rho_i/\pi_i).\]
\end{itemize}
\end{algorithm}

We note that left and right preconditioned systems
(see section~\ref{sec:left-right-prec})
fall under the symmetrisable case: if $A$ and $M$ are symmetric,
$M\inv A$ and $AM\inv$ are not, but can be symmetrised
as described above. Algorithm~\ref{alg:symable} is then essentially
the same as the CG-variant described in section~\ref{sec:alternative-search}.

Making instead of \eqref{eq:symble-trans1} the substitution
\begin{equation} \tilde R=G^{-1/2}R,\qquad \tilde P=G^{1/2}P,
    \label{eq:symble-trans2}\end{equation}
we get from \eqref{eq:cg_symmable}
\[ A\tilde PD=\tilde R(I-J),\qquad \tilde P(I-U)=G\tilde R, \]
which are the ordinary generating equations for CG.
The coefficients are computed
from
\[ r_i^tr_i=\tilde r_i^tG\tilde r_i,\qquad
    p_i^t\hat Ap_i=\tilde p_i^tAG\tilde p_i. \]
Applying the above to the nonsymmetric system $AM\inv$, 
with both $A$ and~$M$ symmetric, and the choice $G=M\inv$,
we see that we have rederived the preconditioned conjugate gradients method.
\begin{question}
Why do we have $p^tAGp$ instead of $p^tAp$?
\end{question}


\Level 1 {Iterative solution as solving reduced systems}
\FurtherReading

One of the characterisations of polynomial iterative methods,
theorem~\ref{main-theorem}[item~4], was $X(I-J)=M\inv RU$
for some upper triangular~$U$.
By lemma~\ref{lemma:IJ-IE-equiv}, this is equivalent to
    \[ X(E_1-J)=M\inv RV \] 
for another upper triangular matrix~$V$.
From this we get $R(E_1-J)=AM\inv RV$, and if $R$~is $M\inv$-orthogonal
we find $R^tM\inv AM\inv RV=R^tM\inv R(E_1-J)$, or, in the $n$th column:
    \[ \begin{pmatrix}\|r_1\|_{M\inv}^2\cr 0\cr \vdots\cr 0\cr
                -\|r_{n+1}\|_{M\inv}^2\cr\end{pmatrix} = 
        R_{n+1}^tM\inv AM\inv R_nv_n, \]
which we restrict\footnote
{The fact that $r_{n+1}^tM\inv AM\inv R_nv_n = -\|r_{n+1}\|_{M\inv}^2$
is probably of no use.} to
    \[ \begin{pmatrix}\|r_1\|_{M\inv}^2\cr 0\cr \vdots\cr 0\cr\end{pmatrix} = 
        R_n^tM\inv AM\inv R_nv_n. \]
The generating formula for~$x_i$ now becomes
\[ x_{i+1}-x_1 = \|r_1\|^2R_n (R_n^tM\inv AM\inv R_n)\inv e_1. \]

We see that the
columns of~$V$ follow from the solution of a reduced system that is
derived from the previous by the addition of a row and column.
From lemma~\ref{lemma:xr-uptri} we know that if $x_{n+1}-x_1=M\inv
R_nv_n$, then $r_n=$.

From $X(I-J)=M\inv RU$ we get similarly
    \[ \begin{pmatrix}0\cr \vdots\cr 0\cr \|r_n\|_{M\inv}^2\cr \end{pmatrix} = 
        R_n^tM\inv AM\inv R_nv_n, \]
so
\[ x_{i+1}-x_i = \|r_n\|^2R_n (R_n^tM\inv AM\inv R_n)\inv e_n. \]

\Level 1 {$AV_n=V_nH+\hbox{\it something}$}
\label{sec:truncated-sequence}

By talking about whole sequences we have neatly avoided certain
complications, that would arise from considering only an initial part,
which are not essential to the gist of the story.
Here we flesh out one of those points, only to show that
indeed it does not matter.

We have many times given the equations $AK=KJ$ and $AR=RH$ for a
Krylov sequence and combinations $R=KU$ thereof.
In this, $H=\nobreak U\inv JU$.
To make a statement about initial parts, for instance~$K_n$,
of these sequences, we write
\begin{equation}
     AK_n=K_{n+1}J_{[n+1,n]}
    \label{eq:finiteAKKJ}\end{equation}
where $J_{[n+1,n]}$ denotes the $n+1\times\nobreak n$ initial part of~$J$.
In order to get $K_{n+1}$ in the left hand side, we have to add
to the right hand side:
\begin{equation}
     AK_{n+1}=K_{n+1}J_{n+1}+Ak_{n+1}e_{n+1}^t.
    \label{eq:AK=KJ+ak}\end{equation}
The finite form of $R=KU$ is $R_{n+1}=K_{n+1}U_{n+1}$, giving
\[ AV_{n+1}=V_{n+1}U_{n+1}\inv J_{n+1}U_{n+1}+Ak_{n+1}e_{n+1}^tu_{n+1n+1}, \]
that is,
\[ AV_{n+1}=V_{n+1}H_{n+1}+Ak_{n+1}e_{n+1}^tu_{n+1n+1}, \]
with $H_{n+1}=U_{n+1}\inv J_{n+1}U_{n+1}$.
Reducing this again to $n$ columns gives
\[ AV_n=V_{n+1}H_{[n+1,n]} \]
with
\[ H_{[n+1,n]} = U_{n+1}\inv J_{n+1}U_{[n+1,n]}
      =U_{n+1}\inv J_{[n+1,n]}U_{n} \]
which is what we found from $AV=VH$ in the first place.

Instead of as \eqref{eq:AK=KJ+ak}, \eqref{eq:finiteAKKJ} could 
also have been split as
\[ AK_n=K_nJ_n+k_{n+1}e_n^t, \]
which is often with less precision rendered as
\[ AK_n=K_nJ_n+re_n^t\qquad\hbox{with $K_n^tr=0$}. \]

\Level 1 {Singular matrices}

Let $A$ be singular, and let $N$ span the \index{nullspace}nullspace:
$AN=0$. This is also called the right nullspace; the left nullspace is
the nullspace of~$A^t$. The left and right nullspace need not be the
same: consider
\[ \begin{pmatrix}1&1\cr 0&0\end{pmatrix}\begin{pmatrix}   1\cr-1\end{pmatrix}=\bar 0;\qquad
  \begin{pmatrix}0&1\end{pmatrix}\begin{pmatrix}    1&1\cr 0&0\end{pmatrix}=\bar 0^t
\]
Let $N_\ell$ be the left nullspace $N_\ell^tA=0$, then in order for
$Ax=b$ to have a solution, we need \index{consistency}consistency:
$N_\ell^tb=0$.

Solutions to $Ax=b$ are not unique. With $N$ the nullspace $AN=0$ and
$x'=x+Ny$, we also have $Ax'=b$. Uniqueness is guaranteed by requiring
$N^tx=0$ for the solution.

Since $x_{n+1}=x_0+\sum_{k\leq n}\alpha_k(M\inv A)^kr_0$, it is enough
for uniqueness if the range of~$M\inv$ does not have components in the
nullspace of~$A$, that is,~$N^tM\inv=0$. We can do this by
post-processing~$M\inv$:
\[ \hbox{let ${M'}\inv=(I-N(N^tN)\inv N^t)M\inv$, then $N^t{M'}\inv=0$.} \]
\begin{question}
  If $M$ is symmetric, is $M'$ too, in some sense?
\end{question}

\input subspace

\Level 1 {Shift invariance}
\FurtherReading

Let $A$, $R$ and $H$ satisfy $AR=RH$, then also
\[ (A-\alpha I)R = R(H-\alpha I). \]
To assuage some people's conscience we derive the finite form
of this: let $AR_n=R_{n+1}H_{n+1n}$, then
\begin{eqnarray*}
    (A-\alpha I)R_n&=&R_{n+1n}H_{n+1n}-R_nI_n\alpha\\
                &=&R_{n+1n}H_{n+1n}-R_{n+1}I_{n+1n}\alpha\\
                &=&R_{n+1n}(H_{n+1n}-\alpha I)
\end{eqnarray*}

In principle one could use this fact to solve an indefinite system
as follows:
\begin{itemize}
\item Compute $r_1=Ax_1-b$, and let $alpha$ be such that
$A+\alpha I$ is definite.
\item Build $R$ and $H$ that satisfy $(A+\alpha I)R=RH$.
\item Construct iterates from $R=X(H-\alpha I)$.
\end{itemize}
This runs into the following objection: for an indefinite system
the dominant components in the solution are likely to be those
eigenvectors corresponding to eigenvalues close to zero. These become
inner eigenvalues in the shifted system, and iterative methods typically
find the outer eigenvalues better than the inner ones.

\Level 1 {Equivalent formulas for the scalar quantities}
        \label{sec:unsync}
\FurtherReading

In a parallel implementation, the conjugate gradient algorithm as
formulated above has two synchronization points per iteration, located
at the inner product calculations. These synchronization points cannot
be coalesced immediately: the scalar $d_{ii}$
needed to compute $r_{i+1}$ needs an inner product with~$p_i$,
and the scalar $u_{ii+1}$
necessary to compute $p_{i+1}$ requires an inner product
with~$r_{i+1}$. 
(The basic structure of the algorithm is sketched, for reference
\begin{rfigure}
\begin{itemize}
\item From $p_i$ compute $p_i^tAp_i$; 
with $r_i^tM\inv r_i$, which was saved from the previous iteration, 
compute $\alpha_i = p_i^tAp_i/r_i^tM\inv r_i$.
\item Update $r_{i+1}=r_i-\alpha_i Ap_i$.
\item Compute $r_{i+1}M\inv r_{i+1}$,
and with it $\beta_i=r_{i+1}M\inv r_{i+1}/r_iM\inv r_i$.
\item Update $p_{i+1}=Mr_{i+1}-\beta_ip_i$.
\end{itemize}
\caption{Structure of inter-dependent inner products in the CG algorithm}.
\label{fig:2inprods}
\end{rfigure}
purposes, in figure~\ref{fig:2inprods}.)
Thus the two inner products are interdependent.
However, for the conjugate gradient method for spd systems the scalars
can be computed in ways that eliminate one synchronization point,
perhaps at
the expense of some extra computation. 

In this section we describe methods that reformulate one or the other
of the inner products in such a way that the inner products can be
computed simultaneously.  At the moment, two complementary approaches
are known. One can either eliminate the inner product $r_i^tr_i$ and
express it in terms of inner products with the search directions
(section~\ref{sec:elim-rr}), or one can try to eliminate the inner
product $p_i^tAp_i$ and express it in terms of inner products
involving gradients (section~\ref{sec:elim-pap}).

We will also describe other methods that do not follow this scheme,
but that achieve the same aim of minimising the delay incurred by the
inner products (section~\ref{sec:hide-dotprod}).

Combining inner products would seem to lead to a reduction of parallel
runtime, as it halves the communication time of the global
operations. However, Kaushik \emph{et
  al}~\cite{KaushikKeyesSmith:interaction} note that the actual time
in global operations is minimal, and that load balance is really to
blame.

\Level 2 {Elimination of the $r_i^tr_i$ inner product}
\label{sec:elim-rr}

The former approach was discovered by Saad~\cite{Sa:practicalKrylov}.
From the orthogonality of~$R$, the equation $APD=R(I-J)$ leads to
\begin{equation}
        r_{i+1}^tM\inv r_{i+1}+r_i^tM\inv r_i=d_{ii}^2(Ap_i)^tM\inv (Ap_i),
        \label{eq:saad}\end{equation}
so the norms of $r_i$ can be computed recursively without the need
for an inner product.
\begin{rfigure}
\begin{itemize}
\item From $p_i$ form $Ap_i$ and $M\inv Ap_i$.
\item Now compute simultaneously the inner products
\[p_i^tAp_i \qquad\hbox{and}\qquad (Ap_i)^tM\inv (Ap_i);\]
with $r_i^tM\inv r_i$, which was saved from the previous iteration, 
compute $\alpha_i = p_i^tAp_i/r_i^tM\inv r_i$,
and the value of the inner product 
\[r_{i+1}^tM\inv r_{i+1}=-r_iM\inv r_i +\alpha_i^2(Ap_i)^tM\inv (Ap_i).\]
With this, also compute $\beta_i=r_{i+1}M\inv r_{i+1}/r_iM\inv r_i$.
Save the vector $M\inv Ap_i$.
\item Update $M\inv r_{i+1}=M\inv r_i-\alpha_i M\inv Ap_i$.
\item Update $p_{i+1}=Mr_{i+1}-\beta_ip_i$.
\end{itemize}
\caption{Saad's CG method with one synchronisation point.}
\label{fig:saad1}
\end{rfigure}
We give the resulting algorithm in figure~\ref{fig:saad1}.
The balance of operations is:
\begin{itemize}
\item The preconditioner application $M\inv r_i$ is replaced 
by~$M\inv (Ap_i)$; only in the first iteration do we compute~$M\inv r_1$
explicitly, other $M\inv r_i$ vectors are obtained by vector updates.
\item The $r_i^tM\inv r_i$ inner product is replaced by~$(Ap_i)^tM\inv (Ap_i)$;
\end{itemize}

However, a simple analysis 
shows that this
method is unstable, so extra measures are necessary.
Hence, Meurant~\cite{Me:multicg} proposed computing $r_i^tM\inv r_i$
explicitly, together with $(Ap_i)^tM\inv(Ap_i)$. 
Thus, the $r_{i+1}^tM\inv r_{i+1}$ value computed by \eqref{eq:saad}
serves only as a predictor; it is later computed exactly.
\input saad_method
The resulting method (figure~\ref{fig:saad2}) takes three inner products
per iteration, and is as stable as the classical formulation of
the conjugate gradient method.
\begin{question}
Would it make the method more stable if $M\inv r_{i+1}$ is computed
explicitly, at the cost of an extra preconditioner application?
\end{question}

\Level 2 {Elimination of the $p_i^tAp_i$ inner product}
\label{sec:elim-pap}

Recently, several methods based on elimination of computing
$p_i^tAp_i$ (which is needed for $d_{ii}=r_i^tM\inv r_i/p_i^tAp_i$)
have been discovered.  These methods combine $R^tM\inv
AP=(I-U)^tP^tAP$ and $R^tM\inv AR=R^tAP(I-U)$ to
\begin{comment}
\[ (I-U)^tP^tAP=R^tAR+R^tAPU \]
and consider the diagonal of the left and right hand side. From
\begin{equation}
        p_i^tAp_i = r_i^tAr_i+r_i^tAp_{i-1}u_{i-1i}
        \label{eq:eijkhout}\end{equation}
we see that $p_i^tAp_i$ can be computed from $r_i^tAr_i$ and
$r_i^tAp_{i-1}=p_{i-1}^t(Ar_i)$. Hence, one extra inner product is
needed, and $Ar_i$ is computed instead of~$Ap_i$. This latter vector
can be computed recursively:
\[ Ap_{i+1}=AM\inv r_{i+1}+\sum_{k=1}^iAp_ku_{ki+1} \]
where the method now computes a matrix-vector product $AM\inv r_i$.
This method was proposed by Eijkhout~\cite{Eij:lawn51}.

Expanding $R^tM\inv AM\inv R$ one step further into $(I-U)^tP^tAP(I-U)$ gives
\end{comment}
\[ R^tM\inv AM\inv R=(I-U)^tP^tAP(I-U) \]
\[ \Rightarrow P^tAP=R^tM\inv AM\inv R+P^tAPU+U^tP^tAP-U^tP^tAPU. \]
Using the $A$-orthogonality of $P$ and the fact that the second and
third term
in the rhs are strictly upper and lower triangular respectively,
we find that 
\begin{equation}
        p_i^tAp_i=r_i^tM\inv AM\inv r_i-u_{i-1i}^2p_{i-1}^tAp_{i-1}.
        \label{eq:azevedo}\end{equation}
For the resulting variant of the conjugate gradient method
$Ap_i$ is again computed recursively from $AM\inv r_i$.
The inner products $r_i^tM\inv r_i$ and $r_i^tM\inv AM\inv r_i$ are computed
simultaneously, and the scalar $p_i^tAp_i$,
needed for $d_{ii}=r_i^tM\inv r_i/p_i^tAp_i$,
is computed from the above recurrence.
This method was proposed by Chronopoulos and Gear~\cite{ChGe:sstep},
and later reinvented by the present author and~\cite{DAzRo:unsync},
with a full analysis in~\cite{DAzEijRo:lawn56}.
The algorithm is given in figure~\ref{fig:azevedo}.
\input chrono_method

There is a fairly strong heuristic argument for the stability of this last
rearrangement: using equations (\ref{three-term:h:formulas}),
(\ref{h:both:sides}) and~(\ref{d:symm}) 
the recurrence for $p_n^tAp_n$ can be derived from the recurrence
\[ d_{nn}\inv=h_{n+1n+1}-h_{n+1n}d_{nn}h_{nn+1} \]
for pivot generation in the factorization of the Hessenberg matrix.
For symmetric positive definite systems this is a stable recurrence,
see the analysis in~\cite{DAzEijRo:lawn56}.

On the other hand, we note that the original CG algorithm had a fairly
good cache behaviour, in that vectors were used pretty much
immediately after they were generated. This rearranged algorithm has a
far less localised behaviour.

Another variant of this scheme, also proposed by Eijkhout~\cite{Eij:lawn51},
results from considering the diagonal of
\[ (I-U)^tPAP = R^tAP = R^tAR(I-U)\inv.\]
Since $R^tAR$ is tridiagonal the infinite expansion of $(I-U)\inv$
terminates quickly, and we find that
\[ p_i^tAp_i = r_i^tAr_i + r_i^tAr_{i-1}u_{i-1i}.\]
However, in the presence of rounding errors, this method, being based
on an infinite number of orthogonalities, becomes unstable in contrast
to the two previous methods which only use a single orthogonality
relation.

\begin{comment}
The number of inner products can be reduced to one by observing
(from the diagonal of $R^tAR=R^tR(I-J)D^{-1}(I-U)$) that
\[ r_{i+1}^tAr_{i+1}=r_{i+1}^tr_{i+1}(1+d_{ii}^{-1}u_{ii+1}); \]
if $u_{ii+1}$ is computed as $r_{i+1}^tr_{i+1}/r_i^tr_i$ then only the
$r_i^tr_i$ inner products need to be computed explicitly.
\end{comment}

Methods for removing a synchronization point generalize to
the Lanczos method. We find that
\[ q_{i+1}^tAp_{i+1}=s_{i+1}^tAr_{i+1}-s_i^tAp_iu_{ii+1} \]
from $(I-U)^tQ^tAP=S^tAR+S^tAPU$;
\[ q_{i+1}^tAp_{i+1}=s_{i+1}^tAr_{i+1}-u_{ii+1}^2q_i^tAp_i \]
using $A$-orthogonality of $Q$ and $P$ in
\[ Q^tAP=S^tAR+Q^tAPU+U^tQ^tAP-U^tQ^tAPU, \]
and we get
\[ s_{i+1}^tr_{i+1}-s_i^tr_i=d_{ii}^2(A^tq_i)^t(Ap_i) \]
from the equations $APD=R(I-J)$ and $A^tQD=S(I-J)$ using orthogonality
of $S$ and~$R$.

\Level 2 {Other approaches for minimizing synchronisation overhead}
\label{sec:hide-dotprod}

An approach for a  reduced synchronization overhead was proposed
by~\cite{VRo:innerproduct} and~\cite{ChGe:sstep}, based on computing
a number of Krylov vectors and orthogonalizing these as a block.
This approach requires some caution since it is less stable.
See also section~\ref{sec:sstep-cg}.

Van der Vorst, in Demmel {\it et al.}~\cite{dehevo92:acta}, proposed another
reformulation CG method that eliminates synchronisation overhead. This
method, which unlike the above ones can not be generalized to the
nonsymmetric case, is based on the following observation. If~$M$~is
symmetric and factored as~$LL^t$, the inner product~$r^tM\inv r$ can
be computed as~$(L\inv r)^t(L\inv r)$, that is, the value of the inner
product can be computed after only the forward solve has been
performed. Therefore, the computation of the backward solve, taking
$L\invt$~times the intermediate result~$L\inv r$, can overlap the
communication involved in the inner product of~$(L\inv r)^t(L\inv
r)$. 

Note that this idea presupposes that $M$ is both a factorisation, and
does not involve communication at all or at least less than for an
inner product. Examples would be the  Block Jacobi method or an
additive Schwarz method, where in both cases the local solve is
realised through a Cholesky factorisation.

In order to accomodate as much communication delay as possible, we
also delay the solution update slightly, to let it also happen during
the inner product communication stage. As a result, the normal
sequence
\begin{quotation}\begin{tabbing}
update \=$r_{i+1}=r_i-\alpha_iAp_i$\\
\>$x_{i+1}=x_i-\alpha_ip_i$\\
solve $z_{i+1}=M\inv r_{i+1}$\\
compute $\beta_{i+1}=z_{i+1}^tr_{i+1}$
\end{tabbing}\end{quotation}
now becomes
\begin{quotation}\begin{tabbing}
update \=$r_{i+1}=r_i-\alpha_iAp_i$\\
solve $s_{i+1}=L\inv r_{i+1}$\\
compu\=te the \=local parts of $s_{i+1}^ts_{i+1}$ and\\
\>\>start an asynchronous reduction to sum them\\
meanwhile:\\
\>update $x_{i+1}=x_i-\alpha_ip_i$\\
\>solve $z_{i+1}=L\invt s_{i+1}$\\
wait for the reduction to finish,\\
compute $\beta_{i+1}=z_{i+1}^tr_{i+1}$
\end{tabbing}\end{quotation}
This method is fully equivalent to the original CG algorithm, except
for the slightly different calculation of the~$z^tr$ inner product. In
fact, it is slightly more stable because of the symmetric nature of
the inner product calculation.

This method still has one inner product that forms a bottleneck. We
can eliminate this by combining van der Vorst's above
approach with Saad's method (or rather, Meurant's modification of it)
from section~\ref{sec:elim-rr}. We proceed as follows:
\begin{quotation}\begin{tabbing}
genera\=te $r_{i+1}$ and $z_{i+1}=M\inv r_{i+1}$ by update relations\\
solve $q_{i+1}=L\inv Ap_{i+1}$\\
compu\=te the \=local parts of $z_i^tr_i$, $p_i^tAp_i$, $q_{i+1}^tq_{i+1}$, and\\
\>\>start an asynchronous reduction to sum them\\
meanwhile:\\
\>update $x_{i+1}=x_i-\alpha_ip_i$\\
\>solve $L\invt (L\inv Ap_i)$\\
wait for the reduction to finish.
\end{tabbing}\end{quotation}

Another way of hiding communication is to delay the update of the
solution vector and let this overlap one of the inner products.

\Level 2 {Pipelined CG}

The overlapping approach of van der Vorst also underlies the
`pipelined' iterative methods. For instance, pipelined CG is based on
introducing explicit recurrences for $M\inv r_i$ and $Ap_i$:
\[ \hbox{let $q_1=Ap_1, z_1=M\inv r_1$ be given} \]
\[
\begin{cases}
  \pi = p_1^tAp_1\\ \alpha = \rho_1 /\pi \\
  r_2 = r_1 + Ap_1\alpha\\
  z_2 = M\inv r_2\\
  \rho_2 = r_2^tz_2\\ \beta=\rho_2/\rho_1 \\
  p_2 = z_2 + p_1\beta\\
\end{cases}
\Rightarrow
\begin{cases}
  \pi = p_1^tq_1\\ \alpha = \rho_1 /\pi \\
  \begin{cases}
  r_2 = r_1 + q_1\alpha\\ z_2 = z_1+ M\inv q_1 \\
  \end{cases}\\  
  \rho_2 = r_2^tz_2\\ \beta=\rho_2/\rho_1 \\
  \begin{cases}
    p_2 = z_2 + p_1\beta\\
    q_2 = Az_2+q_1\beta\\
  \end{cases}\\  
\end{cases}
\]
We now observe that the added relations contain terms $M\inv q_1$ and
$Az_2$ that can be overlapped with inner product calculations:
\[
\begin{array}{l}
  \begin{cases}
    \hbox{overlapped:}\\
    \hbox{compute $M\inv q_1$} \\ \pi = p_1^tq_1\\ 
  \end{cases}\\  
  \alpha = \rho_1 /\pi \\
  \begin{cases}
    \hbox{in arbitrary order:}\\    
    r_2 = r_1 + Ap_1\alpha\\
    z_2 = z_1 + M\inv q_1\alpha\\  
  \end{cases}\\
  \begin{cases}
    \hbox{overlapped:}\\
    \hbox{compute $Az_2$} \\ \rho_2 = r_2^tz_2\\
  \end{cases}\\  
  \beta=\rho_2/\rho_1 \\
  \begin{cases}
    \hbox{in arbitrary order:}\\    
    p_2 = z_2 + p_1\beta\\
    q_2 = Az_2 + q_1\beta\\
  \end{cases}\\  
\end{array}
\]

\begin{comment}
Another similar approach was proposed by Gropp. Replacing the explicit
calculation of $q=Ap$ and $z=M\inv r$ by recurrences, and introducing
the computation of $Az$ and $M\inv q$, gives the following scheme:
\[
\begin{array}{l@{{}={}}l}
  p&z+\beta p\\
  q&Ap\\ \multispan{2}\strut\\ \multispan{2}\strut\\ 
  \tau&p^tq\\ \alpha&\rho/\tau\\ 
  r&r+\alpha q\\ z&M\inv r\\ \multispan{2}\strut\\ \multispan{2}\strut\\ 
  \rho&r^tz\\ \beta&\rho/\rho'\\ 
\end{array}
\quad\Rightarrow\quad
\begin{array}{l@{{}={}}l}
  p&z+\beta p\\
  ;\,q&Ap\\ q&v+\beta q\\ w&M\inv q\\ 
  \tau&p^tq\\ \alpha&\rho/\tau\\ 
  r&r+\alpha q\\ ;z&M\inv r\\ z&z+\alpha w\\ v&Az\\
  \rho&r^tz\\ \beta&\rho/\rho'\\
\end{array}
\quad\Rightarrow\quad
\begin{array}{l@{{}={}}l}
  p&z+\beta p\\
  ;\,q&Ap\\ q&v+\beta q\\ \multispan{2}$\begin{cases}w=M\inv q\\ 
  \tau=p^tq\\ \end{cases}$\\ \alpha&\rho/\tau\\ 
  r&r+\alpha q\\ ;z&M\inv r\\ z&z+\alpha w\\ 
  \multispan{2}$\begin{cases}v=Az\\ \rho=r^tz\end{cases}$\\ 
  \beta&\rho/\rho'\\
\end{array}
\]
As indicated by the grouping in the third column, the computationally
intensive parts (the $Az$ and $M\inv q$ computation) can now be
overlapped with the inner product calculations.
\end{comment}

\Level 1 {Complex symmetric CG}

We recall lemma~\ref{lemma:cg-tri} which states that $AR=RH$ is a
tri-diagonalisation of~$A$ if $A$~is symmetric. It is easy to see that
this statement also holds for complex symmetric systems. Thus, if we
apply a CG algorithm to complex a symmetric system, but we use the
$x^ty$ inner product\footnote{Which is not strictly speaking an inner
product in complex spaces.} instead of the $x^*y$ one, we arrive at a
three-term recurrence for the residual vectors. 

The straightforward generalisation of the CG algorithm to complex
spaces would use the $x^*y$ inner product, and would only yield
three-term recurrences for Hermitian systems. General complex
non-Hermitian systems would need long recurrences, as in the case of
Arnoldi orthogonalisation methods for nonsymmetric real systems.

\Level 1 {Double size systems}

There are several observations to be made about the augmented matrix 
\[\begin{pmatrix}0&A\cr A^t&0\end{pmatrix}.\]

\Level 2 {Singular value computations}\label{sec:svd}
\index{Singular values}

Let $P$ be normalized orthogonal combinations of a Krylov sequence
$\KmethodAv{AA^t}{p_1}$, so that the Hessenberg matrix in
$AA^tP=PH$, which is tridiagonal because of the symmetry of~$AA^t$,
can be written as $H=BB^t$ with $B$ upper bidiagonal (see
lemma~\ref{lemma:symmetric-H}). Introducing a sequence~$Q$ that
satisfies $AQ=PB$ we get a coupled system
\[ AQ=PB,\quad A^tP=QB^t. \]
Lanczos in~\cite{La:differential-book} observed that this coupled
system could be generated by iterating on a double size matrix:
the resulting system is 
\[ \begin{pmatrix}0&A\cr A^t&0\end{pmatrix}\begin{pmatrix}P&0\cr 0&Q\end{pmatrix}=
    \begin{pmatrix}P&0\cr 0&Q\end{pmatrix}\begin{pmatrix}0&B\cr B^t&0\end{pmatrix}.\]

The equation $AA^tR=RBB^t$ can be used to compute singular values: if
$R$~is unitary ($R^t=R\inv$; Conjugate Gradient iteration yields an
orthogonal~$R$, which can be made unitary by scaling), then $AA^t$ and
$BB^t$ have the same eigenvalues, hence $A$ and~$B$ have the same
singular values.  Consequently, the double size iteration, or
equivalently the coupled relations for $P$ and~$Q$, can be used as a
preliminary step of bi-diagonalisation in computing singular values.
This method is used by Golub, Luk, and
Overton~\cite{GoLuOv:BlockLanczos}; they use a block method which
turns~$B$ into a block bidiagonal matrix where the diagonal blocks are
upper triangular, and the upper diagonal blocks are lower triangular.

See also section~\ref{sec:bicg-augmented}.

\Level 2 {Iterating on the normal equations; LSQR}

The Conjugate Gradient method only works for symmetric (and positive
definite) systems. Hence, people have advocated solving nonsymmetric
systems by iterating on the normal equations, that is,
replacing~$Ax=b$ by \[ A^tAx=A^b.\]
We can split this equation as
\[ \left.\begin{matrix}y=Ax\cr A^ty=A^tb\end{matrix}\right\}\Rightarrow
    \begin{pmatrix}0&A\cr A^t&0\end{pmatrix}\begin{pmatrix}y&x\end{pmatrix}=\begin{pmatrix}0&A^tb\end{pmatrix}.\] This seems
to be the basis of the LSQR method of Paige and
Saunders~\cite{PaSa:lsqr}.

\Level 2 {Lanczos iteration as augmented
Conjugate Gradient iteration}
\label{sec:bicg-augmented}

\def\Ab{{\mathbf A}}\def\Mb{{\mathbf M}\inv}
\def\pb{{\mathbf p}}\def\rb{{\mathbf r}}
Lanczos~\cite{La:differential-book} observed that the left and right
sequences of the Lanczos iteration can be generated from a single
Conjugate Gradient iteration process with double size vectors.
Specifically, it uses as iteration matrix \[\Ab=\begin{pmatrix}0&A\cr A^t&0\end{pmatrix}.\]

Let $R$ and $S$ be the right and left sequences, that is, based on the
Krylov sequences for $A$ and~$A^t$, and $P$ and $Q$ the corresponding
search directions.
We express the augmented iteration in terms of the sequences
\[ \pb_i=\begin{pmatrix}q_i\cr p_i\end{pmatrix},\quad \rb_i=\begin{pmatrix}r_i\cr s_i\end{pmatrix}, \]
and we use a preconditiong matrix \[\Mb=\begin{pmatrix}0&M\invt\cr
M\inv&0\end{pmatrix};\]
we note that the matrix and preconditioner are symmetric.

The matrix vector product is
\[ \Ab\pb_i=\begin{pmatrix}0&A\cr A^t&0\end{pmatrix}\begin{pmatrix}q_i\cr p_i\end{pmatrix}=\begin{pmatrix}Ap_i\cr
A^tq_i\end{pmatrix},\]
and the preconditioner application is
\[ \Mb\rb_i=\begin{pmatrix}0&M\invt\cr M\inv&0\end{pmatrix}\begin{pmatrix}r_i\cr s_i\end{pmatrix}=\begin{pmatrix}M\invt
s_i\cr M\inv r_i\end{pmatrix}. \]
The $q_i^tAp_i$ inner product becomes
\[ \pb_i^t\Ab\pb_i=
\begin{pmatrix}q_i^t&p_i^t\end{pmatrix}\begin{pmatrix}0&A\cr A^t&0\end{pmatrix}\begin{pmatrix}q_i\cr p_i\end{pmatrix} =
q_i^tAp_i + p_i^tA^tq_i = 2q_i^tAp_i, \]
and the $s_iM\inv r_i$ inner product becomes
\[ \rb_i^t\Mb\rb_i=
\begin{pmatrix}r_i^t&s_i^t\end{pmatrix}\begin{pmatrix}0&M\invt\cr M\inv&0\end{pmatrix}\begin{pmatrix}r_i\cr
s_i\end{pmatrix} = r_i^tM\invt s_i+s_i^tM\inv r_i=2s_i^tM\inv r_i. \]
However, in both inner products we use the fact that scalars can be
transposed, that is, this method will not work as a block method.
\begin{question}Reconcile this with the fact that Golub et al use this
trick for a block method.\end{question}

The residual update
\[ \rb_{i+1}=\rb_i+\Ab\pb{\rb_i^t\Mb\rb_i\over \pb_i^t\Ab\pb_i} \]
comprises both the left and right residual update, and likewise does
the search direction update
\[ \pb_{i+1}=\Mb\rb_{i+1}-\pb_i{\rb_{i+1}^t\Mb\rb_{i+1}\over
\rb_i\Mb\rb_i}. \]

If we compute the Lanczos sequences by three-term recurrences
(section~\ref{sec:three-term}) we need to compute inner products
such as $s_i^tM\inv AM\inv r_i$.
For the augmented vectors:
\[ \rb_i^t\Mb\Ab\Mb\rb_i=
\begin{pmatrix}s_i^tM\inv&r_i^tM\invt\end{pmatrix}\begin{pmatrix}AM\inv r_i\cr A^tM\invt
s_i\end{pmatrix}\]
\[ =s_i^tM\inv AM\inv r_i+r_i^tM\invt A^tM\invt s_i=2s_i^tM\inv AM\inv
r_i. \]

\Level 1 {Scaled sequences}
\label{sec:scaled}
\FurtherReading

If $V$ is an arbitrary sequence satisfying $AV=V\tilde H$ with
$\tilde H$~an upper Hessenberg matrix, then $V$~can sometimes be scaled
to a residual sequence~$R$. We first give the algorithm, then discuss
the conditions under which it is well-defined.

\begin{algorithm}{Scale combination of Krylov sequence to residual sequence}
\label{hess-to-res-hess}
Given $AV=V\tilde H$, construct a diagonal matrix~$\Omega$ such that
$R=V\Omega$ is a residual sequence.

Define $H=\Omega\inv \tilde H\Omega$, then $AR=RH$,
and the algorithm is equivalent to constructing
$\Omega$ such that~$H$, 
by theorem~\ref{zero-col-residual},
has zero column sums.

The choice for~$\omega_1$ is arbitrary, or determined by the context.
For $n=1,2,\ldots$:
\begin{enumerate}
\item Let $\omega_k$ be given for $k\leq n$. This is true for~$n=\nobreak1$.
\item For $k<n$, $h_{kn}=\omega_k^{-1}\tilde h_{kn}\omega_n$ can be computed.
\item Define $h_{nn}=\tilde h_{nn}$.
\item  The zero column sum requirement
        lets us compute $h_{n+1n}=-\sum_{k\leq n}h_{kn}$;
        from $h_{n+1n}=\omega_{n+1}^{-1}\tilde h_{n+1n}\omega_n$
        we can compute
    \begin{equation}
     \omega_{n+1}=\tilde h_{n+1n}\omega_n/h_{n+1n}.
        \label{eq:new-omega}\end{equation}
\end{enumerate}
\end{algorithm}

\Level 2 {Theory}
\label{sec:scaling-theory}

In this section we will show that orthogonal sequences satisfying
a Hessenberg relation can be scaled to a residual sequence.
We start by giving a lemma that translates \eqref{eq:new-omega}
in terms of the sequence to be scaled.

\begin{llemma}{Condition for scaling a sequence to a residual sequence}
\label{lemma:scale-if-1}
Let a matrix~$A$, an irreducible upper Hessenberg matrix~$\tilde H$, and
a sequence~$V$ satisfying $AV=V\tilde H$ be given, then $V$~can be scaled
as $R=V\Omega$ to a residual sequence if and only if every~$v_i$ has a nonzero
component of~$v_1$.
\end{llemma}
\begin{proof} Let $K=K\method{A,v_1}$, and suppose that $V$~can be scaled
to a residual sequence~$R=V\Omega$. In particular, all~$\omega_i\not=0$.
We then have
\[ AV=V\tilde H\qquad\hbox{and}\qquad AR=R\Omega\inv \tilde H\Omega
                \equiv RH, \]
so there are nonsingular triangular matrices~$U$,~$\tilde U$ such that
\[ V=K\tilde U,\qquad R=KU, \]
and they are related by $U=\tilde U\Omega$.
The `only if' half of the statement now follows simply:
since $u_{1j}\equiv\nobreak1$ (see lemma~\ref{R-Krylov-combo})
it follows that all~$\tilde u_{1j}\not =\nobreak 0$.

Conversely, suppose that, with $\tilde U$ as defined above,
$\tilde u_{1j}\not=\nobreak0$.
From the above algorithm we find that
\[ \omega_{n+1}
    =\tilde h_{n+1n}\omega_n/h_{n+1n}
    =-{\tilde h_{n+1n}\omega_n\over \sum_{k\leq n}h_{kn}}
    =-{\tilde h_{n+1n}\over \sum_{k\leq n}\omega_k\inv \tilde h_{kn}}, \]
Rewriting the sum in the numerator in terms of~$\tilde U$,
where we recall from \eqref{eq:hess-from-u}
that $\tilde H=\tilde U\inv J\tilde U$, gives 
    \[ \sum_{k\leq n}\omega_k\inv \tilde h_{kn} = 
        -\tilde u_{1n+1}\tilde u_{nn}/\tilde u_{n+1n+1}. \]
Having $\tilde u_{1j}\not=0$
(and note that $u_{nn}\not=0$ from the irreducibility of~$H$)
is thus seen to be a sufficient condition
for the above algorithm to be well-defined.\end{proof}

As an example of a sequence that can not be scaled to a residual
sequence, consider a Krylov sequence, that is, $H=J$. 
In this case the Hessenberg matrix is not irreducible, so the
condition of lemma~\ref{lemma:scale-if-1} are not satisfied.
As a more intuitive argument, note that the scaling would have to
be such that $H$~winds up with zero
column sums, which is clearly impossible.

In the following theorem we prove that the $\tilde u_{1j}\not=0$
condition is always satisfied if $V$~is an orthogonal sequence.

\begin{ttheorem}{An orthogonal sequence can be scaled to a residual sequence}
Let a non-singular matrix matrix~$A$,
an irreducible upper Hessenberg matrix~$H$, and
a sequence~$V$ satisfying $AV=VH$ be given.
If $V$~is orthogonal, it can be scaled to a residual sequence.
\end{ttheorem}
\begin{proof}
Let $V$ be an orthogonal sequence,
let $K$ be the Krylov sequence
    \[ K=\Kmethodv{\alpha v_1},\qquad\hbox{some $\alpha\not=0$}, \]
and let $U$~be a non-singular upper triangular matrix such that~$V=KU$.
Suppose by contradiction
that there is a smallest index~$n$ for which~$u_{1n}=\nobreak0$.
From the above we recall that the existence of such an index
precludes the sequence from being scaled to a residual sequence.

We will now investigate the consequences of the identity
\[ v_n=\sum_{i=1}^{n}k_iu_{in} \]
which is the $n$-th column of~$V=KU$.
Using $u_{1n}=0$, elementary rewriting gives
\begin{equation}
     v_n=\sum_{i=1}^nk_iu_{in}=\sum_{i=2}^nk_iu_{in}
    =\sum_{i=2}^nAk_{i-1}u_{in}=A\sum_{i=1}^{n-1}k_iu_{i+1n}.
    \label{eq:scale-proof-t1}\end{equation}
From the non-singularity of~$U$ we can express each~$k_i$ as
a linear combination of~$v_j$ vectors ($j\leq\nobreak i$),
so \eqref{eq:scale-proof-t1} translates to the statement that
there are coefficients~$\alpha_i$ such that
\[ v_n=A\sum_{i=1}^{n-1}v_i\alpha_i \]
From the orthogonality of~$V$ we then find that
\[ v_n^tA\inv v_n=v_n^t\sum_{i=1}^{n-1}v_i\alpha_i =
     \sum_{i=1}^{n-1}0\cdot\alpha_i=0, \]
and from the non-singularity of~$A$ this implies that~$v_n=\nobreak0$.
Since this contradicts the orthogonality of~$V$,
we find that~$u_{1n}\not=\nobreak0$.
Hence, by lemma~\ref{lemma:scale-if-1}, we find that the sequence~$V$
can be scaled to a residual sequence.
\end{proof}

\begin{question}
How do you reconstruct the $\alpha$, $\beta$
coefficients of the original algorithm?
\end{question}

The condition of the irreducibility of~$H$ in the above results
is essential for iterative methods. 

\begin{remark}\label{H-red}
Suppose that $h_{n+1n}=0$,
then $AV_n=V_nH_n$, so $V_n$, and consequently the Krylov sequence~$K_n$
from which it was built, form an invariant subspace.
\end{remark}

Thus, a reducible Hessenberg matrix
precludes forming more accurate approximations to 
the solution of the linear system under consideration.
See section~\ref{sec:breakdown} for more details.

\Level 2 {A normalised CG algorithm}

We will now derive a CG algorithm that uses normalised residuals.
There is a point to such methods.
For instance, suppose that the matrix-vector product is not an 
explicitly given operation, but (in the context of a non-linear
solver) evaluated as
\[ F(u)v \approx (f(u+hv)-f(u))/h\qquad\hbox{some fixed $h$},\]
then the error is proportional to~$||v||$, so it pays to
normalise, or at least limit the size of, the vectors
that are multiplied.

In particular, we will derive here a variant of the 
Conjugate Gradient algorithm
with coupled two-term recurrences, that generates an orthonormal sequence
which later will be scaled to a residual sequence. 
This is of course reminiscent of the GMRES algorithm which also generates
an orthonormal sequence. However, that sequence is never explicitly
scaled to a residual sequence.

Consider the usual equations for a conjugate gradient method
\[ APD=R(I-J),\qquad M\inv R=P(I-U),\qquad R^tM\inv R=\Omega^2. \]
We will now derive an algorithm that directly generates
the normalised vectors
\[ V=R\Omega\inv.\]
With
\[ Q=P\Omega\inv,\qquad \tilde U=\Omega U\Omega\inv,
    \qquad L=\Omega J\Omega\inv,\]
it is easy to check that we get transformed relations
(and note that $D$ is not transformed)
\[ M\inv V=Q(I-\tilde U),\qquad AQD=V(I-L). \]
Replacing $V\rightarrow R$, et cetera, we now get the defining
relations
\begin{equation} APD=R(I-L),\qquad MR=P(I-U),\qquad R^tM\inv R=I.
    \label{eq:cg-norm}\end{equation}
First of all, the coefficients of~$L$ follow from the normalisation,
since \[ r_{i+1}\ell_{i+1i}=r_i-Ap_id_i. \]
From the second relation of \eqref{eq:cg-norm} we get
    \[ R^tM\inv R=R^tP(I-U) \]
so $P^tR$ is lower triangular with identity diagonal.
From the first relation we get \[ P^tAPD=P^tR(I-L) \]
so $P^tAP$ is lower triangular and \[ d_i=1/p_i^tAp_i.\]
Combining the first two relations we get
\begin{eqnarray*}
    R^tM\invt AP&=&R^tM\invt R(I-L)D\inv\cr &=&(I-U)^tP^tAP\end{eqnarray*}
so \[ P^tA^tP(I-U)=D\inv(I-L^t). \]
In this last equation we consider column~$n+\nobreak1$:
\begin{equation}
        \begin{pmatrix}p_1^tA^tp_1&\ldots&p_1^tA^tp_{n+1}\cr
                &\ddots&\vdots\cr
                &&p_{n+1}^tA^tp_{n+1}\end{pmatrix}
        \begin{pmatrix}-u_{1n+1}\cr \vdots \cr -u_{nn+1}\cr 1\end{pmatrix}
        =\begin{pmatrix}0\cr \vdots \cr 0 \cr -d_n\inv\ell_{n+1n}\cr d_{n+1}\inv\end{pmatrix}
\end{equation}
\iffalse
Deleting the last row and column:
\begin{equation}
        {1\over p_n^tAp_n}
        \begin{pmatrix}p_1^tA^tp_1&\ldots&p_1^tA^tp_n\cr
                &\ddots&\vdots\cr
                &&p_n^tA^tp_n\end{pmatrix}
        \begin{pmatrix}u_{1n+1}\cr \vdots \cr u_{nn+1}\end{pmatrix}
        =\begin{pmatrix}0\cr \vdots \cr 0 \cr d_n\inv\ell_{n+1n}\end{pmatrix}
\end{equation}
\fi
This first of all confirms the value for~$d_{ii}$ derived above.
In general, we can now solve the $n+1$st column of~$U$;
in the particular case of symmetry we get $u_{nn+1}=\ell_{n+1n}$.

The one issue we have not tackled in this normalised algorithm
is the solution update. Remembering that the true search
direction is $\omega_i$ times the normalised one,
we find the solution update to be
\[ x_{i+1} = x_i+p_id_i\omega_i.\]

We now give the full preconditioned algorithm for the symmetric case.
\begin{algorithm}{Conjugate Gradient algorithm with normalised matrix-vector
product}
Let $x_1$, $f$, $A$ be given, and compute
\[ r_1 = Ax_1-f,z_1=M\inv r_1,\qquad \omega_1=\|r_1\|,
    \qquad r_1\leftarrow r_1/\omega_1,z_1\leftarrow z_1/\omega_1. \]
Now iterate for~$i=1,\ldots$:
\begin{itemize}
\item Perform a stopping test on~$r_i\omega_i$.
\item For $i=1$, $p_1=z_1$; in general $u_{i-1i}=\omega_{i-1}$, and
    \[ p_i=z_i+p_{i-1}\omega_{i-1}. \]
\item Let $d_i=1/p_i^tAp_i$; use this to update:
\[ x_{i+1} = x_i-p_id_i\omega_i,\qquad r_{i+1}=r_i-Ap_id_i,z_i=M\inv r_i. \]
\item Subsequently we normalise the residual:
\[ \ell_{i+1i} = \sqrt{z_{i+1}^tr_{i+1}};\qquad
     r_{i+1}\leftarrow r_{i+1}/\ell_{i+1i},
     z_{i+1}\leftarrow z_{i+1}/\ell_{i+1i}. \]
\item Construct column~$i$ of the Hessenberg matrix
$\tilde H=(I-L)D(I-U)$ of
the normalised sequence, and derive~$H$ from it as described in 
algorithm~\ref{hess-to-res-hess}; construct~$\omega_{i+1}$.
\end{itemize}
\end{algorithm}

In this algorithm we normalised the $r_i$~vectors, while the matrix-vector
product is performed on the $p_i$~vectors. While these are not themselves
normalised, we expect them to be very much limited in norm, given their
close relation to the $r_i$ vectors. It would be possible to formulate
the algorithm such that not $\|r_i\|=1$, but $\|p_i\|_A=1$ is satisfied.

\Level 2 {Normalised CGS}

This attempt is in the right direction:
\verbatiminput{cgs_norm.m}
Note:
\begin{itemize}
\item The \verb+rr+ vector is divided by $\ell^2$, so that the $r_1^t\verb+rr+$
product comes out as one.
\item The $\B2i$ update is probably wrong because we don't normalise
the left $r_i$ sequence. Maybe reconstruct the $\alpha$ and $beta$ 
coefficients and do the left sequence exact?
\end{itemize}

\Level 1 {Seed systems}
\label{sec:seed}
\FurtherReading

In several sections of this monograph we have seen block methods,
where multiple systems with identical coefficient matrix but different
righthand sides are solved. It is also possible to reuse information
from one system if the next system has the same coefficient matrix, or
a sufficiently similar one.

Chan and Ng~\cite{ChanNg:Galerkin-multiple} consider the case of not
identical coefficient matrices. Their convergence theorem relates the
true solution~$x$ of a second system to the approximate one~$\bar x$
obtained by projecting onto the subspace~$K_m$ from the first system
as
\[ x^{(2)}-\bar x^{(2)}=\sum_k c_k q_k, \qquad |c_k|\leq E_k+F \]
where $q_k$ are eigenvectors of the second system and
\[ E_k = \|P^\perp_m x^{(2)}\|_2\,|\sin\angle(q_k,K_m)| \]
and
\[ F=\|A^{(1)\inv}\|_2\, \|(A^{(2)}-A^{(1)})x^{(2)}\|_2\]
where $V_m$ is the orthonormal scaling of~$K_m$,
$P^\perp_m=I-V_mT_m\inv V^t_mA^{(1)}$ is the $A^{(1)}$-orthogonal
projection onto~$K_m$ (see section~\ref{sec:projection} for the
theory of projection in iterative methods), and $T_m=V^t_mA^{(1)}V_m$.

\Level 1 {Polynomial acceleration}\label{sec:poly:acc}
\FurtherReading

Methods such as conjugate gradients are sometimes considered to be
an acceleration of a basic iterative method. Suppose iterates $x_n$
have been generated by~(\ref{very:basic:update}), then
the idea behind acceleration is to take combinations 
\begin{equation}
        \tilde x_n=\sum_{i=1}^n x_iu_{in}
        \label{semi-iterative:combine}\end{equation}
with the consistency condition 
\begin{equation}
        \sum_{i=1}^nu_{in}=1.
        \label{U:one:colsum}\end{equation}
Varga~\cite{Va:book} calls (\ref{semi-iterative:combine}) the
semi-iterative method corresponding to the basic iterative method. The
residuals $\tilde r_n=A\tilde x_n-f$ are seen to satisfy~$\tilde R=RU$
(where $U$~is an upper triangular matrix containing the
$u_{in}$~coefficients) 
because of the above consistency condition.

Writing the basic iterative method as $AR=R(I-J)$ we find for the
accelerated residuals $A\tilde R=\tilde RH$ where $H$~is the upper
Hessenberg matrix~$I-U\inv JU$.

\begin{lemma}
The consistency condition (\ref{U:one:colsum}) implies the consistency
condition of lemma~\ref{H:zero:colsum}  for $H=I-U\inv JU$.
\end{lemma}
\begin{proof} We can express equation~(\ref{U:one:colsum}) as $e^tU=e^t$.
Since this implies $e^tU\inv =e^t$, and since $e^tJ=e^t$, we find that
$e^tH=0$.\end{proof}

These acceleration methods are also called `polynomial acceleration'
methods because of the following fact.

\begin{lemma}
The accelerated residuals are related to the original residuals as
\[ \tilde r_n=P_n(I-A)r_1 \] where $P_n$ is an $n-1$-st degree
polynomial with its coefficients in the $n$-th column of~$U$, where
$U$~contains the $u_{in}$~coefficients of
equation~(\ref{semi-iterative:combine}).
\end{lemma}

\begin{proof} Lemma \ref{I-A:Krylov} established that the residuals
of the basic method
are Krylov vectors in a sequence with matrix~$I-A$. It then follows
from lemma~\ref{th:polynomial} that the combinations~$\tilde R$ are
obtained by multiplying by a matrix polynomial as stated above.
\end{proof}

\Level 1 {Left and right preconditioning}
\label{sec:left-right-prec}
\FurtherReading

\begin{comment}
The proof of theorem~\ref{main-theorem} noted that all systems with
$A\inv f$ as solution can be written as~$MAx=Mf$. The matrix~$M$ is
commonly called the `left-preconditioner'\footnote{But note that the
residuals are in a Krylov space of the matrix~$AM$.}. 

A~right preconditioner can
be employed as follows. If $\bar x=A\inv f$, then a polynomial method
$X=\PmethodA{AN}$ is a method for~$N\inv x$, and we need to transform
the iterates --~which was not necessary in the case of a left
preconditioner~-- to obtain a method for the original system.

Specifically, we are interested in the sequence $NX=\{Nx_i\}_{i\geq
1}$. From lemma~\ref{BX-method} we already know that this is again a
polynomial method, so by theorem~\ref{main-theorem} above it can be
characterized by a single {\em left\/} preconditioner, but we will
derive this fact in a second way.

Consider any polynomial method for $N\inv A\inv f$,
and let residuals be defined by $R=ANX-fe^t$.
By theorem~\ref{main-theorem} above, we can compute iterates,
residuals, and search directions by
        \[ X(I-J)=PD,\qquad ANPD=R(I-J),\qquad MR=P(I-U). \]
Since $X$ is a method for $N\inv\bar x$, we introduce the sequences
        \[ \tilde X=NX,\qquad \tilde P=NP, \]
with which we get the method
        \[ \tilde X(I-J)=PD,\qquad A\tilde PD=R(I-J),\qquad
                NMR=\tilde P(I-U). \]
We see that the right preconditioner is simply aborbed as part of
the total preconditioner~$NM$.
Note also that $R=ANX-fe^t=A\tilde X-fe^t$, that is, the residuals of the
right preconditioned method are also the residuals of the sequence
for~$\bar x$.
\end{comment}

In this monograph we have derived all methods in their
`preconditioned' form, that is, involving a matrix~$M$.
The substitution $M=I$ in the methods derived above gives the 
`unpreconditioned' version, which is traditionally the one derived
first. Deriving a preconditioned version could then be done
by substituting either $AM\inv$ or $M\inv A$ for~$A$ in the unpreconditioned
method. This gives rise to two methods:
\begin{equation}
 \hbox{Left preconditioned method: solve $M\inv Ax=M\inv b$.}\end{equation}
and 
\begin{equation}
 \hbox{Right preconditioned method: solve $AM\inv x'=b$; $x=M\inv x'$.}\end{equation}
and a combination two-sided preconditioning method:
$M\inv_1AM\inv_2x'=M\inv_1b$, $x=M_2\inv x'$.

These methods are equivalent to the normal form derived above,
as we will show now. In particular, we address these two points:
\begin{itemize}
\item Is a one-sided preconditioner equivalent to (symmetric) Conjugate
Gradients for a combination of symmetric matrix and symmetric preconditioner?
and,
\item Are the solution and residual corresponding to the original
problem easily available in each iteration, without spending
too much work on back transformations?
\end{itemize}

\Level 2 {Left preconditioning}

The left-preconditioned BiCG method (which we take as prototypical
of Krylov space methods) is derived by applying an unpreconditioned
algorithm to the coefficient matrix $M\inv A$, the initial guess $x_1$,
and the right hand side~$M\inv b$.
\begin{algorithm}{Left preconditioned BiConjugate Gradient Method}
\label{alg:bicg-left}
Let $x_1$, $A$, $b$, and $M$ be given. 
Set initially $r_1=M\inv Ax_1-M\inv b$. Then, in each iteration:
\begin{eqnarray*}
    x_{n+1}&=&x_n+p_n\alpha_n\\
    r_{n+1}&=&r_n+M\inv Ap_n\alpha_n\\
    s_{n+1}&=&s_n+A^tM\invt q_n\alpha_n\\
    &&\hbox{where $\alpha_n=(s_n^tr_n)/(q_n^tM\inv Ap_n)$}\\
    \hbox{and}\\
    p_{n+1}&=&r_{n+1}-p_n\beta_n\\
    q_{n+1}&=&s_{n+1}-q_n\beta_n\\
    &&\hbox{where $\beta_n=(s_{n+1}^tr_{n+1})/(s_n^tr_n)$}
\end{eqnarray*}
\end{algorithm}
We rewrite, expressing the algorithm in terms of 
transformed quantities $M r_n$ and~$M\invt q_n$:
\begin{eqnarray*}
    M r_1&=&Ax_1-b\\
    x_{n+1}&=&x_n+p_n\alpha_n\\
    M r_{n+1}&=&M r_n+Ap_n\alpha_n\\
    s_{n+1}&=&s_n+A^t(M\invt q_n)\alpha_n\\
    &&\hbox{where $\alpha_n=(s_n^tM\inv(M r_n))/((M\invt q_n)^tAp_n)$}\\
    p_{n+1}&=&M\inv(M r_{n+1})-p_n\beta_n\\
    M\invt q_{n+1}&=&M\invt s_{n+1}-M\invt q_n\beta_n\\
    &&\hbox{where $\beta_n=(s_{n+1}^tM\inv (Mr_{n+1}))/(s_n^tM\inv (Mr_n))$}\\
\end{eqnarray*}
We see that this describes a preconditioned
BiCG method on normal form, using $M$ as a preconditioner and $I$ as
the inner product,
and where $M\inv r_i$ are the right residuals and $M\invt s_i$ the
left search directions. To be precise:

\begin{lemma}The left preconditioner biconjugate gradient algorithm
is equivalent to a symmetrically preconditioned method with preconditioner~$M$
in that it generates the same iterates.
For the  residual sequence~$r_i$ generated by the left preconditioned method
we find that in each iteration~$i$, $Mr_i=Ax_i-b$.
\end{lemma}
\begin{proof}
The rewritten algorithm is on normal form, with residuals~$Mr_i$.
The residual vectors actually computed satisfy $AM\inv MR=MRH$,
that is, the transformed residuals $Mr_i$ could have been derived
from symmetrically preconditioned method with preconditioner~$M$.
Likewise, the left sequence~$S$ satisfies $A^tM\invt S=SH$.
with scalars such that $S^tM\inv\tilde R$ is diagonal.
\end{proof}

\begin{remark}
\index{Newton method!and left preconditioning}
\index{Left preconditioning!in nonlinear solvers}
From the definition $r_1=M\inv(Ax_1-b)$ we see that $r_i$ is related
in order of magnitude to~$M\inv b$, hence probably to~$A\inv b$. In
the context of nonlinear solves this means that we can adjust the
stopping criterium to the size of the Newton update~$A\inv f(u_i)$.
This observation was made in~\cite{ErnEtAl:polyalgorithmic-nonlinear}.
\end{remark}

From the resulting equation 
\[ S^tM\inv AM\inv\tilde R=S^tM\inv\tilde RH \]
we see that the method reduces to a symmetric one for symmetric
$A$~and~$M$.

Since $Mr_i=Ax_i-f$,
we can derive the true residuals from the computed ones
by applying~$M$.
This, however, may be an infeasible demand
on the preconditioner; recall that normally we apply~$M\inv$.
Instead, we could replace the $r_n$-update \[r_{n+1}=r_n+M\inv Ap_n\alpha_n\]
by 
\begin{eqnarray*} t_1&=&Ax_1-b\\ t_{n+1}&=&t_n+Ap_n\alpha_n\\
    r_n&=&M\inv t_n \end{eqnarray*}
which takes one extra vector to store, but introduces no new
operations. The sequence $t_i$ then has the true residuals.
Thus, we can monitor the true iterate and residual at no extra cost.

However, this monitoring presupposes that we can alter the code
to add this extra vector. If the iterative method simply contains
a hook for
\begin{verbatim}
    monitor(A,M,xi,ri);
\end{verbatim}
then we have incur the extra cost of computing $Mr_i$ in order to
monitor the true residual.

\Level 2 {Right preconditioning}

The right-preconditioned BiCG method is derived by applying an
unpreconditioned algorithm to coefficient matrix $AM\inv$, right
hand side~$b$, and initial guess~$x_1$; after the iteration stops
with a final solution~$x_\infty$,
the approximate solution to the original system is~$M\inv x_\infty$.
\begin{algorithm}{Right Preconditioned BiConjugate Gradient Method}
Let $A$, $M$, $x_1$ and~$b$ be given. Set initially
$r_1=AM\inv x_1-b$. Now iterate:
\begin{eqnarray*}
    x_{n+1}&=&x_n+p_n\alpha_n\\
    r_{n+1}&=&r_n+AM\inv p_n\alpha_n\\
    s_{n+1}&=&s_n+M\invt A^tq_n\alpha_n\\
    &&\hbox{where $\alpha_n=(s_n^tr_n)/(q_n^tAM\inv p_n)$}\\
    p_{n+1}&=&r_{n+1}-p_n\beta_n\\
    q_{n+1}&=&s_{n+1}-q_n\beta_n\\
    &&\hbox{where $\beta_n=(s_{n+1}^tr_{n+1})/(s_n^tr_n)$}
\end{eqnarray*}
The approximation to the true solution is $M\inv x_\infty$.
\end{algorithm}

We rewrite, expressing the algorithm in terms of 
transformed quantities $M\invt s_n$ and~$M\inv p_n$ as:
\begin{eqnarray*}
    r_1&=&AM\inv x_1-b\\
    &&\alpha_n=((M\invt s_n)^tM\inv r_n)/(q_n^tA(M\inv p_n))\\
    M\inv x_{n+1}&=&M\inv x_n+M\inv p_n\alpha_n\\
    r_{n+1}&=&r_n+A(M\inv p_n)\alpha_n\\
    M\invt s_{n+1}&=&M\invt s_n+A^tq_n\alpha_n\\
    M\inv p_{n+1}&=&M\inv r_{n+1}-(M\inv p_n)\beta_n\\
    q_{n+1}&=&M\invt (M\invt s_{n+1})-q_n\beta_n\\
\end{eqnarray*}
We see that this is a biconjugate gradient method on normal
form with preconditioner~$M$; 
the iterates are~$M\inv x_i$ and the residuals
are $r_i=A(M\inv x_i)-b$.

\begin{lemma}
The right preconditioned method is equivalent to a symmetrically
preconditioned method with preconditioner~$M$ 
and initial guess~$M\inv x_1$ in that is generates the same
residuals. For the iterates~$x_i$ generated we find that in
each iteration $r_i=AM\inv x_i-b$.
\end{lemma}

In other words,
contrary to the left-preconditioned case, here the $r_n$~quantities
are the true residuals. However, the $x_n$ computed are not approximations
to the solution; $\tilde x_n=M\inv x_n$~are, and just for completeness
we reiterate that $r_n=AM\inv x_n-b$, not~$Ax_n\nobreak-b$.
Whenever needed, $\tilde x_n$~can be computed by a single application
of~$M\inv$. Thus, we can monitor the true residual at no extra cost,
but monitoring the true iterates carries additional expense.

From the reduction
equation \[ \tilde S^tM\inv AM\inv R=\tilde S^tM\inv RH \] we see that the
method again reduces in the symmetric case.

\Level 2 {Two-sided preconditioning}

In the two-sided method we assume that $M=M_LM_R$, and we replace
the coefficient matrix by~$M_L\inv AM_R\inv$. The algorithm is:
\begin{eqnarray*}
    r_1&=&M_L\inv AM_R\inv x_1-M_L\inv b;
                    \qquad\hbox{true solution is $M_R\inv x_\infty$}\\
    &&\alpha_n=(s_n^tr_n)/(q_n^tM_L\inv AM_R\inv p_n)\\
    x_{n+1}&=&x_n+p_n\alpha_n\\
    r_{n+1}&=&r_n+M_L\inv AM_R\inv p_n\alpha_n\\
    s_{n+1}&=&s_n+M_R\invt A^tM_L\invt q_n\alpha_n\\
    &&\beta_n=(s_{n+1}^tr_{n+1})/(s_n^tr_n)\\
    p_{n+1}&=&r_{n+1}-p_n\beta_n\\
    q_{n+1}&=&s_{n+1}-q_n\beta_n
\end{eqnarray*}
Rewriting this in terms of $M_Lr_n$, $M_R^ts_n$, $M_R\inv p_n$, 
and~$M_L\invt q_n$:
\begin{eqnarray*}
    M_Lr_1&=&A(M_R\inv x_1)-b\\
    &&\alpha_n=\bigl((M_R^ts_n)M_R\inv M_L\inv(M_Lr_n)\bigr) /
                \bigl((M_L\invt q_n)^tA(M_R\inv p_n)\bigr)\\
    M_R\inv x_{n+1}&=&M_R\inv x_n+M_R\inv p_n\alpha_n\\
    M_Lr_{n+1}&=&M_Lr_n+A(M_R\inv p_n)\alpha_n\\
    M_R^ts_{n+1}&=&M_R^ts_n+A^t(M_L\invt q_n)\alpha_n\\
    M_R\inv p_{n+1}&=&M_R\inv M_L\inv(M_Lr_{n+1})-M_R\inv p_n\beta_n\\
    M_L\invt q_{n+1}&=&M_L\invt M_R\invt*(M_R^ts_{n+1})-M_L\invt q_n\beta_n
\end{eqnarray*}
With $\tilde R=M_LR$ and $\tilde S=M_R^tS$ we find generating equations
\begin{eqnarray*}
    \hbox{$\tilde X=M_R\inv X$ is a polynomial method for $A\inv b$}\\
    \hbox{$\tilde R$ is the residual sequence $R\langle A,\tilde X,b\rangle$}\\
    AM\inv R&=&RH\\ A^tM\invt\tilde S&=&\tilde SH
    \end{eqnarray*}
with $\tilde S^tM\inv \tilde R$ constructed to be diagonal.
The reduction equation is 
\[ \tilde S^tM\inv AM\inv \tilde R=\tilde S^tM\inv \tilde RH \]
so again this reduces to the symmetric case.

Since $M_LR$ is the residual sequence of $M_R\inv X$, the true iterates,
we can obtain the true residuals as~$M_LR$, or, if this is infeasible,
compute
\begin{eqnarray*} t_1&=&AM_R\inv x_1-b\\ t_{n+1}&=&t_n+AM_R\inv p_n\alpha_n\\
    r_n&=&M_L\inv t_n\\ \end{eqnarray*}
so that~$T=\nobreak M_LR$.

\Level 2 {The Eisenstat trick}

In~\cite{Ei:efficientimplementation}, Eisenstat gives an efficient
implementation of certain preconditioned Conjugate Gradient methods,
based on the following observations:
\begin{itemize}
\item A CG method for a system with coefficient matrix~$A$,
preconditioned by $M=LD\inv U\approx A$, is equivalent
to one with matrix~$L\inv AU\inv$ preconditioned by~$D\inv$, and
\item a product with $L\inv AU\inv$ can be evaluated cheaply
if $A=L+U+D$ for a certain~$D$.
\end{itemize}

We derive the first point for a Biconjugate Gradient Method:
\begin{eqnarray*}
    x_{n+1}&=&x_n+p_n\alpha_n\\
        &&\alpha_n={s_n^tM\inv r_n\over q_n^tAp_n}\\
    r_{n+1}&=&r_n+Ap_n\alpha_n\\
    s_{n+1}&=&s_n+A^tq_n\alpha_n\\
    p_{n+1}&=&M\inv r_{n+1}-p_n\beta_n\\
    q_{n+1}&=&M\invt s_{n+1}-q_n\beta_n
\end{eqnarray*}
Rearranging the parts of~$M$:
\begin{eqnarray*}
    x_{n+1}&=&x_n+U\inv(Up_n)\alpha_n\\
        &&\alpha_n={(U\invt s_n)^tD (L\inv r_n)\over
                 (L^tq_n)^tL\inv AU\inv (Up_n)}\\
    L\inv r_{n+1}&=&L\inv r_n+L\inv AU\inv(Up_n)\alpha_n\\
    U\invt s_{n+1}&=&U\invt s_n+(L\inv AU\inv)^t(L^tq_n)\alpha_n\\
    Up_{n+1}&=&D(L\inv r_{n+1})-Up_n\beta_n\\
    L^tq_{n+1}&=&D^t(U\invt s_{n+1})-L^tq_n\beta_n
\end{eqnarray*}
which with the  transformed variables
\[ \tilde r_n=L\inv r_n,\quad \tilde s_n=U\invt s_n,\quad
    \tilde p_n=Up_n,\quad \tilde q_n=L^tq_n \]
becomes
\begin{eqnarray*}
    \tilde r_1&=&L\inv r_1\\
    \tilde s_1&=&\hbox{arbitrary, or $U\invt s_1$}\\
    x_{n+1}&=&x_n+U\inv\tilde p_n\alpha_n\\
        &&\alpha_n={\tilde s_n^tD \tilde r_n\over
                 \tilde q_n^tL\inv AU\inv \tilde p_n}\\
    \tilde r_{n+1}&=&\tilde r_n+L\inv AU\inv\tilde p_n\alpha_n\\
    \tilde s_{n+1}&=&\tilde s_n+(L\inv AU\inv)^t\tilde q_n\alpha_n\\
    \tilde p_{n+1}&=&D\tilde r_{n+1}-\tilde p_n\beta_n\\
    \tilde q_{n+1}&=&D^t\tilde s_{n+1}-\tilde q_n\beta_n
\end{eqnarray*}

The second observation is that, with $A=L+U+K$ where $K=D_A-D_L-D_U$,
\[ L\inv AU\inv = L\inv(L+U+K)U\inv =U\inv +L\inv(I+KU\inv). \]
In other words, applying~$A$ and solving with~$M$ can be done for
a joint cost of one upper and one lower solve, 
multiplication by~$K$, and two vector additions.

Given the requirements that $M=LD\inv U$ and~$A=L+U+K$,
it is most natural to write $M=(L_A+D)D\inv(D+U_A)$.
Thus we could construct~$D$ by a point factorisation (including SSOR)
of~$A$, but everything generalises to block versions of these methods.
Multiplication by $K=D_A-2D$ is straightforward, in both the
point and the block case.

\begin{comment}
\Level 0 {$A$-orthogonalization}

If $Q$ and $P$ are not orthogonalized under the usual inner
product, but under $(A\cdot,\cdot)$ instead, a tridiagonalization
of $A^2$ is found: if $Q^tAP=\Omega$ with $\Omega$ diagonal,
then \begin{eqnarray*}Q^tA^2=Q^tA(AP)&=&Q^tAP(J+U)=\Omega(J+U)\\
        &=&(Q^tA)AP=(J^t+V^t)Q^tAP=(J^t+V^t)\Omega
\end{eqnarray*}
\end{comment}

\Level 1 {Preconditioning by the symmetric part and separable approximations}

As symmetric matrices are often more pleasant to work with than
nonsymmetric ones, one could consider preconditioning a matrix~$A$
with its symmetric part~$Q=\nobreak(A+A^t)/2$.
In a generalization of this idea, one could precondition by an
approximation of the symmetric part; a~separable one is a particularly
good idea, since those can be solved very efficiently.

Here are some of the forms this idea has gone through.

\begin{itemize}
\item Solving a linear system from a {\bf separable} equation
\[ Au = -(a(x)u_x)_x-(b(y)u_y)_y \]
can be done by {\em fast solvers} such as the Fast Fourier Transform
in $O(N\log N)$ operations or thereabouts~\cite{Sw:poissonrectangle}.
\item If $A$ is {\bf nonseparable, but self-adjoint elliptic,}
  approximating it with a {\em self-adjoint separable} operator~$M$
  gives a spectrally equivalent
  preconditioner. D'Yakonov~\cite{DY:adi} considered ADI as a
  preconditioner; Widlund~\cite{Wi:fastseparable} used a stationary
  iteration method here, which Concus and
  Golub~\cite{CoGo:nonseparable} accelerated with Chebyshev iteration,
  and Bank~\cite{Ba:marching} (using the \index{Marching
  algorithm}marching algorithm) by Conjugate Gradients.
\item The general, {\bf nonself-adjoint, nonseparable, elliptic} case
  was discussed by Concus and Golub~\cite{CoGo:cgnonsymmetric} and
  Widlund~\cite{Wi:nonsymm}. Elman and Schultz~\cite{ElSc:fastdirect}
  proved spectral equivalence for self-adjoint preconditioners, but
  gave numerical evidence that nonself-adjoint preconditioners give
  similar asymptotic rates.
\end{itemize}

\Level 1 {Diagonal coefficient matrices}

While it is silly to solve a system with a coefficient matrix of
diagonal form by an iterative method, such matrices can be good test
problems. Stationary iterative methods converge in one step with such
a matrix, but, surprisingly, conjugacy based methods do not see the
form of the matrix, other than through different roundoff behaviour.

\begin{question}Prove this.\end{question}

\Level 1 {CG in Function Spaces}

\verbatiminput{banach.txt}

\Level 0 {History}

The evolution of the conjugate gradient method and other Krylov
methods has proceeded along branching paths. Here are some
of the defining papers.

\begin{description}
    \item[Lanczos 1950~\cite{Lanczos1950:iteration_method}]
Cornelius Lanczos proposed three-term recurrences 
$b_{n+1}=Ab_{n-1}-\alpha_nb_n-\beta_{n-1}b_{n-1}$
for the solution of the eigenvalue problem. 
Orthogonality of the $b_n$ vectors follows from the requirement
that $b_{n+1}$~be of minimal length (Lanczos credits the idea
of successive orthogonalisation of a sequence to~\cite{Szaz:19}).
The generalisation to a nonsymmetric matrix is by generating
two mutually orthogonal sequences, the second generated from
$b^*_{n+1}=A^tb^*_{n-1}-\alpha_nb^*_n-\beta_{n-1}b^*_{n-1}$,
that is, both satisfying a three-term recurrence.
No normalisation of the sequences is applied, with as result
$b_n=P_n(A)b_1$ where the polynomial~$P_n$ is normalised to
$P_n(x)=\nobreak x^n+\nobreak\cdots$.
Although the sequences satisfy three-term recurrences, the
actual algorithm uses coupled two-term recurrences.
    \item[Arnoldi 1951~\cite{Arnoldi1951:minimized_iterations}]
W.E. Arnoldi, also targeting the eigenvalue problem,
gave a further discussion of the Lanczos method,
and proposed a variant, which he terms a Galerkin method,
leaving out the $A^t$-based sequence~$b^*n$. As a result he
derives an upper Hessenberg matrix of $b_i^tAb_j$ coefficients.
    \item[Lanczos 1952~\cite{Lanczos1952:solution_of_systems}]
    \item[Hestenes and Stiefel 1952~\cite{HestenesStiefel1952:cg}]
    \item[Reid 1971~\cite{Reid1971:cg}]
    \item[Fletcher 1975~\cite{Fletcher1975:indefinite}]
    \item[Axelsson 1980 \cite{Ax:unsymmetricinconsistent}]
    \item[O'Leary 1980~\cite{OLeary1980:blockcg}]
    \item[Young and Jea 1980~\cite{YoungJea1980:generalizedcg}]
    \item[Faber and Manteuffel 1984~%
\cite{FaberManteuffel:conditions-for-existence}]
    \item[Sonneveld 1989~\cite{Sonneveld1989:cgs}]
    \item[van der Vorst 1992~\cite{vdVorst1992:bicgstab}]
\end{description}

For an extensive bibliography we refer to~\cite{GoOLe:cghist}.

\begin{comment}
\Level 0 {Conclusion}

We have presented the conjugate gradient-like methods
in a matrix framework. A~clear separation between the
Hessenberg matrix associated with Krylov sequences, orthogonalization
under different inner products, preconditioning,
and minimization properties was made. Facts normally taken for
granted, such as the three-term form of recurrences, or the fact
that these methods can
be used for iterative solution of linear systems, were derived,
not taken as a premise.
Additionally, we have
shown how cg methods can be considered as polynomial
accelerations of basic stationary iterative method.

Using the matrix framework for talking about vector sequences, we have
given short derivations of the conjugate gradients method, both for
symmetric and unsymmetric systems, the Lanczos algorithm and the equivalent
biconjugate gradient method, the least squares methods GMRES and QMR
and the conjugate gradients squared method. 
\end{comment}

\newpage

\bibliographystyle{plain}
\bibliography{vle}

\newpage

\appendix
\Level 0 {Matlab code examples}

\Level 1 {Minimum residual methods}
\Level 2 {GMRES}
\label{sec:matlab-gmres}
\verbatiminput{matlab/iterative/my_gmres.m}

\Level 1 {Polynomial squaring methods}
\label{sec:matlab-cgs}

\Level 2 {Conjugate Gradient Squared}
\verbatiminput{matlab/iterative/cgs.m}

\Level 1 {Arnoldi method}
\label{sec:matlab-arnoldi}
See section~\ref{sec:matlab-ortho} for the {\tt xqr} routine.
\verbatiminput{matlab/iterative/arnoldi.m}

\Level 1 {Householder reflectors}
\label{sec:matlab-householder}
These code examples illustrate the theory of section~\ref{sec:householder}.

\Level 2 {Basic reduction routines}
Reduction to Hessenberg form by Householder reflectors:
\verbatiminput{matlab/matrix/householder_vector.m}
\verbatiminput{matlab/matrix/householder_reflector.m}
\verbatiminput{matlab/matrix/hessenberg_reduction.m}

\Level 2 {Retrieving Householder vectors from a Lanczos basis}
\verbatiminput{matlab/matrix/householder_from_lanczos.m}

\Level 1 {Utilities}

\Level 2 {Orthogonalisation}
\label{sec:matlab-ortho}
\verbatiminput{matlab/iterative/xqr.m}

\printindex

\end{Outline}

\end{document}

\Level 0 {Krylov sequences and Hessenberg matrices}\label{sec:krylov}

\Level 1 {Axelsson Derivation of Preconditioned Conjugate Gradients}

If $M$ is symmetric and positive definite, we can split it
(theoretically, not computationally) as $M=EE^t$, and we formulate the
conjugate gradients method as iterating on the system 
\[ E^tAE(E^{-1}x)=E^tf,\]
that is, $r_n$~vectors are generated from the equation
\[ E^tAER=RH \]
where $H$ is again factored as $H=(I-J)D\inv (I-U)$.

The first of the coupled recurrences in section~\ref{coupled:recur}
is now replaced by $E^tAEPD=R(I-J)$, and we get for the full method
\[ A(EP)D=(E^{-t}R)(I-J),\qquad E^{-t}R=E^{-t}E^{-1}(EP)(I-U). \]
Introducing transformed sequences
$\tilde R= E^{-t}R$ and $\tilde P= EP$, we get
the coupled recurrences
\begin{equation}
        A\tilde PD=\tilde R(I-J),\qquad M\tilde R=\tilde P(I-U).
        \label{prec:cg:recur}\end{equation}
Computationally, the algorithm is extended by a single step of forming
the product~$MR$, and the splitting of~$M$ is never explicitly required.

In order to compute the scalar quantities of the algorithm we note
that $R^tR=\tilde R^tM\tilde R$. Where the conjugate gradient
algorithm uses $p_i^tAp_i$, we now get, because of the transformed
system, $p_i^tE^tAEp_i$, which is equal to $\tilde p_i^tA\tilde p_i$.
For the methods in section~\ref{sec:unsync}
we further note that $R^tAR$ becomes in the
transformed system $R^tE^tAER=\tilde R^tM\invt AM\tilde R$.

If we let $\tilde x$ be the solution of $E^tAE\tilde x=E^tf$, then in
the end we are interested in obtaining $x=E\tilde x$. Noting that the
updating formula $\tilde X(J-I)=PD$ follows from
equation~(\ref{prec:cg:recur}), we find that
\begin{equation} X(J-I)=E\tilde X(J-I)=\tilde PD,
        \end{equation}
that is, using the transformed search directions we can actually
update approximations to the solution of the original system.

Also,
        \[ \tilde R=E^{-t}R=E^{-t}(E^tAE\tilde x-E^tf)=Ax-f, \]
that is, the transformed residuals are the residuals of the original
system.

Summarizing, we get the following algorithm (where we now simply
write~$r_i$ for the $\tilde r_i$~of the above theory).

\begin{algorithm}{Preconditioned Conjugate Gradient method}
Choose $x_1$ arbitrarily, and let $r_1=Ax_1-f$ and $p_1=\hat r_1=Mr_1$.
Then perform the following steps
for $i=1,\ldots$:
\begin{equation}
        \hbox{compute $d_{ii}=\hat r_i^tr_i/ p_i^tAp_i$}
        \end{equation}
Update the iterate
\begin{equation} x_{i+1}=x_i-p_id_{ii} \end{equation}
and the residual
\begin{equation} r_{i+1}=r_i-Ap_id_{ii}. \end{equation}
Apply the preconditioner
\begin{equation} \hat r_{i+1}=Mr_{i+1}, \end{equation}
\begin{equation}
        \hbox{compute $u_{ii+1}=\hat r_{i+1}^tr_{i+1} /
                                \hat r_i^tr_i$.}
        \end{equation}
Update the search direction
\begin{equation} p_{i+1}=\hat r_{i+1}+p_iu_{ii+1}.
        \end{equation}
\end{algorithm}

\Level 1 {Preconditioning as a change of matrix and inner product}

We started formulating the preconditioned iteration as arising from a
Krylov sequence with the matrix~$E^tAE$. However, the resulting
sequence~$R$ satisfying $E^tAER=RH$ is never formed. Instead we
compute a sequence~$\tilde R=E^{-t}R$. This sequence can be
interpreted as  arise from a
Krylov sequence of another system, orthogonalized under another inner
product. 

\begin{lemma}
\label{lemma:prec:cg}
The sequence $\tilde R$ consists of linear combinations of a Krylov
sequence of the matrix~$AM$, and it is orthogonal under the $M$-inner
product.
\end{lemma}
\begin{proof} The sequence $\tilde R$ satisfies $AM\tilde R=\tilde RH$,
that is, it arises from a Krylov sequence of the matrix~$AM$
(see lemma~\ref{R-Krylov-combo}). For a
general symmetric choice for~$M$ this matrix will be nonsymmetric, but
noting that the scalars in the iterative method are determined
from the equation
\[ \tilde R^tMAM\tilde R=\tilde R^tM\tilde RH \]
we find by comparing to equation~(\ref{other:orthogonal}) that we have 
orthogonalized~$\tilde R$ under the $M$~inner product, which symmetrizes
the matrix~$AM$ again.\end{proof}

Although the discussion so far used a symmetric
preconditioner $M=EE^t$, it is easy to see that taking $M=FE^t$ and
iterating on a system with coefficient matrix $E^tAF$ will also lead
to a formula $M\tilde R=\tilde P(I-U)$, that is, involving only
the unfactored preconditioner.

\Level 1 {Preconditioned Lanczos}

Preconditioning the Lanczos iteration is largely analogous to
preconditioning the conjugate gradient method as described above.
(The only exposition of a preconditioned Lanczos method in the
literature that we are aware of is in~\cite{DoDuSovdVo:solving}; apart
from an implementational detail it is identical to the method derived
here.)

Since the Lanczos method can handle nonsymmetric matrices, assume that
the preconditioner is nonsymmetric too, and that we
iterate on a system with matrix $E^tAF$. From
\[ E^tAFPD=R(I-J),\qquad F^tA^tEQD=S(I-J) \]
we find
\[ A\tilde PD=\tilde R(I-J),\qquad A^t\tilde QD=\tilde S(I-J) \]
where
\[ \tilde R=E^{-t}R,\qquad \tilde S=F^{-t}S,\qquad
   \tilde P=FP,\qquad \tilde Q=EQ. \]
For the scalar quantities we find first of all that
\[ S^tR=\tilde SM\tilde R \]
where $M=FE^t$ is the preconditioner.
Furthermore, the quantity $q_i^tAp_i$ in the conjugate gradient method
now becomes $q_i^tE^tAFp_i$, which is equal to $\tilde q_i^tA\tilde p_i$.

The update relations $R=P(I-U)$ and $S=Q(I-U)$
for the search directions become
\[ M\tilde R=\tilde P(I-U),\qquad M\invt \tilde S=\tilde Q(I-U) \]
for the transformed sequences. We see that applications of
both the preconditioner and its transpose are necessary.

\begin{comment}

\Level 1 {Orthogonal directions}

If the Hessenberg matrix $H$ generating $R$ through $AR=RH$ is
factored $H=(I-L)U$, and we introduce as above $P=RU^{-1}$, then
\[ AP=RL,\qquad\hbox{and}\qquad AP=P\tilde H \]
where $\tilde H=U(I-L)$ is an upper Hessenberg matrix. By taking
scalar multiples $p_ic_i$ of $p_i$ we find that
\[ APC=(PC)(C^{-1}UC)(C^{-1}(I-L)C) \]
so that we can write
\[ AP=R(I-J) \] by a suitable choice of~$C$. We can now generate
$\tilde H$ directly, for instance in such a way that $P^tP$ is
diagonal.

